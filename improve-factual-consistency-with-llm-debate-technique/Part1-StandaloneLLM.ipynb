{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604ee77c-0e19-4a36-9f70-e8ab02cfaf54",
   "metadata": {},
   "source": [
    "<center><img src=\"images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "# <a name=\"0\">Improve Factual Consistency Part 1 </a>\n",
    "## <a name=\"0\">Improving Factual Consistency and Explainability using standalone LLM  </a>\n",
    "\n",
    "### Glossary of Terms\n",
    "- Naive Judge : This LLM has **no** access to transcript but only question and two summaries. Measure the baseline performance.\n",
    "- Expert Judge : This LLM has access to transcript along with question and two summaries\n",
    "- Question asked to LLM (in all experiments): It is always the same: `Which one of these summaries is the most factually consistent one?`\n",
    "\n",
    "## Dataset\n",
    "Our dataset is distilled from the Amazon Science evaluation benchmark dataset called <a href=\"https://github.com/amazon-science/tofueval\">TofuEval</a>. 10 summaries have been curated from the [MediaSum documents](https://github.com/zcgzcgzcg1/MediaSum) inside the tofueval dataset for this notebook. \n",
    "\n",
    "MediaSum is a large-scale media interview dataset contains 463.6K transcripts with abstractive summaries, collected from interview transcripts and overview / topic descriptions from NPR and CNN.\n",
    "\n",
    "## LLM Access\n",
    "\n",
    "We will need access to Anthropic Claude v3 Sonnet, Mistral 7b and  Mixtral 8x7b LLMs for this notebook.\n",
    "\n",
    "[Anthropic Claude v3(Sonnet)](https://www.anthropic.com/news/claude-3-family) , [Mixtral 8X7B](https://mistral.ai/news/mixtral-of-experts/), [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) - all of them pre-trained on general text summarization tasks.\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "In this notebook, we navigate the LLM debating technique with more persuasive LLMs having two expert debater LLMs (Claude and Mixtral) and one judge (using Claude - we can use others like Mistral/Mixtral, Titan Premier) to measure, compare and contrast its performance against other techniques like self-consistency (with naive and expert judges) and LLM consultancy. This notebook is an adapted and partial implementation of one of the ICML 2024 best papers, <a href=\"https://arxiv.org/pdf/2402.06782\"> Debating with More Persuasive LLMs Leads to More Truthful Answers </a> on a new and different Amazon Science evaluation dataset <a href=\"https://github.com/amazon-science/tofueval\">TofuEval</a>. \n",
    "\n",
    "\n",
    "- Part 1.  **[THIS notebook]** Demonstrate typical Standalone LLM approach\n",
    "\n",
    "- Part 2.  Demonstrate the LLM Consultancy approach and compare with Part 1.\n",
    "\n",
    "- Part 3.  Demonstrate the LLM Debate approach and compare with other methods.\n",
    "\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    While this notebook(part 1, 2 and 3) compares various methods and demonstrates the efficacy of LLM Debates in notebook part 3 with a supervised dataset, the greater benefit is possible in unsupervised scenarios where ground truth is unknown and ground truth alignment and/or curation is required. Human annotation can be expensive plus slow and agreement amongst human annotators adds another level of intricacy. A possible `scalable oversight direction could be this LLM debating technique to align on the ground truth options` via this debating and critique mechanism by establishing factual consistency(veracity). This alignment and curation of ground truth for unsupervised data could be a possible win direction for the debating technique in terms of cost versus benefit analysis.\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "\n",
    "#### Notebook Kernel\n",
    "Please choose `conda_python3` as the kernel type of the top right corner of the notebook if that does not appear by default.\n",
    "\n",
    "#### LLMs used\n",
    "[Anthropic Claude v3(Sonnet)](https://www.anthropic.com/news/claude-3-family) , [Mixtral 8X7B](https://mistral.ai/news/mixtral-of-experts/), [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) - all of them pre-trained on general text summarization tasks.\n",
    "\n",
    "## Use-Case Overview\n",
    "\n",
    "To demonstrate the measurement and improvement of factual consistency (veracity) with explainability in this notebook, we conduct a series of experiments to choose the best summary for each transcript. In each experiment, we measure the veracity and correctness of the summaries generated from transcripts and improve upon the decision to choose the correct one via methods like LLM consultancy and LLM debates.\n",
    "\n",
    "The <b>overall task in this notebook</b> is choose which one of the two summaries is most appropriate for a given transcript. There are a total of 10 transcripts and each transcript has 2 summaries - one correct and other incorrect. The incorrect summaries have various classes of errors like `Nuanced Meaning Shift`, `Extrinsic Information` and  `Reasoning errors`. \n",
    "\n",
    "In this notebook we will conduct the following set of experiment combinations to measure, compare and contrast LLM debating techniques with others.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    If you see throttling exception, please increase timeout from 10 seconds in `time.sleep(10)` to say 20 and retry\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "## Experiments\n",
    "For each of these experiments we flip the side of the argument the LLM takes to account for `position bias` and `verbosity bias` and re-run each experiment.\n",
    "\n",
    "**Note** We always use the same Judge LLM (Mistral 7B) across all the experiments in this notebook\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Experiment 1: (Naive judge - this judge has no access to transcripts): \n",
    "<center><img src=\"images/veracitylab01-llm-naive-judge.png\" alt=\"In this image, we depict the flow of Naive LLM judge. First the naive judge LLM has NO access to transcripts just the question and two summaries to choose from\n",
    "as the more factually consistent. Next the naive judge makes a random guess\n",
    "which of the two summaries are more factually consistent for 3 rounds. Majority answer is chosen based on self-consistency technique.\"  height=\"700\" width=\"700\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "Mistral as naive judge with no access to transcripts. This continues for N(=3 in this notebook) rounds to ensure self-consistency and assert the majority answer as correct. We use this to mark the baseline performance of these series of experiments.\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment 2: (Expert judge: This LLM has access to transcripts): \n",
    "<center><img src=\"images/veracitylab01-llm-expert-judge.png\" alt=\"In this image, we depict the flow of LLM Expert Judge. First the expert Judge LLM has access to transcripts along with the question and two summaries to choose from\n",
    "as more factually consistent. Next the expert judge uses the transcript contents to decide which of the two summaries are more factually consistent for 3 rounds. Majority answer is chosen based on self-consistency technique\"  height=\"700\" width=\"700\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "\n",
    "Mistral as expert judge with access to transcripts. This continues for N(=3 in this notebook) rounds.This continues for N(=3 in this notebook) rounds to ensure self-consistency and assert the majority answer as correct.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "## Evaluation Metrics\n",
    "For each type of experiment we evaluate the accuracy of the answers for that experiment/method type to compare and contrast each method at the end.\n",
    "\n",
    "For the final experiment on LLM Debate, we also calculate the `win rate` of the LLM debaters to evaluate which of the LLMs actually got most of the answers right as adjudicated by the judge. This can be considered a mechanism to choose one LLM over the other given this use-case.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This notebook notebook has the following sections:\n",
    "\n",
    "1. <a href=\"#1\">Dataset exploration</a>\n",
    "2. <a href=\"#2\">Naive Judge: no access to transcripts - Arguing for 1st summary</a>\n",
    "3. <a href=\"#3\">Naive Judge: no access to transcripts - Arguing for 2nd summary</a>\n",
    "4. <a href=\"#4\">Accuracy of Naive Judge</a>\n",
    "5. <a href=\"#5\">Expert Judge: access to transcripts - Arguing for 1st summary</a>\n",
    "6. <a href=\"#6\">Expert Judge: access to transcripts - Arguing for 2nd summary</a>\n",
    "7. <a href=\"#7\">Accuracy of Expert Judge</a>\n",
    "8. <a href=\"#16\">Challenge exercise and notebook quiz</a>\n",
    "    \n",
    "Please work top to bottom of this notebook and don't skip sections as this could lead to error messages due to missing code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf33f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<a class=\"github-button\" href=\"https://github.com/aws-samples/improve-factual-consistency-with-llm-debate-technique\" data-color-scheme=\"no-preference: light; light: light; dark: dark;\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star Improve Factual Consistency with LLM Debates on GitHub\">Star</a>\n",
    "<script async defer src=\"https://buttons.github.io/buttons.js\"></script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f889117-2f21-4fd1-9563-2dc23e984c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install setuptools==70.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2ad2f4-b720-48b9-bb5a-7b99bcafce8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q -U pip --root-user-action=ignore\n",
    "!pip3 install -q -r requirements.txt --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e6507-fc9a-4f1f-8535-7e2deb20a9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We load all prompts from a separate file prompts.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from prompts import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mlu_utils.veracity_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573ddc8-9290-484c-86f9-f16531648cac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_up_files_in_dir(\"./transcripts\")\n",
    "clear_file_contents(\"./log_files/notebook_run_logs.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c84b37-369c-405c-ac08-23be8dbb61a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re, time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import Markdown\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import logging\n",
    "import boto3, warnings\n",
    "import pandas as pd\n",
    "# Supress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(filename='log_files/notebook_run_logs.log', encoding='utf-8', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"----- Test logging setup -----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df8234-935a-44de-a751-b8eb9e73df43",
   "metadata": {},
   "source": [
    "### Bedrock Model Access check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdabd249-b40b-4910-b86d-633394e2b1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test if all bedrock model access has been enabled \n",
    "test_llm_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c14a1-7d5b-48dc-a4a0-5c5e0a205f57",
   "metadata": {},
   "source": [
    "### Constants used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf134bf-7fbd-4c70-a84c-4b077f79f2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rounds = 3\n",
    "question = \"Which one of these summaries is the most factually consistent one?\"\n",
    "total_data_points = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829f829-7d19-420c-85ea-0e5c370304e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a name=\"1\">Dataset Exploration</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d79c9-c30d-4daa-9d25-4dac7de60e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pre-process the dataset\n",
    "answers_df = pd.read_csv(\"./tofueval_dataset/mediasum_dev_doc_id_group_final_dual_summaries_manual_final_dataset.csv\")\n",
    "#answers_df.head()\n",
    "interview_df = pd.read_csv(\"./tofueval_dataset/mediasum_dev_doc_complete_final.csv\")\n",
    "#interview_df.head()\n",
    "\n",
    "result = pd.merge(answers_df, interview_df, on=\"doc_id\")\n",
    "final_dataset = result[[\"doc_id\", \"topic\", \"summ_sent_incorrect_original\", \"summ_sent_correct_manual\", \"exp\", \"type\", \"source\"]]\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47427f17-5e00-44ab-bef7-3d148a451090",
   "metadata": {},
   "source": [
    "### <a name=\"2\">Naive Judge: no access to transcripts - Arguing for 1st summary</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf3d27-3a30-406e-98c6-87b2be4c290a",
   "metadata": {},
   "source": [
    "Naive judge has no access to actual transcripts - it just has access to the question and the 2 summaries/answers. We use `self-consistency` technique to test this judge's answers for 3 rounds. It is possible the Naive Judge might be guessing randomly. We flip the answer options in the next set of experiment to determine the baseline performance of a naive judge accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9587731-8b77-4ac7-ae54-3b2fff5acf49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "naive_judge_regular_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    naive_judge_per_round = list()\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------NAIVE JUDGE Debate_id {debate_id}-------------------\")\n",
    "    for round_number in range(number_of_rounds):\n",
    "        logger.info(f\"START OF Naive Judge Round #{round_number + 1} for debate_id {debate_id} >>>>>> \\n\")\n",
    "        judge_response = invoke_mistral_standalone_naive(\n",
    "            debate_id = debate_id,\n",
    "            question = question,\n",
    "            answer_a = answer_a,\n",
    "            answer_b = answer_b\n",
    "        )\n",
    "        naive_judge_per_round.append(extract_final_answer(judge_response, flipped=False))\n",
    "        logger.info(f\">>>>>>> judge_response Round #{round_number + 1}>>>>> ::  {judge_response}\")\n",
    "        # Print the final response for turn-3\n",
    "        format_final_response(debate_id,\n",
    "                              round_number + 1, \n",
    "                              question=question, \n",
    "                              answer_a=answer_a, \n",
    "                              answer_b=answer_b, \n",
    "                              judge_response=judge_response)\n",
    "        logger.info(f\"END OF Naive Judge Round #{round_number + 1} for debate_id {debate_id} >>>>>> \\n\")\n",
    "    print(f\"=========== END OF Naive Judge Round #{round_number + 1} for debate_id {debate_id} ======= \\n\")\n",
    "    naive_judge_regular_answers.append(Counter(naive_judge_per_round).most_common()[0][0]) # get the value of the counter\n",
    "    print(f\"naive_judge_regular_answers :: {naive_judge_regular_answers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411c273-42e2-4bf6-8bc3-e6f51453265f",
   "metadata": {},
   "source": [
    "### <a name=\"3\">Naive Judge: no access to transcripts - Arguing for 2nd summary </a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f640f-97d4-4800-85a8-553659d0d819",
   "metadata": {},
   "source": [
    "Naive Judge (with 3 rounds of self-consistency) :: Flip the answers to account for any position bias of the summaries and re-run the experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9550217-2b0e-45b9-8d1c-bac07f8b0a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "naive_judge_flipped_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    naive_judge_flipped_per_round = list()\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------NAIVE JUDGE Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        logger.info(f\"START OF Naive Judge Round #{round_number + 1} >>>>>> \\n\")\n",
    "        judge_response = invoke_mistral_standalone_naive(\n",
    "            debate_id = debate_id,\n",
    "            question = question,\n",
    "            answer_a = answer_b, # flipped ans\n",
    "            answer_b = answer_a  # flipped ans\n",
    "        )\n",
    "        naive_judge_flipped_per_round.append(extract_final_answer(\n",
    "            judge_response=judge_response, \n",
    "            flipped=True))\n",
    "        logger.info(f\">>>>>>> judge_response Round #{round_number + 1}>>>>> ::  {judge_response}\")\n",
    "        # Print the final response for turn-3\n",
    "        format_final_response(debate_id,\n",
    "                              round_number + 1, \n",
    "                              question=question, \n",
    "                              answer_a=answer_b, \n",
    "                              answer_b=answer_a, \n",
    "                              judge_response=judge_response)\n",
    "        logger.info(f\"END OF Naive Judge Round #{round_number + 1} for debate_id {debate_id} >>>>>> \\n\")\n",
    "    print(f\"=========== END OF Naive Judge Round #{round_number + 1} for debate_id {debate_id} ======= \\n\")\n",
    "    naive_judge_flipped_answers.append(Counter(naive_judge_flipped_per_round).most_common()[0][0]) # get the value of the counter\n",
    "    print(f\"naive_judge_flipped_answers :: {naive_judge_flipped_answers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887dc11-9743-4d9f-ad2c-3a3fe6eb37e3",
   "metadata": {},
   "source": [
    "### <a name=\"4\">Accuracy of Naive Judge</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d8474-9097-46a2-a536-3f17791c61be",
   "metadata": {},
   "source": [
    "Accuracy is defined as the matching results from the judge even if the answer options are flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c47f343-35a9-4795-b2b9-3dce3cd5d6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_naive_judge = find_num_matching_elements(naive_judge_regular_answers, naive_judge_flipped_answers)/total_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ab2b1-3b51-415c-94f3-2b5219e35368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_naive_judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d89c3-31d4-4958-abb4-0a11aaddb927",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fbe9ac-3d34-4fa8-943d-3cbed781ee11",
   "metadata": {},
   "source": [
    "### <a name=\"5\">Expert Judge: access to transcripts - Arguing for 1st summary</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d798becb-c385-4bd8-8994-9eccd887e337",
   "metadata": {},
   "source": [
    "EXPERT JUDGE (with 3 rounds of self-consistency)  - Access to actual transcripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c84cf6-4e9b-415b-af3f-7b360ed0ac0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "expert_judge_regular_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    expert_judge_per_round = list()\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------EXPERT JUDGE Debate_id {debate_id}-------------------\")\n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        logger.info(f\"Expert Judge Round #{round_number + 1} >>>>>> \\n\")\n",
    "        judge_response = invoke_mistral_standalone_expert(\n",
    "            debate_id = debate_id,\n",
    "            question = question,\n",
    "            answer_a = answer_a,\n",
    "            answer_b = answer_b,\n",
    "            complete_interview = complete_interview_transcript\n",
    "        )\n",
    "        expert_judge_per_round.append(extract_final_answer(judge_response, flipped=False))\n",
    "        logger.info(f\">>>>>>> judge_response Round #{round_number + 1}>>>>> ::  {judge_response}\")\n",
    "        # Print the final response for turn-3\n",
    "        format_final_response(debate_id, \n",
    "                              round_number + 1, \n",
    "                              question=question, \n",
    "                              answer_a=answer_a, \n",
    "                              answer_b=answer_b, \n",
    "                              judge_response=judge_response)\n",
    "        logger.info(f\"END OF Expert Judge Round #{round_number + 1} >>>>>> \\n\")\n",
    "    print(f\"=========== END OF Expert Judge Round #{round_number + 1} for debate_id {debate_id} ======= \\n\")\n",
    "    expert_judge_regular_answers.append(Counter(expert_judge_per_round).most_common()[0][0]) # get the value of the counter\n",
    "    print(f\"expert_judge_regular_correct_answers :: {expert_judge_regular_answers}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e088a-09d7-48d5-b37a-a4d565c68f29",
   "metadata": {},
   "source": [
    "### <a name=\"6\">Expert Judge: access to transcripts - Arguing for 2nd summary</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc69dd-a4a0-4243-b75d-e3a3faee4f42",
   "metadata": {},
   "source": [
    "Expert JUDGE with access to transcripts  (with 3 rounds of self-consistency) :: flip answer for position bias situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943607df-7998-4512-93be-da84e7c8f087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "expert_judge_flipped_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    expert_judge_flipped_per_round = list()\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------EXPERT JUDGE Debate_id {debate_id}-------------------\")\n",
    "    \n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        logger.info(f\"Expert Judge Round #{round_number + 1} >>>>>> \\n\")\n",
    "        judge_response = invoke_mistral_standalone_expert(\n",
    "            debate_id = debate_id,\n",
    "            question = question,\n",
    "            answer_a = answer_b, # flipped\n",
    "            answer_b = answer_a, # flipped\n",
    "            complete_interview = complete_interview_transcript\n",
    "        )\n",
    "        expert_judge_flipped_per_round.append(extract_final_answer(judge_response, flipped=True))\n",
    "        logger.info(f\">>>>>>> judge_response Round #{round_number + 1}>>>>> ::  {judge_response}\")\n",
    "        # Print the final response for turn-3\n",
    "        format_final_response(debate_id,\n",
    "                              round_number + 1, \n",
    "                              question=question, \n",
    "                              answer_a=answer_b, \n",
    "                              answer_b=answer_a, \n",
    "                              judge_response=judge_response)\n",
    "        logger.info(f\"END OF Expert Judge Round #{round_number + 1} >>>>>> \\n\")\n",
    "    print(f\"=========== END OF Expert Judge Round #{round_number + 1} for debate_id {debate_id} ======= \\n\")\n",
    "    expert_judge_flipped_answers.append(Counter(expert_judge_flipped_per_round).most_common()[0][0]) # get the value of the counter\n",
    "    print(f\"expert_judge_flipped_answers :: {expert_judge_flipped_answers}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9dab4-1d4c-4aec-bcd3-0e1d5afaeea0",
   "metadata": {},
   "source": [
    "### <a name=\"7\">Accuracy of Expert Judge</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c38d55-add7-451f-b337-f5fbbc3c5ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expert_judge_regular_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a80446-836c-4246-8c6c-562d13ece5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expert_judge_flipped_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa7518e0-d7f5-4325-9078-bb2d5a67b0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_expert_judge = find_num_matching_elements(expert_judge_regular_answers, expert_judge_flipped_answers)/total_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b13be-c2a1-41ae-b461-09e41a17112b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_expert_judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75434e9f-bc20-4066-aa30-0ed9673152db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mlu_utils.veracity_utils import *\n",
    "\n",
    "init_results_file()\n",
    "results_dict = {\"accuracy_naive_judge\":accuracy_naive_judge, \"accuracy_expert_judge\": accuracy_expert_judge}\n",
    "save_each_experiment_result(results_dict)\n",
    "print(\"notebook results saved in results folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fbd255-a593-438f-a44f-fce489056f26",
   "metadata": {},
   "source": [
    "## <a name=\"14\">Compare Accuracies across experiments/methods.</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97c205-fa79-4c09-b115-0cd3e9cae864",
   "metadata": {},
   "source": [
    "Here we compare the accuracies of each method/experiment to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39472b99-25dc-4d47-8242-450df4c13559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_consultant_judge = None\n",
    "accuracy_debate_judge = None\n",
    "\n",
    "final_accuracy_comparison_judge(\n",
    "    accuracy_naive_judge = accuracy_naive_judge,\n",
    "    accuracy_expert_judge = accuracy_expert_judge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e751a8d-fd46-4192-b100-655e8342ffe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the plot\n",
    "%matplotlib inline\n",
    "x_values = [ \"Naive Judge\", \"Expert Judge\"]\n",
    "y_values = [ accuracy_naive_judge, accuracy_expert_judge]\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Compare Accuracies across experiments')\n",
    "plt.xlabel('Experiment Type')\n",
    "plt.ylabel('Accuracy')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900272de-338f-4d20-96c9-b4a3536da531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba477a7c-5eb6-4ad6-a295-30e467f30e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e103f6-762e-4bb2-a719-bd45a0d21b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
