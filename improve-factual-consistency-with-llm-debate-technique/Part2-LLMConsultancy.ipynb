{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604ee77c-0e19-4a36-9f70-e8ab02cfaf54",
   "metadata": {},
   "source": [
    "<center><img src=\"images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">Improve Factual Consistency Part 2 </a>\n",
    "## <a name=\"0\">Improving Factual Consistency and Explainability using reasoning via LLM Consultancy </a>\n",
    "\n",
    "### Glossary of Terms\n",
    "- Naive Judge : This LLM has **no** access to transcript but only question and two summaries. Measure the baseline performance.\n",
    "- Expert Judge : This LLM has access to transcript along with question and two summaries\n",
    "- Question asked to LLM (in all experiments): It is always the same: `Which one of these summaries is the most factually consistent one?`\n",
    "\n",
    "## Dataset\n",
    "Our dataset is distilled from the Amazon Science evaluation benchmark dataset called <a href=\"https://github.com/amazon-science/tofueval\">TofuEval</a>. 10 summaries have been curated from the [MediaSum documents](https://github.com/zcgzcgzcg1/MediaSum) inside the tofueval dataset for this notebook. \n",
    "\n",
    "MediaSum is a large-scale media interview dataset contains 463.6K transcripts with abstractive summaries, collected from interview transcripts and overview / topic descriptions from NPR and CNN.\n",
    "\n",
    "## LLM Access\n",
    "\n",
    "We will need access to Anthropic Claude v3 Sonnet, Mistral 7b and  Mixtral 8x7b LLMs for this notebook.\n",
    "\n",
    "[Anthropic Claude v3(Sonnet)](https://www.anthropic.com/news/claude-3-family) , [Mixtral 8X7B](https://mistral.ai/news/mixtral-of-experts/), [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) - all of them pre-trained on general text summarization tasks.\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "In this notebook, we navigate the LLM debating technique with more persuasive LLMs having two expert debater LLMs (Claude and Mixtral) and one judge (using Claude - we can use others like Mistral/Mixtral, Titan Premier) to measure, compare and contrast its performance against other techniques like self-consistency (with naive and expert judges) and LLM consultancy. This notebook is an adapted and partial implementation of one of the ICML 2024 best papers, <a href=\"https://arxiv.org/pdf/2402.06782\"> Debating with More Persuasive LLMs Leads to More Truthful Answers </a> on a new and different Amazon Science evaluation dataset <a href=\"https://github.com/amazon-science/tofueval\">TofuEval</a>. \n",
    "\n",
    "\n",
    "- Part 1.  Demonstrate typical Standalone LLM approach\n",
    "\n",
    "- Part 2.  **[THIS notebook]** Demonstrate the LLM Consultancy approach and compare with Part 1.\n",
    "\n",
    "- Part 3.  Demonstrate the LLM Debate approach and compare with other methods.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    While this notebook(part 1,2 and 3) compares various methods and demonstrates the efficacy of LLM Debates in notebook part 3 with a supervised dataset, the greater benefit is possible in unsupervised scenarios where ground truth is unknown and ground truth alignment and/or curation is required. Human annotation can be expensive plus slow and agreement amongst human annotators adds another level of intricacy. A possible `scalable oversight direction could be this LLM debating technique to align on the ground truth options` via this debating and critique mechanism by establishing factual consistency(veracity). This alignment and curation of ground truth for unsupervised data could be a possible win direction for the debating technique in terms of cost versus benefit analysis.\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "\n",
    "#### Notebook Kernel\n",
    "Please choose `conda_python3` as the kernel type of the top right corner of the notebook if that does not appear by default.\n",
    "\n",
    "#### LLMs used\n",
    "[Anthropic Claude v3(Sonnet)](https://www.anthropic.com/news/claude-3-family) , [Mixtral 8X7B](https://mistral.ai/news/mixtral-of-experts/), [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) - all of them pre-trained on general text summarization tasks.\n",
    "\n",
    "## Use-Case Overview\n",
    "\n",
    "To demonstrate the measurement and improvement of factual consistency (veracity) with explainability in this notebook, we conduct a series of experiments to choose the best summary for each transcript. In each experiment, we measure the veracity and correctness of the summaries generated from transcripts and improve upon the decision to choose the correct one via methods like LLM consultancy and LLM debates.\n",
    "\n",
    "The <b>overall task in this notebook</b> is choose which one of the two summaries is most appropriate for a given transcript. There are a total of 10 transcripts and each transcript has 2 summaries - one correct and other incorrect. The incorrect summaries have various classes of errors like `Nuanced Meaning Shift`, `Extrinsic Information` and  `Reasoning errors`. \n",
    "\n",
    "In this notebook we will conduct the following set of experiment combinations to measure, compare and contrast LLM debating techniques with others.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    If you see throttling exception, please increase timeout from 10 seconds in `time.sleep(10)` to say 20 and retry\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "## Experiments\n",
    "For each of these experiments we flip the side of the argument the LLM takes to account for `position bias` and `verbosity bias` and re-run each experiment.\n",
    "\n",
    "**Note** We always use the same Judge LLM (Mistral 7B) across all the experiments in this notebook\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment 3: (LLM consultancy) \n",
    "<center><img src=\"images/veracitylab01-llm-consultancy.png\" alt=\"In this image, we depict the flow of LLM Consulancy. First a consultant LLMs is assigned a side to defend.They persuade the judge why their choice of summary is correct\n",
    "based on transcript contents. Next each consultation from the LLM is saved to a file and the consultant picks up the entire rationale history before posting their next thought. Finally, Once all 3 rounds of consultancy are over, the Judge LLM reads all the content and decides whether to agree or disagree with the consultant.\"  height=\"700\" width=\"700\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "We use Claude as consultancy for both sides of the answers separately and then take the average of both the experiments 3a and 3b as final accuracy. This continues for N(=3 in this notebook) rounds. This accounts for errors due to position (choosing an answer due to its order/position) and verbosity bias (one answer longer than the other)\n",
    "\n",
    "##### Experiment 3a: (LLM consultancy for Answer A): \n",
    "Claude v3(Sonnet) acting as a consultant always picks Answer A(Ground Truth:False Answer) and shares rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of these rounds, Claude as a judge adjudicates whether Claude as a debater's rationale is correct and if answer A is correct or not.\n",
    "##### Experiment 3b: (LLM consultancy for Answer B): \n",
    "Claude v3(Sonnet) acting as a consultant always picks Answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of these rounds, Claude  as a judge adjudicates whether Claude as a debater rationale is correct and if answer B is correct or not.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "## Evaluation Metrics\n",
    "For each type of experiment we evaluate the accuracy of the answers for that experiment/method type to compare and contrast each method at the end.\n",
    "\n",
    "For the final experiment on LLM Debate, we also calculate the `win rate` of the LLM debaters to evaluate which of the LLMs actually got most of the answers right as adjudicated by the judge. This can be considered a mechanism to choose one LLM over the other given this use-case.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This notebook notebook has the following sections:\n",
    "\n",
    "1. <a href=\"#1\">Dataset exploration</a>\n",
    "8. <a href=\"#8\">LLM Consultancy: 1 expert LLM consulting for 2nd summary , 1 naive judge</a>\n",
    "9. <a href=\"#9\">LLM Consultancy: 1 expert LLM consulting for 1st summary, 1 naive judge</a>\n",
    "10. <a href=\"#10\">Accuracy of LLM Consultancy</a>\n",
    "14. <a href=\"#14\">Compare Accuracies across experiments</a>\n",
    "16. <a href=\"#16\">Challenge exercise</a>\n",
    "    \n",
    "Please work top to bottom of this notebook and don't skip sections as this could lead to error messages due to missing code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b05d1f2-f629-4982-81a5-d7cbc3133e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install setuptools==70.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2ad2f4-b720-48b9-bb5a-7b99bcafce8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -q -U pip --root-user-action=ignore\n",
    "!pip3 install -q -r requirements.txt --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e6507-fc9a-4f1f-8535-7e2deb20a9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We load all prompts from a separate file prompts.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from prompts import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mlu_utils.veracity_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573ddc8-9290-484c-86f9-f16531648cac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_up_files_in_dir(\"./transcripts\")\n",
    "clear_file_contents(\"./log_files/notebook_run_logs.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c84b37-369c-405c-ac08-23be8dbb61a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re, time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import Markdown\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import logging\n",
    "import boto3, warnings\n",
    "import pandas as pd\n",
    "# Supress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(filename='log_files/notebook_run_logs.log', encoding='utf-8', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"----- Test logging setup -----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55085434-912c-4d24-b257-0136851ec650",
   "metadata": {},
   "source": [
    "### Bedrock Model Access check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e35bd98-f987-4c3e-98f7-372fc1f113df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if all bedrock model access has been enabled \n",
    "test_llm_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c14a1-7d5b-48dc-a4a0-5c5e0a205f57",
   "metadata": {},
   "source": [
    "### Constants used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf134bf-7fbd-4c70-a84c-4b077f79f2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rounds = 3\n",
    "question = \"Which one of these summaries is the most factually consistent one?\"\n",
    "total_data_points = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829f829-7d19-420c-85ea-0e5c370304e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a name=\"1\">Dataset Exploration</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d79c9-c30d-4daa-9d25-4dac7de60e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pre-process the dataset\n",
    "answers_df = pd.read_csv(\"./tofueval_dataset/mediasum_dev_doc_id_group_final_dual_summaries_manual_final_dataset.csv\")\n",
    "#answers_df.head()\n",
    "interview_df = pd.read_csv(\"./tofueval_dataset/mediasum_dev_doc_complete_final.csv\")\n",
    "#interview_df.head()\n",
    "\n",
    "result = pd.merge(answers_df, interview_df, on=\"doc_id\")\n",
    "final_dataset = result[[\"doc_id\", \"topic\", \"summ_sent_incorrect_original\", \"summ_sent_correct_manual\", \"exp\", \"type\", \"source\"]]\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ffe9b-db2b-4022-aa27-f7b15fd03b79",
   "metadata": {},
   "source": [
    "### <a name=\"8\">LLM Consultancy: 1 expert LLM consulting for 2nd summary , 1 naive judge</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018a5d1-42bb-473b-9e22-57ceeb2c2cce",
   "metadata": {},
   "source": [
    "LLM Consultancy with Claude (defending False) as Consultant for Answer B (incorrect) and Claude as Judge for 3 rounds.\n",
    "\n",
    "In this experiment we choose Claude v3(Sonnet) to act as a LLM consultant always picking Answer B(Ground Truth:False Answer). Claude also outputs its rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of these rounds, Claude as a judge adjudicates whether Claude's rationale is correct and if answer A is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f2429-9027-4f1d-ba56-dd7587d90dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    claude_defending_summary=answer_b\n",
    "    claude_opposing_summary=answer_a\n",
    "    delete_file(f\"./transcripts/full_transcript_consultancy_{debate_id}.log\")\n",
    "\n",
    "    logger.info(f\"-------------Consultancy Debate_id {debate_id}-------------------\")\n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        logger.info(f\"START Consultancy with Claude Debate_id {debate_id} Round #{round_number + 1} >>>>>> \\n\")\n",
    "\n",
    "        consultancy_response = invoke_claude_v3(debate_id = debate_id, \n",
    "                         round_number = round_number + 1,\n",
    "                         question=question,\n",
    "                         summary_defending = claude_defending_summary, \n",
    "                         summary_opposing = claude_opposing_summary, \n",
    "                         complete_interview = complete_interview_transcript,\n",
    "                         debate=False\n",
    "                         )\n",
    "        logger.info(f\"END Consultancy with Claude Debate_id {debate_id} Round #{round_number + 1} >>>>>> \\n\")\n",
    "    print(f\"=========== Consultancy debate_id {debate_id} Round #1..{round_number + 1} ======= saved in ./transcripts/full_transcript_consultancy_{debate_id}.log\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65d255d-37a5-4129-9f55-392235ddc318",
   "metadata": {},
   "source": [
    "### Judge decision for LLM Consulting on 2nd summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b8240-d797-4f7b-8573-5b260f6c5a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "consultancy_judge_regular_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------CONSULTANCY JUDGE Debate_id {debate_id}-------------------\")\n",
    "\n",
    "\n",
    "    judge_response = invoke_claude_judge_consultant(debate_id = debate_id,\n",
    "                                  question = question, \n",
    "                                  answer_a = answer_a,\n",
    "                                  answer_b = answer_b)\n",
    "    consultancy_judge_regular_answers.append(extract_final_answer(judge_response, flipped=False))\n",
    "    logger.info(f\" >>>>> invoke_titan_judge_consultant - judge_response  >>>>> {judge_response}\")\n",
    "    # Print the final response \n",
    "    format_final_response(debate_id, \n",
    "                          round_num=1, \n",
    "                          question=question, \n",
    "                          answer_a=answer_a, \n",
    "                          answer_b=answer_b, \n",
    "                          judge_response=judge_response)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f1ca2a-51a0-454b-b582-bfa6a9153b6e",
   "metadata": {},
   "source": [
    "### <a name=\"9\">LLM Consultancy: 1 expert LLM consulting for 1st summary, 1 naive judge</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8ef50-b9b6-4daf-b9f7-cc814fb2e004",
   "metadata": {},
   "source": [
    "**FLIPPED LLM CONSULTANCY** with Claude (defending True) as Consultant and Titan as Judge:\n",
    "\n",
    "\n",
    "In this experiment we choose Claude v3(Sonnet) to act as a LLM consultant always picking Answer A(Ground Truth:True Answer). Claude also outputs its rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of these rounds, Claude as a judge adjudicates whether Claude's rationale is correct and if answer A is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a583ba-fb01-4fa3-83cc-3e6754cd6280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    claude_defending_summary=answer_a\n",
    "    claude_opposing_summary=answer_b\n",
    "    \n",
    "    logger.info(f\"-------------Consultancy Flipped Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    #### Consultancy Claude - defending true - 3 rounds\n",
    "    delete_file(f\"./transcripts/full_transcript_consultancy_{debate_id}{FLIPPED_FILE_SUFFIX}.log\")\n",
    "    \n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        logger.info(f\"START Flipped Consultancy with Claude Round #{round_number + 1} >>>>>> \\n\")\n",
    "\n",
    "        consultancy_response = invoke_claude_v3(debate_id = debate_id + FLIPPED_FILE_SUFFIX, \n",
    "                         round_number = round_number + 1,\n",
    "                         question=question,\n",
    "                         summary_defending = claude_defending_summary, \n",
    "                         summary_opposing = claude_opposing_summary, \n",
    "                         complete_interview = complete_interview_transcript,\n",
    "                         debate=False\n",
    "                         )\n",
    "        logger.info(f\" >>>>> consultancy_response Round #{round_number + 1} >>>>> {consultancy_response}\")\n",
    "        logger.info(f\"END Flipped Consultancy with Claude debate_id {debate_id}  Round #{round_number + 1} >>>>>> \\n\")\n",
    "    print(f\"=========== END OF Flipped Consultancy debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47aae2f-46bd-4f7b-a569-d7a5c0061518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "consultancy_judge_flipped_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------CONSULTANCY Flipped JUDGE Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    time.sleep(4) # sleep 4 seconds to fix timeout errors\n",
    "    judge_response = invoke_claude_judge_consultant(debate_id = debate_id + FLIPPED_FILE_SUFFIX,\n",
    "                                  question = question, \n",
    "                                  answer_a = answer_a,\n",
    "                                  answer_b = answer_b)\n",
    "\n",
    "    logger.info(f\" >>>>> Flipped invoke_titan_judge_consultant - judge_response  >>>>> {judge_response}\")\n",
    "    consultancy_judge_flipped_answers.append(extract_final_answer(judge_response, flipped=False))\n",
    "    \n",
    "    # Print the final response \n",
    "    format_final_response(debate_id, \n",
    "                          round_num=1, \n",
    "                          question=question, \n",
    "                          answer_a=answer_a, \n",
    "                          answer_b=answer_b, \n",
    "                          judge_response=judge_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d4dd10-f4f2-4d9f-b439-6e07c26833a7",
   "metadata": {},
   "source": [
    "### <a name=\"8\">Accuracy of LLM Consultancy</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf50eb-f514-4d70-86f6-e6fa8eb50cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "consultancy_judge_regular_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a81b7-4feb-4243-a62e-153d14121b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "consultancy_judge_flipped_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c28c3b-fa91-458b-a0d6-b70b8d22f6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_consultant_judge = find_num_matching_elements(consultancy_judge_regular_answers, consultancy_judge_flipped_answers)/total_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b2914-3dcd-438c-b63c-919f58464d14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_consultant_judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335b912-7541-43be-9e9c-186d5100dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "results_dict = {\"accuracy_consultant_judge\" : accuracy_consultant_judge}\n",
    "save_each_experiment_result(results_dict)\n",
    "print(\"notebook results saved in results folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fbd255-a593-438f-a44f-fce489056f26",
   "metadata": {},
   "source": [
    "## <a name=\"14\">Compare Accuracies across experiments/methods.</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97c205-fa79-4c09-b115-0cd3e9cae864",
   "metadata": {},
   "source": [
    "Here we compare the accuracies of each method/experiment to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39472b99-25dc-4d47-8242-450df4c13559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_naive_judge = get_each_experiment_result(\"accuracy_naive_judge\")\n",
    "accuracy_expert_judge = get_each_experiment_result(\"accuracy_expert_judge\")\n",
    "\n",
    "final_accuracy_comparison_judge_and_consultant(\n",
    "    accuracy_naive_judge = accuracy_naive_judge,\n",
    "    accuracy_expert_judge = accuracy_expert_judge,\n",
    "    accuracy_consultant_judge = accuracy_consultant_judge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e751a8d-fd46-4192-b100-655e8342ffe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the plot\n",
    "%matplotlib inline\n",
    "x_values = [ \"Naive Judge\", \"Expert Judge\", \"LLM Consultant\"]\n",
    "y_values = [ accuracy_naive_judge, accuracy_expert_judge, accuracy_consultant_judge]\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Compare Accuracies across experiments')\n",
    "plt.xlabel('Experiment Type')\n",
    "plt.ylabel('Accuracy')\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
