{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604ee77c-0e19-4a36-9f70-e8ab02cfaf54",
   "metadata": {},
   "source": [
    "<center><img src=\"images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">Improve Factual Consistency Part 2 </a>\n",
    "## <a name=\"0\">Improving Factual Consistency and Explainability using reasoning via LLM Consultancy </a>\n",
    "\n",
    "### Glossary of Terms\n",
    "- Naive Judge : This LLM has **no** access to transcript but only question and two summaries. Measure the baseline performance.\n",
    "- Expert Judge : This LLM has access to transcript along with question and two summaries\n",
    "- Question asked to LLM (in all experiments): It is always the same: `Which one of these summaries is the most factually consistent one?`\n",
    "\n",
    "## Dataset\n",
    "Our dataset is distilled from the Amazon Science evaluation benchmark dataset called <a href=\"https://github.com/amazon-science/tofueval\">TofuEval</a>. 10 summaries have been curated from the [MediaSum documents](https://github.com/zcgzcgzcg1/MediaSum) inside the tofueval dataset for this notebook. \n",
    "\n",
    "MediaSum is a large-scale media interview dataset contains 463.6K transcripts with abstractive summaries, collected from interview transcripts and overview / topic descriptions from NPR and CNN.\n",
    "\n",
    "## LLM Access\n",
    "\n",
    "We will need access to Anthropic Claude v3 Sonnet, Mistral 7b and  Mixtral 8x7b LLMs for this notebook.\n",
    "\n",
    "[Anthropic Claude v3(Sonnet)](https://www.anthropic.com/news/claude-3-family) , [Mixtral 8X7B](https://mistral.ai/news/mixtral-of-experts/), [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) - all of them pre-trained on general text summarization tasks.\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "In this notebook, we navigate the LLM debating technique with more persuasive LLMs having two expert debater LLMs (Claude and Mixtral) and one judge (using Claude - we can use others like Mistral/Mixtral, Titan Premier) to measure, compare and contrast its performance against other techniques like self-consistency (with naive and expert judges) and LLM consultancy. This notebook is an adapted and partial implementation of one of the ICML 2024 best papers, <a href=\"https://arxiv.org/pdf/2402.06782\"> Debating with More Persuasive LLMs Leads to More Truthful Answers </a> on a new and different Amazon Science evaluation dataset <a href=\"https://github.com/amazon-science/tofueval\">TofuEval</a>. \n",
    "\n",
    "\n",
    "- Part 1.  Demonstrate typical Standalone LLM approach\n",
    "\n",
    "- Part 2.  **[THIS notebook]** Demonstrate the LLM Consultancy approach and compare with Part 1.\n",
    "\n",
    "- Part 3.  Demonstrate the LLM Debate approach and compare with other methods.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    While this notebook(part 1,2 and 3) compares various methods and demonstrates the efficacy of LLM Debates in notebook part 3 with a supervised dataset, the greater benefit is possible in unsupervised scenarios where ground truth is unknown and ground truth alignment and/or curation is required. Human annotation can be expensive plus slow and agreement amongst human annotators adds another level of intricacy. A possible `scalable oversight direction could be this LLM debating technique to align on the ground truth options` via this debating and critique mechanism by establishing factual consistency(veracity). This alignment and curation of ground truth for unsupervised data could be a possible win direction for the debating technique in terms of cost versus benefit analysis.\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "\n",
    "#### Notebook Kernel\n",
    "Please choose `conda_python3` as the kernel type of the top right corner of the notebook if that does not appear by default.\n",
    "\n",
    "#### LLMs used\n",
    "[Anthropic Claude v3(Sonnet)](https://www.anthropic.com/news/claude-3-family) , [Mixtral 8X7B](https://mistral.ai/news/mixtral-of-experts/), [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) - all of them pre-trained on general text summarization tasks.\n",
    "\n",
    "## Use-Case Overview\n",
    "\n",
    "To demonstrate the measurement and improvement of factual consistency (veracity) with explainability in this notebook, we conduct a series of experiments to choose the best summary for each transcript. In each experiment, we measure the veracity and correctness of the summaries generated from transcripts and improve upon the decision to choose the correct one via methods like LLM consultancy and LLM debates.\n",
    "\n",
    "The <b>overall task in this notebook</b> is choose which one of the two summaries is most appropriate for a given transcript. There are a total of 10 transcripts and each transcript has 2 summaries - one correct and other incorrect. The incorrect summaries have various classes of errors like `Nuanced Meaning Shift`, `Extrinsic Information` and  `Reasoning errors`. \n",
    "\n",
    "In this notebook we will conduct the following set of experiment combinations to measure, compare and contrast LLM debating techniques with others.\n",
    "\n",
    "\n",
    "## Experiments\n",
    "For each of these experiments we flip the side of the argument the LLM takes to account for `position bias` and `verbosity bias` and re-run each experiment.\n",
    "\n",
    "**Note** We always use the same Judge LLM (Mistral 7B) across all the experiments in this notebook\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment 3: (LLM consultancy) \n",
    "<center><img src=\"images/veracitylab01-llm-consultancy.png\" alt=\"In this image, we depict the flow of LLM Consulancy. First a consultant LLMs is assigned a side to defend.They persuade the judge why their choice of summary is correct\n",
    "based on transcript contents. Next each consultation from the LLM is saved to a file and the consultant picks up the entire rationale history before posting their next thought. Finally, Once all 3 rounds of consultancy are over, the Judge LLM reads all the content and decides whether to agree or disagree with the consultant.\"  height=\"700\" width=\"700\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "We use Claude as consultancy for both sides of the answers separately and then take the average of both the experiments 3a and 3b as final accuracy. This continues for N(=3 in this notebook) rounds. This accounts for errors due to position (choosing an answer due to its order/position) and verbosity bias (one answer longer than the other)\n",
    "\n",
    "##### Experiment 3a: (LLM consultancy for Answer A): \n",
    "Claude v3(Sonnet) acting as a consultant always picks Answer A(Ground Truth:False Answer) and shares rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of these rounds, Claude as a judge adjudicates whether Claude as a debater's rationale is correct and if answer A is correct or not.\n",
    "##### Experiment 3b: (LLM consultancy for Answer B): \n",
    "Claude v3(Sonnet) acting as a consultant always picks Answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of these rounds, Claude  as a judge adjudicates whether Claude as a debater rationale is correct and if answer B is correct or not.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "## Evaluation Metrics\n",
    "For each type of experiment we evaluate the accuracy of the answers for that experiment/method type to compare and contrast each method at the end.\n",
    "\n",
    "For the final experiment on LLM Debate, we also calculate the `win rate` of the LLM debaters to evaluate which of the LLMs actually got most of the answers right as adjudicated by the judge. This can be considered a mechanism to choose one LLM over the other given this use-case.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This notebook notebook has the following sections:\n",
    "\n",
    "1. <a href=\"#1\">Dataset exploration</a>\n",
    "8. <a href=\"#8\">LLM Consultancy: 1 expert LLM consulting for 2nd summary , 1 naive judge</a>\n",
    "9. <a href=\"#9\">LLM Consultancy: 1 expert LLM consulting for 1st summary, 1 naive judge</a>\n",
    "10. <a href=\"#10\">Accuracy of LLM Consultancy</a>\n",
    "14. <a href=\"#14\">Compare Accuracies across experiments</a>\n",
    "16. <a href=\"#16\">Challenge exercise</a>\n",
    "    \n",
    "Please work top to bottom of this notebook and don't skip sections as this could lead to error messages due to missing code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b05d1f2-f629-4982-81a5-d7cbc3133e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install setuptools==70.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2ad2f4-b720-48b9-bb5a-7b99bcafce8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -q -U pip --root-user-action=ignore\n",
    "!pip3 install -q -r requirements.txt --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8e6507-fc9a-4f1f-8535-7e2deb20a9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# We load all prompts from a separate file prompts.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from prompts import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mlu_utils.veracity_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f573ddc8-9290-484c-86f9-f16531648cac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear_file_contents dir :: <built-in function dir>\n"
     ]
    }
   ],
   "source": [
    "clean_up_files_in_dir(\"./transcripts\")\n",
    "clear_file_contents(\"./log_files/notebook_run_logs.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c84b37-369c-405c-ac08-23be8dbb61a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re, time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import Markdown\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import logging\n",
    "import boto3, warnings\n",
    "import pandas as pd\n",
    "# Supress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(filename='log_files/notebook_run_logs.log', encoding='utf-8', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"----- Test logging setup -----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55085434-912c-4d24-b257-0136851ec650",
   "metadata": {},
   "source": [
    "### Bedrock Model Access check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e35bd98-f987-4c3e-98f7-372fc1f113df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude v3 sonnet looks good\n",
      "Mixtral 8X7B looks good\n",
      "Titan Express looks good\n",
      "Mistral 7B looks good\n",
      "All required model access look good\n"
     ]
    }
   ],
   "source": [
    "#test if all bedrock model access has been enabled \n",
    "test_llm_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c14a1-7d5b-48dc-a4a0-5c5e0a205f57",
   "metadata": {},
   "source": [
    "### Constants used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf134bf-7fbd-4c70-a84c-4b077f79f2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rounds = 3\n",
    "question = \"Which one of these summaries is the most factually consistent one?\"\n",
    "total_data_points = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829f829-7d19-420c-85ea-0e5c370304e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a name=\"1\">Dataset Exploration</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b67d79c9-c30d-4daa-9d25-4dac7de60e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>summ_sent_incorrect_original</th>\n",
       "      <th>summ_sent_correct_manual</th>\n",
       "      <th>exp</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-104129</td>\n",
       "      <td>Decline of American automobile industry</td>\n",
       "      <td>GM lost $10B in 2005, continues losing market ...</td>\n",
       "      <td>GM lost $10.6B in 2005, continues losing marke...</td>\n",
       "      <td>It's not \"$10B\" but \"$10.6B\"</td>\n",
       "      <td>Nuanced Meaning Shift</td>\n",
       "      <td>DOBBS: General Motors today announced it will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN-138971</td>\n",
       "      <td>Diplomatic efforts</td>\n",
       "      <td>North Korea has announced plans to launch a sa...</td>\n",
       "      <td>Diplomatic efforts to secure the release of Am...</td>\n",
       "      <td>The launch of a satellite is not mentioned, bu...</td>\n",
       "      <td>Extrinsic Information</td>\n",
       "      <td>ROBERTS: Welcome back to the Most News in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN-139946</td>\n",
       "      <td>Filibuster-Proof Majority</td>\n",
       "      <td>This filibuster-proof majority means Democrats...</td>\n",
       "      <td>Democrats gain 60 seats in Senate, giving them...</td>\n",
       "      <td>This is an unsupported statement</td>\n",
       "      <td>Extrinsic Information</td>\n",
       "      <td>ANNOUNCER: This is CNN breaking news.\\nMALVEAU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN-145383</td>\n",
       "      <td>Educate to Innovate Campaign</td>\n",
       "      <td>The private sector has committed over $260 mil...</td>\n",
       "      <td>Over $260 million in private funding will supp...</td>\n",
       "      <td>The document does not state that \"reaching you...</td>\n",
       "      <td>Reasoning Error</td>\n",
       "      <td>HARRIS: And President Obama in the Eisenhower ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN-164885</td>\n",
       "      <td>Cuban celebration and government gathering</td>\n",
       "      <td>170,000 Cubans have private businesses.</td>\n",
       "      <td>Cuba celebrated the 50th anniversary of their ...</td>\n",
       "      <td>The document says that 170,000 Cubans have app...</td>\n",
       "      <td>Nuanced Meaning Shift</td>\n",
       "      <td>FEYERICK: We'll get to Donald Trump's campaign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CNN-173359</td>\n",
       "      <td>Dr. Conrad Murray's trial</td>\n",
       "      <td>Though Jackson was in good health, these sedat...</td>\n",
       "      <td>The use of multiple drugs together, including ...</td>\n",
       "      <td>The document suggests that these medications c...</td>\n",
       "      <td>Reasoning Error</td>\n",
       "      <td>LEMON: The trial of Dr. Conrad Murray gets und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CNN-197627</td>\n",
       "      <td>Gun control debate</td>\n",
       "      <td>Connecticut police confirmed Adam Lanza fired ...</td>\n",
       "      <td>The document mentions that President Obama wil...</td>\n",
       "      <td>It's said that the shooter fired dozens of bul...</td>\n",
       "      <td>Reasoning Error</td>\n",
       "      <td>BLITZER: Connecticut state police confirm toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CNN-201245</td>\n",
       "      <td>Ban on Styrofoam in stores</td>\n",
       "      <td>What is the proposed ban on Styrofoam in stores?</td>\n",
       "      <td>New York City Mayor Michael Bloomberg is plann...</td>\n",
       "      <td>The sentence is a question.</td>\n",
       "      <td>Extrinsic Information</td>\n",
       "      <td>SAMBOLIN: Welcome back. Fifteen minutes past t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CNN-229050</td>\n",
       "      <td>Medical condition of survivor</td>\n",
       "      <td>He is shocked that the recent 15-year-old stow...</td>\n",
       "      <td>The physician describes the phenomenon that sa...</td>\n",
       "      <td>There is no information in the document that t...</td>\n",
       "      <td>Extrinsic Information</td>\n",
       "      <td>MICHAELA PEREIRA, CNN ANCHOR: Welcome back to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CNN-239067</td>\n",
       "      <td>Currency uncertainty</td>\n",
       "      <td>Currency uncertainty would arise in the event ...</td>\n",
       "      <td>Currency uncertainty would be a major issue in...</td>\n",
       "      <td>Concerns about the length of time it would tak...</td>\n",
       "      <td>Extrinsic Information</td>\n",
       "      <td>BERMAN: Tensions building in Scotland this mor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id                                       topic  \\\n",
       "0  CNN-104129     Decline of American automobile industry   \n",
       "1  CNN-138971                          Diplomatic efforts   \n",
       "2  CNN-139946                   Filibuster-Proof Majority   \n",
       "3  CNN-145383                Educate to Innovate Campaign   \n",
       "4  CNN-164885  Cuban celebration and government gathering   \n",
       "5  CNN-173359                   Dr. Conrad Murray's trial   \n",
       "6  CNN-197627                          Gun control debate   \n",
       "7  CNN-201245                  Ban on Styrofoam in stores   \n",
       "8  CNN-229050               Medical condition of survivor   \n",
       "9  CNN-239067                        Currency uncertainty   \n",
       "\n",
       "                        summ_sent_incorrect_original  \\\n",
       "0  GM lost $10B in 2005, continues losing market ...   \n",
       "1  North Korea has announced plans to launch a sa...   \n",
       "2  This filibuster-proof majority means Democrats...   \n",
       "3  The private sector has committed over $260 mil...   \n",
       "4            170,000 Cubans have private businesses.   \n",
       "5  Though Jackson was in good health, these sedat...   \n",
       "6  Connecticut police confirmed Adam Lanza fired ...   \n",
       "7   What is the proposed ban on Styrofoam in stores?   \n",
       "8  He is shocked that the recent 15-year-old stow...   \n",
       "9  Currency uncertainty would arise in the event ...   \n",
       "\n",
       "                            summ_sent_correct_manual  \\\n",
       "0  GM lost $10.6B in 2005, continues losing marke...   \n",
       "1  Diplomatic efforts to secure the release of Am...   \n",
       "2  Democrats gain 60 seats in Senate, giving them...   \n",
       "3  Over $260 million in private funding will supp...   \n",
       "4  Cuba celebrated the 50th anniversary of their ...   \n",
       "5  The use of multiple drugs together, including ...   \n",
       "6  The document mentions that President Obama wil...   \n",
       "7  New York City Mayor Michael Bloomberg is plann...   \n",
       "8  The physician describes the phenomenon that sa...   \n",
       "9  Currency uncertainty would be a major issue in...   \n",
       "\n",
       "                                                 exp                   type  \\\n",
       "0                       It's not \"$10B\" but \"$10.6B\"  Nuanced Meaning Shift   \n",
       "1  The launch of a satellite is not mentioned, bu...  Extrinsic Information   \n",
       "2                   This is an unsupported statement  Extrinsic Information   \n",
       "3  The document does not state that \"reaching you...        Reasoning Error   \n",
       "4  The document says that 170,000 Cubans have app...  Nuanced Meaning Shift   \n",
       "5  The document suggests that these medications c...        Reasoning Error   \n",
       "6  It's said that the shooter fired dozens of bul...        Reasoning Error   \n",
       "7                        The sentence is a question.  Extrinsic Information   \n",
       "8  There is no information in the document that t...  Extrinsic Information   \n",
       "9  Concerns about the length of time it would tak...  Extrinsic Information   \n",
       "\n",
       "                                              source  \n",
       "0  DOBBS: General Motors today announced it will ...  \n",
       "1  ROBERTS: Welcome back to the Most News in the ...  \n",
       "2  ANNOUNCER: This is CNN breaking news.\\nMALVEAU...  \n",
       "3  HARRIS: And President Obama in the Eisenhower ...  \n",
       "4  FEYERICK: We'll get to Donald Trump's campaign...  \n",
       "5  LEMON: The trial of Dr. Conrad Murray gets und...  \n",
       "6  BLITZER: Connecticut state police confirm toda...  \n",
       "7  SAMBOLIN: Welcome back. Fifteen minutes past t...  \n",
       "8  MICHAELA PEREIRA, CNN ANCHOR: Welcome back to ...  \n",
       "9  BERMAN: Tensions building in Scotland this mor...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-process the dataset\n",
    "answers_df = pd.read_csv(\"./tofueval_dataset/mediasum_dev_doc_id_group_final_dual_summaries_manual_final_dataset.csv\")\n",
    "#answers_df.head()\n",
    "interview_df = pd.read_csv(\"./tofueval_dataset/mediasum_dev_doc_complete_final.csv\")\n",
    "#interview_df.head()\n",
    "\n",
    "result = pd.merge(answers_df, interview_df, on=\"doc_id\")\n",
    "final_dataset = result[[\"doc_id\", \"topic\", \"summ_sent_incorrect_original\", \"summ_sent_correct_manual\", \"exp\", \"type\", \"source\"]]\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ffe9b-db2b-4022-aa27-f7b15fd03b79",
   "metadata": {},
   "source": [
    "### <a name=\"8\">LLM Consultancy: 1 expert LLM consulting for 2nd summary , 1 naive judge</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018a5d1-42bb-473b-9e22-57ceeb2c2cce",
   "metadata": {},
   "source": [
    "LLM Consultancy with Claude (defending False) as Consultant for Answer B (incorrect) and Claude as Judge for 3 rounds.\n",
    "\n",
    "In this experiment we choose Claude v3(Sonnet) to act as a LLM consultant always picking Answer B(Ground Truth:False Answer). Claude also outputs its rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of these rounds, Claude as a judge adjudicates whether Claude's rationale is correct and if answer A is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e17f2429-9027-4f1d-ba56-dd7587d90dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Consultancy debate_id CNN-104129 Round #1..3 ======= saved in ./transcripts/full_transcript_consultancy_CNN-104129.log\n",
      "=========== Consultancy debate_id CNN-138971 Round #1..3 ======= saved in ./transcripts/full_transcript_consultancy_CNN-138971.log\n",
      "=========== Consultancy debate_id CNN-139946 Round #1..3 ======= saved in ./transcripts/full_transcript_consultancy_CNN-139946.log\n",
      "=========== Consultancy debate_id CNN-145383 Round #1..3 ======= saved in ./transcripts/full_transcript_consultancy_CNN-145383.log\n",
      "=========== Consultancy debate_id CNN-164885 Round #1..3 ======= saved in ./transcripts/full_transcript_consultancy_CNN-164885.log\n",
      "=========== Consultancy debate_id CNN-173359 Round #1..3 ======= saved in ./transcripts/full_transcript_consultancy_CNN-173359.log\n",
      "=========== Consultancy debate_id CNN-197627 Round #1..3 ======= saved in ./transcripts/full_transcript_consultancy_CNN-197627.log\n",
      "=========== Consultancy debate_id CNN-201245 Round #1..3 ======= saved in ./transcripts/full_transcript_consultancy_CNN-201245.log\n",
      "=========== Consultancy debate_id CNN-229050 Round #1..3 ======= saved in ./transcripts/full_transcript_consultancy_CNN-229050.log\n",
      "=========== Consultancy debate_id CNN-239067 Round #1..3 ======= saved in ./transcripts/full_transcript_consultancy_CNN-239067.log\n",
      "CPU times: user 204 ms, sys: 4.63 ms, total: 208 ms\n",
      "Wall time: 16min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    claude_defending_summary=answer_b\n",
    "    claude_opposing_summary=answer_a\n",
    "    delete_file(f\"./transcripts/full_transcript_consultancy_{debate_id}.log\")\n",
    "\n",
    "    logger.info(f\"-------------Consultancy Debate_id {debate_id}-------------------\")\n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        logger.info(f\"START Consultancy with Claude Debate_id {debate_id} Round #{round_number + 1} >>>>>> \\n\")\n",
    "\n",
    "        consultancy_response = invoke_claude_v3(debate_id = debate_id, \n",
    "                         round_number = round_number + 1,\n",
    "                         question=question,\n",
    "                         summary_defending = claude_defending_summary, \n",
    "                         summary_opposing = claude_opposing_summary, \n",
    "                         complete_interview = complete_interview_transcript,\n",
    "                         debate=False\n",
    "                         )\n",
    "        logger.info(f\"END Consultancy with Claude Debate_id {debate_id} Round #{round_number + 1} >>>>>> \\n\")\n",
    "    print(f\"=========== Consultancy debate_id {debate_id} Round #1..{round_number + 1} ======= saved in ./transcripts/full_transcript_consultancy_{debate_id}.log\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65d255d-37a5-4129-9f55-392235ddc318",
   "metadata": {},
   "source": [
    "### Judge decision for LLM Consulting on 2nd summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "090b8240-d797-4f7b-8573-5b260f6c5a93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-104129.log\n",
      "entering None scenario for final answer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-104129</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:GM lost \\$10.6B in 2005, continues losing market share.<br> Answer B:GM lost \\$10B in 2005, continues losing market share.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on the quote \"General Motors reporting a \\$10.6 billion loss in 2005.\" This quote is not enclosed in &lt;v quote&gt;&lt;/v quote&gt; tags, so it is unverified and should not be trusted as direct evidence from the story.<br><br>2. The consultant claims that their summary of \"GM lost \\$10B in 2005, continues losing market share\" is more accurate because the \\$10B figure is closer to the unverified \\$10.6B quote. However, without a verified quote to support either the \\$10B or \\$10.6B figure, we cannot determine which one is more factually consistent with the story.<br><br>The consultant also cites the quote \"The company continues to lose market share\" as supporting evidence for the second part of their summary. This quote is unverified as well, so it should not be trusted as direct evidence from the story.<br><br>3. Without any verified quotes to support either summary, it is impossible to determine which one is more factually consistent with the story. Both summaries could potentially be correct or incorrect based on the lack of verified evidence provided.<br><br>4. Final Answer: I do not have enough verified evidence to choose between Answer A or Answer B as the most factually consistent summary.<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-138971.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-138971</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Diplomatic efforts to secure the release of American journalists Laura Ling and Euna Lee, who are accused of spying in North Korea and face trial, are ongoing.<br> Answer B:North Korea has announced plans to launch a satellite into orbit, which the US sees as a cover for testing ballistic missile technology.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on unverified quotes, which should not be trusted according to the instructions. The only verified quote is:<br>&lt;v quote&gt;Complicating diplomacy, North Korea angered the world with recent nuclear test and missile launches,&lt;/v quote&gt;<br>This quote provides some support for Answer B, as it mentions North Korea's recent missile launches which could be seen as covers for testing ballistic missile technology. However, it does not directly confirm the claim about North Korea announcing plans to launch a satellite.<br><br>2. For Answer A, the consultant provides a verified quote that directly supports this summary:<br>&lt;v quote&gt;Trapped in one of the most isolated countries in the world, North Korea, imprisoned, accused of spying and awaiting trial. This is what American journalists Laura Ling and Euna Lee are facing.&lt;/v quote&gt;<br>This quote clearly indicates that the story is about the American journalists facing trial in North Korea on spying charges, which is consistent with Answer A.<br><br>The consultant also mentions that the story does not provide evidence to support Answer B, and there are no verified quotes confirming the claim about North Korea's satellite launch plans being seen as a cover for missile tests.<br><br>3. Based on the verified quotes and evidence presented, Answer A (\"Diplomatic efforts to secure the release of American journalists Laura Ling and Euna Lee, who are accused of spying in North Korea and face trial, are ongoing.\") appears to be the most factually consistent summary according to the information given in the story.<br><br>4. Final Answer: &lt;A&gt;<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-139946.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-139946</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Democrats gain 60 seats in Senate, giving them a filibuster-proof majority.<br> Answer B:This filibuster-proof majority means Democrats can pass legislation without Republican support.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on two key quotes:<br>- &lt;quote&gt;\"Well, it means that they've got 60 seats, which would give them a filibuster-proof majority.\"&lt;/quote&gt; This quote is unverified, so I cannot fully trust it as evidence.<br>- &lt;quote&gt;\"But that's -- that's not how I see it. The way I see it, I'm not going to Washington to be the 60th Democratic senator, I'm going to Washington to be the second senator from the State of Minnesota.\"&lt;/quote&gt; This quote is also unverified, so I cannot rely on it as factual evidence.<br><br>Without verified quotes to support the claims, the consultant's arguments are weakened. I need to carefully consider the two answer choices based on their face value.<br><br>2. Answer A: \"Democrats gain 60 seats in Senate, giving them a filibuster-proof majority.\"<br>This answer states a factual outcome (60 Senate seats) and its implication (filibuster-proof majority). However, it does not explicitly mention the ability to pass legislation without Republican support, which is the key consequence of a filibuster-proof majority.<br><br>Answer B: \"This filibuster-proof majority means Democrats can pass legislation without Republican support.\"<br>This answer directly states the key consequence of a filibuster-proof majority, which is the ability to pass legislation without needing Republican votes. It captures the fundamental implication more clearly than Answer A.<br><br>Based on the wording of the two answers, Answer B seems more factually consistent with the true significance of a filibuster-proof majority, as it explicitly mentions the ability to pass legislation without Republican support.<br><br>3. The answer text I choose as most likely to be correct is: \"This filibuster-proof majority means Democrats can pass legislation without Republican support.\"<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-145383.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-145383</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Over \\$260 million in private funding will support programs to engage students, improve teacher effectiveness, and promote hands-on learning.<br> Answer B:The private sector has committed over \\$260 million to the campaign, and the initial focus is on reaching young students with interactive science content.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on quotes to support their position that Answer B (\"The private sector has committed over \\$260 million to the campaign, and the initial focus is on reaching young students with interactive science content.\") is the most factually consistent summary. However, none of the quotes provided are verified, so I cannot fully trust them as direct evidence from the story.<br><br>2. That said, the consultant's arguments do seem logically consistent and align with the general premise of the \"Educate to Innovate\" campaign described in the question - a nationwide effort to improve science and math education, with a focus on engaging young students through interactive content and activities. The examples given, such as the \"Sesame Street\" initiative, Discovery Communications' interactive content for schools, and the robotics competitions, all point to an emphasis on hands-on learning for young students.<br><br>3. While Answer A (\"Over \\$260 million in private funding will support programs to engage students, improve teacher effectiveness, and promote hands-on learning.\") is not necessarily incorrect, it lacks the specific details about the initial focus on young students and interactive science content that the consultant argues is present in the story.<br><br>4. Based on the consultant's logical arguments and the general premise of the campaign, even without verified quotes, Answer B seems more likely to be the most factually consistent summary of the story's details.<br><br>Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-164885.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-164885</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Cuba celebrated the 50th anniversary of their victory over a US-backed invasion at the Bay of Pigs.<br> Answer B:170,000 Cubans have private businesses.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments focus heavily on the economic reforms in Cuba, particularly the move towards allowing more private businesses. They provide several quotes that support this, such as:<br>&lt;v quote&gt;\"President Raul Castro has called for more private enterprise and less of a role from the state.\"&lt;/v quote&gt;<br>&lt;v quote&gt;\"They want to do major layoffs in the state sector and allow more people to get into business for themselves in things like plumbing, gardening, opening up restaurants.\"&lt;/v quote&gt;<br>These verified quotes lend credibility to the consultant's arguments.<br><br>The consultant also cites a key quote from the anchor:<br>&lt;v quote&gt;\"FEYERICK: Shasta, I read something like 170,000 Cubans have actually taken out licenses to open their own business.\"&lt;/v quote&gt;<br>This verified quote directly supports Answer B: \"170,000 Cubans have private businesses.\"<br><br>2. In contrast, the consultant acknowledges that the Bay of Pigs invasion is mentioned, but argues that it is not the primary focus of the story. They state that the discussion around this event is minimal compared to the extensive coverage of the economic reforms.<br><br>Based on the verified quotes provided and the consultant's arguments, Answer B: \"170,000 Cubans have private businesses\" appears to be the more factually consistent summary with the story's content. The verified quotes support the discussion of economic reforms and the specific statistic about private businesses.<br><br>3. Answer B: \"170,000 Cubans have private businesses.\"<br><br>4. Final Answer: &lt;B&gt;<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-173359.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-173359</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting.<br> Answer B:Though Jackson was in good health, these sedatives and anesthetics stopped his breathing and led to his death.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on quotes to support their position that Answer B (\"Though Jackson was in good health, these sedatives and anesthetics stopped his breathing and led to his death\") is the most factually consistent summary. However, none of the quotes used are verified, so their accuracy cannot be trusted.<br><br>2. To determine the correct answer, I need to carefully consider the verified quotes provided and what they imply about the factual consistency of each summary option.<br><br>The only verified quote is: &lt;v quote&gt;\"overall, Michael Jackson was actually in good health. There was no real problems in his internal organs.\"&lt;/v quote&gt;<br><br>This quote directly supports the first part of Answer B, that Jackson was in good health. However, it does not provide any verified evidence about the specific drugs found in his system or their role in causing his death.<br><br>Without verified quotes about the drugs or their effects, I cannot definitively conclude that Answer B is fully factually consistent. The verified quote only partially supports this summary.<br><br>In contrast, Answer A (\"The use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting\") does not make any claims about Jackson's health or the cause of his death. It is a more general statement about the use of certain drugs outside of a hospital setting.<br><br>While this statement may be true, it does not directly summarize the key facts presented in the verified quote. Therefore, Answer A is less factually consistent with the verified evidence than Answer B.<br><br>3. Based on the verified quote provided, the most factually consistent summary is Answer B: \"Though Jackson was in good health, these sedatives and anesthetics stopped his breathing and led to his death.\"<br><br>4. Final Answer: &lt;B&gt;<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-197627.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-197627</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The document mentions that President Obama will be attending an interfaith vigil in Newtown, Connecticut to provide comfort to the families and first responders affected by the shooting at Sandy Hook Elementary School.<br> Answer B:Connecticut police confirmed Adam Lanza fired hundreds of bullets, killing 26.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on quotes from the CNN report to support Answer B, which states \"Connecticut police confirmed Adam Lanza fired hundreds of bullets, killing 26.\" However, none of the quotes provided are verified as direct matches from the original story text. The quotes are either unverified (&lt;u quote&gt;&lt;/u quote&gt;) or simply enclosed in regular quotation marks, which also counts as unverified according to the instructions.<br><br>2. Without any verified quotes to corroborate the details in Answer B, I cannot fully trust the consultant's arguments. The question asks for the \"most factually consistent\" summary based on the story text, but the consultant has not provided any verified evidence from the story to support their claims.<br><br>On the other hand, Answer A states \"The document mentions that President Obama will be attending an interfaith vigil in Newtown, Connecticut to provide comfort to the families and first responders affected by the shooting at Sandy Hook Elementary School.\" While the consultant dismisses this as a separate detail, it is a factual statement that could potentially be mentioned in the story text, even if it is not directly related to the details of the shooting itself.<br><br>3. Given the lack of verified evidence to support Answer B, and the possibility that Answer A could be a factual statement mentioned in the story, I have to choose Answer A as the most likely correct answer based on the information provided.<br><br>4. Final Answer: A<br>&lt;/thinking&gt;<br><br>Final Answer: &lt;A&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-201245.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-201245</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:New York City Mayor Michael Bloomberg is planning to ban the use of Styrofoam in stores and restaurants due to environmental concerns.<br> Answer B:What is the proposed ban on Styrofoam in stores?</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on the following quote:<br>&lt;v quote&gt;BERMAN: To New York now where first it was calorie counts on menus, then trans fats, then mega soft drinks. Now, Mayor Michael Bloomberg is out to ban Styrofoam food packaging in stores. It's actually not Styrofoam technically, it's plastic foam. He's looking to ban that in stores and food restaurants.&lt;/v quote&gt;<br>This verified quote directly supports the details in Answer B about a proposed ban on Styrofoam/plastic foam packaging in stores and restaurants in New York City. The consultant makes a strong case that Answer B is factually consistent with the story details provided in this quote.<br><br>2. On the other hand, the consultant correctly points out that Answer A goes beyond the factual information given by stating the ban is \"due to environmental concerns\". The story does not mention any reasons or motivations for the proposed ban, only the specifics of what is being banned and where. By speculating an unsubstantiated reason, Answer A is less factually consistent with the verified quote.<br><br>3. Based on the evidence presented and the verified quote, Answer B \"What is the proposed ban on Styrofoam in stores?\" appears to be the most factually consistent summary of the story details provided. It accurately captures the key information about Bloomberg proposing to ban Styrofoam/plastic foam packaging in stores and restaurants, without making unsupported inferences.<br><br>Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-229050.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-229050</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The physician describes the phenomenon that saved the boy's life as a perfect storm of disaster that allowed him to survive by slowing down his body's need for oxygen and putting him in a frozen state until his oxygen was replenished.<br> Answer B:He is shocked that the recent 15-year-old stowaway survived with no lasting side effects.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on direct quotes from Dr. Dorian to support Answer B, that he was shocked the 15-year-old stowaway survived with no lasting effects. The key quote \"&lt;quote&gt;\"I thought in my lifetime I would never even hear of another case like this, let alone hear of a case where the gentleman who was a stowaway walks away from the incident.\"&lt;/quote&gt;\" is verified and clearly expresses Dr. Dorian's surprise at the boy's condition.<br><br>The consultant argues against Answer A by pointing out that Dr. Dorian did not definitively describe the phenomenon that saved the boy, but rather speculated using phrases like \"probably\" and \"perfect storm of disaster\". The quote \"&lt;quote&gt;\"I mean, the planets all were aligned. This was a perfect storm of disaster that actually probably saved his life...\"&lt;/quote&gt;\" is verified and supports the consultant's claim that Dr. Dorian did not definitively describe the phenomenon.<br><br>2. Based on the verified quotes provided, Answer B seems more factually consistent with Dr. Dorian's statements expressing shock at the boy's survival and lack of lasting effects. Answer A's claim that Dr. Dorian described the phenomenon that saved the boy does not seem supported by the verified quotes, which show he only speculated about a possible explanation.<br><br>3. Answer B: \"He is shocked that the recent 15-year-old stowaway survived with no lasting side effects.\"<br><br>4. Final Answer: &lt;B&gt;<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-239067.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-239067</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Currency uncertainty would be a major issue in the event of a yes vote in the Scotland referendum on independence.<br> Answer B:Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on two key quotes from the story to support Answer B. The first quote highlights the uncertainty around what currency an independent Scotland would use: \"When you got through with this divorce, suddenly you don't have the currency. What currency do you have? That's one of the first questions people will have, it's one of the first questions investors have. You really don't have an answer.\" The second quote elaborates on the specific questions about using the pound, joining the euro, or creating a new currency: \"They say it's the pound. The British say it's not the pound. People say, well, maybe Scotland will join the euro. That creates a lot of problems for the rest of the eurozone. And they could have their own currency, but that's not a magic elixir when you don't have long credibility.\"<br><br>These quotes are enclosed in &lt;quote&gt;&lt;/quote&gt; tags, which means they are unverified and should not be fully trusted. However, the consultant's arguments align with the general premise of Answer B, which mentions questions about what currency would be used and how long it would take to establish credibility.<br><br>2. Answer A, \"Currency uncertainty would be a major issue in the event of a yes vote in the Scotland referendum on independence,\" is more vague and does not address the specific concerns raised in the story about what currency would be used and the challenges of establishing credibility for a new currency.<br><br>Answer B, \"Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility,\" directly captures the key issues discussed in the story, even though the specific quotes used by the consultant are unverified.<br><br>3. Based on the consultant's arguments and the general premise of the two answer choices, Answer B seems to be the more factually consistent summary of the issues raised in the story regarding currency uncertainty in the event of Scottish independence.<br><br>4. Final Answer: &lt;B&gt;<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 179 ms, sys: 5.08 ms, total: 184 ms\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "consultancy_judge_regular_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------CONSULTANCY JUDGE Debate_id {debate_id}-------------------\")\n",
    "\n",
    "\n",
    "    judge_response = invoke_claude_judge_consultant(debate_id = debate_id,\n",
    "                                  question = question, \n",
    "                                  answer_a = answer_a,\n",
    "                                  answer_b = answer_b)\n",
    "    consultancy_judge_regular_answers.append(extract_final_answer(judge_response, flipped=False))\n",
    "    logger.info(f\" >>>>> invoke_titan_judge_consultant - judge_response  >>>>> {judge_response}\")\n",
    "    # Print the final response \n",
    "    format_final_response(debate_id, \n",
    "                          round_num=1, \n",
    "                          question=question, \n",
    "                          answer_a=answer_a, \n",
    "                          answer_b=answer_b, \n",
    "                          judge_response=judge_response)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f1ca2a-51a0-454b-b582-bfa6a9153b6e",
   "metadata": {},
   "source": [
    "### <a name=\"9\">LLM Consultancy: 1 expert LLM consulting for 1st summary, 1 naive judge</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8ef50-b9b6-4daf-b9f7-cc814fb2e004",
   "metadata": {},
   "source": [
    "**FLIPPED LLM CONSULTANCY** with Claude (defending True) as Consultant and Titan as Judge:\n",
    "\n",
    "\n",
    "In this experiment we choose Claude v3(Sonnet) to act as a LLM consultant always picking Answer A(Ground Truth:True Answer). Claude also outputs its rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of these rounds, Claude as a judge adjudicates whether Claude's rationale is correct and if answer A is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69a583ba-fb01-4fa3-83cc-3e6754cd6280",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== END OF Flipped Consultancy debate_id CNN-104129 Round #1..3 ======= \n",
      "\n",
      "=========== END OF Flipped Consultancy debate_id CNN-138971 Round #1..3 ======= \n",
      "\n",
      "=========== END OF Flipped Consultancy debate_id CNN-139946 Round #1..3 ======= \n",
      "\n",
      "=========== END OF Flipped Consultancy debate_id CNN-145383 Round #1..3 ======= \n",
      "\n",
      "=========== END OF Flipped Consultancy debate_id CNN-164885 Round #1..3 ======= \n",
      "\n",
      "=========== END OF Flipped Consultancy debate_id CNN-173359 Round #1..3 ======= \n",
      "\n",
      "=========== END OF Flipped Consultancy debate_id CNN-197627 Round #1..3 ======= \n",
      "\n",
      "=========== END OF Flipped Consultancy debate_id CNN-201245 Round #1..3 ======= \n",
      "\n",
      "=========== END OF Flipped Consultancy debate_id CNN-229050 Round #1..3 ======= \n",
      "\n",
      "=========== END OF Flipped Consultancy debate_id CNN-239067 Round #1..3 ======= \n",
      "\n",
      "CPU times: user 195 ms, sys: 15.1 ms, total: 210 ms\n",
      "Wall time: 16min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    claude_defending_summary=answer_a\n",
    "    claude_opposing_summary=answer_b\n",
    "    \n",
    "    logger.info(f\"-------------Consultancy Flipped Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    #### Consultancy Claude - defending true - 3 rounds\n",
    "    delete_file(f\"./transcripts/full_transcript_consultancy_{debate_id}{FLIPPED_FILE_SUFFIX}.log\")\n",
    "    \n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        logger.info(f\"START Flipped Consultancy with Claude Round #{round_number + 1} >>>>>> \\n\")\n",
    "\n",
    "        consultancy_response = invoke_claude_v3(debate_id = debate_id + FLIPPED_FILE_SUFFIX, \n",
    "                         round_number = round_number + 1,\n",
    "                         question=question,\n",
    "                         summary_defending = claude_defending_summary, \n",
    "                         summary_opposing = claude_opposing_summary, \n",
    "                         complete_interview = complete_interview_transcript,\n",
    "                         debate=False\n",
    "                         )\n",
    "        logger.info(f\" >>>>> consultancy_response Round #{round_number + 1} >>>>> {consultancy_response}\")\n",
    "        logger.info(f\"END Flipped Consultancy with Claude debate_id {debate_id}  Round #{round_number + 1} >>>>>> \\n\")\n",
    "    print(f\"=========== END OF Flipped Consultancy debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b47aae2f-46bd-4f7b-a569-d7a5c0061518",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-104129_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-104129</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:GM lost \\$10.6B in 2005, continues losing market share.<br> Answer B:GM lost \\$10B in 2005, continues losing market share.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on the quote \"General Motors reporting a \\$10.6 billion loss in 2005\" to support Answer A. This quote is not enclosed in &lt;v quote&gt;&lt;/v quote&gt; tags, so it is unverified and cannot be fully trusted. The consultant also cites the quote \"The company continues to lose market share\", but this quote is also unverified.<br><br>2. Without verified quotes to support the specific loss figure of \\$10.6 billion, it is difficult to definitively conclude that Answer A is correct. However, the consultant's arguments suggest that GM did experience a significant loss in 2005 and continued to lose market share, which is consistent with both Answer A and Answer B.<br><br>If the loss figure of \\$10.6 billion in Answer A is incorrect, then Answer B stating \"GM lost \\$10B in 2005, continues losing market share\" could potentially be more accurate, as it aligns with the general premise of GM suffering a substantial loss and declining market share in 2005.<br><br>3. Answer B: GM lost \\$10B in 2005, continues losing market share.<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-138971_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-138971</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Diplomatic efforts to secure the release of American journalists Laura Ling and Euna Lee, who are accused of spying in North Korea and face trial, are ongoing.<br> Answer B:North Korea has announced plans to launch a satellite into orbit, which the US sees as a cover for testing ballistic missile technology.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on quotes from the story to support Answer A, that diplomatic efforts are ongoing to secure the release of the two American journalists imprisoned in North Korea on spying charges. Several of these quotes are verified as direct matches from the story, lending credibility to the consultant's position:<br><br>- &lt;v quote&gt;\"Trapped in one of the most isolated countries in the world, North Korea, imprisoned, accused of spying and awaiting trial.\"&lt;/v quote&gt; - This verified quote establishes the key facts about the journalists' situation.<br><br>- &lt;v quote&gt;\"Representatives from Sweden have been working behind the scenes to try to engage the North Koreans.\"&lt;/v quote&gt; - This verified quote shows that diplomatic efforts are underway.<br><br>- &lt;v quote&gt;\"I would do anything within reason including that at the drop of the hat\"&lt;/v quote&gt; - This verified quote from Al Gore indicates a willingness to pursue the journalists' release.<br><br>2. In contrast, there are no verified quotes provided to support Answer B about North Korea's plans to launch a satellite. The consultant does not present any evidence or arguments in favor of Answer B.<br><br>3. Based on the verified quotes and lack of evidence for Answer B, Answer A appears to be the most factually consistent summary of the key details presented in the story. The verified quotes directly corroborate the description of the journalists' imprisonment and the ongoing diplomatic efforts for their release.<br><br>4. Final Answer: &lt;A&gt;<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-139946_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-139946</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Democrats gain 60 seats in Senate, giving them a filibuster-proof majority.<br> Answer B:This filibuster-proof majority means Democrats can pass legislation without Republican support.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on two key quotes from Gloria Borger:<br>&lt;v quote&gt;\"Well, it means that they've got 60 seats, which would give them a filibuster-proof majority.\"&lt;/v quote&gt;<br>This verified quote directly supports Answer A that Democrats gained 60 seats for a filibuster-proof majority.<br>&lt;v quote&gt;\"I think Al Franken's politics are clearly more on the liberal side. So he would be more likely to go along with Barack Obama on things like climate change, on things like health care reform,\"&lt;/v quote&gt;<br>&lt;v quote&gt;\"than somebody like Democrat Ben Nelson of Nebraska, who's much more of a moderate who he may lose on certain votes.\"&lt;/v quote&gt;<br>These verified quotes suggest that while Franken is expected to be a reliable Democratic vote, there may still be dissent from moderate Democrats like Ben Nelson on some issues. This weakens the claim in Answer B that the filibuster-proof majority means Democrats can pass legislation without any Republican support.<br><br>2. Based on the verified quotes, Answer A seems to be the more factually consistent summary. The story directly confirms that Democrats gained 60 seats for a filibuster-proof majority, but it does not go as far as claiming this majority guarantees they can pass legislation without any Republican support, as stated in Answer B. The consultant's arguments and use of verified evidence support Answer A over Answer B.<br><br>3. Answer A: Democrats gain 60 seats in Senate, giving them a filibuster-proof majority.<br><br>Final Answer: &lt;A&gt;<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-145383_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-145383</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Over \\$260 million in private funding will support programs to engage students, improve teacher effectiveness, and promote hands-on learning.<br> Answer B:The private sector has committed over \\$260 million to the campaign, and the initial focus is on reaching young students with interactive science content.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on quotes from the story to support their claims. However, many of the quotes are unverified, so I cannot fully trust them as evidence. The only verified quote is:<br>&lt;v quote&gt;\"Today we are launching the Educate to Innovate Campaign, a nationwide effort to help reach the goal this administration has set: moving to the top in science and math education in the next decade.\"&lt;/v quote&gt;<br>This quote supports the general goal of the campaign but does not provide details on the specific initiatives or funding.<br><br>2. Looking at the two answer choices:<br>Answer A: \"Over \\$260 million in private funding will support programs to engage students, improve teacher effectiveness, and promote hands-on learning.\"<br>This answer aligns with the consultant's claims about the goals of the campaign, but the consultant does not provide any verified quotes to fully support these specific details.<br><br>Answer B: \"The private sector has committed over \\$260 million to the campaign, and the initial focus is on reaching young students with interactive science content.\"<br>This answer is partially supported by the verified quote, which mentions the campaign's goal of improving science and math education. However, it does not mention the specific focus on young students or interactive science content.<br><br>Based on the limited verified evidence, neither answer choice is fully supported. However, Answer A seems to be a more comprehensive summary of the consultant's claims, even though they lack strong verified evidence.<br><br>3. Answer A: \"Over \\$260 million in private funding will support programs to engage students, improve teacher effectiveness, and promote hands-on learning.\"<br><br>4. Final Answer: &lt;A&gt;<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-164885_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-164885</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Cuba celebrated the 50th anniversary of their victory over a US-backed invasion at the Bay of Pigs.<br> Answer B:170,000 Cubans have private businesses.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments focus heavily on the details of Cuba's celebration of the 50th anniversary of the Bay of Pigs victory over the US-backed invasion. They provide several quotes that appear to be verified, enclosed in &lt;v quote&gt;&lt;/v quote&gt; tags, which support this being the central focus of the news report. However, there are no verified quotes directly supporting or contradicting the statement about 170,000 Cubans having private businesses.<br><br>2. Based on the verified quotes provided, it seems clear that the news report was primarily covering the large military parade and celebrations commemorating Cuba's victory at the Bay of Pigs 50 years ago. The quotes describe this event in vivid detail as a \"massive celebration\" honoring a \"major victory\" for Cuba against US intervention. While the statistic about private businesses is mentioned, it does not appear to be the main focus of the report.<br><br>Therefore, Answer A: \"Cuba celebrated the 50th anniversary of their victory over a US-backed invasion at the Bay of Pigs\" is likely the most factually consistent summary of the news report based on the evidence presented.<br><br>3. Answer A: \"Cuba celebrated the 50th anniversary of their victory over a US-backed invasion at the Bay of Pigs\"<br><br>4. Final Answer: &lt;A&gt;<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-173359_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-173359</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting.<br> Answer B:Though Jackson was in good health, these sedatives and anesthetics stopped his breathing and led to his death.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on quotes from an expert, Dr. Cain, warning against the use of Propofol outside of a hospital setting due to the need for close monitoring of vital signs. These quotes seem to support Answer A, which states that the use of multiple drugs including Propofol is not recommended outside of a hospital setting.<br><br>However, the consultant also cites a toxicology report quote listing the drugs found in Michael Jackson's system, but this quote is not verified with &lt;v quote&gt;&lt;/v quote&gt; tags, so I cannot fully trust its accuracy.<br><br>2. Looking at the two answer choices:<br>Answer A states that the use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting. This aligns with the verified quotes from Dr. Cain about the dangers of using Propofol without proper medical monitoring.<br><br>Answer B states that the sedatives and anesthetics stopped Jackson's breathing and led to his death, but it also claims he was in good health, which is not supported by any evidence presented.<br><br>Based on the verified quotes provided, Answer A seems more factually consistent with the evidence about the risks of using Propofol outside of a hospital setting.<br><br>3. The answer text I choose as most likely to be correct is: \"The use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting.\"<br><br>4. Final Answer: &lt;A&gt;<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-197627_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-197627</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The document mentions that President Obama will be attending an interfaith vigil in Newtown, Connecticut to provide comfort to the families and first responders affected by the shooting at Sandy Hook Elementary School.<br> Answer B:Connecticut police confirmed Adam Lanza fired hundreds of bullets, killing 26.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments in favor of Answer A are strong and well-supported by verified quotes from the story transcript. The quotes \"&lt;v quote&gt;He is going to speak with the families of the 26 people, most of them elementary school children who were shot dead Friday morning.&lt;/v quote&gt;\" and \"&lt;v quote&gt;And of course, that interfaith vigil, the entirety, all of it we will bring it to you live.&lt;/v quote&gt;\" directly confirm that President Obama was attending an interfaith vigil in Newtown to provide comfort to the families affected by the Sandy Hook shooting.<br><br>2. On the other hand, the consultant correctly points out that Answer B about Adam Lanza firing hundreds of bullets and killing 26 people is not supported by any verified quotes from the transcript. While the story mentions the shooting, it does not provide specific details about the number of bullets fired or victims killed.<br><br>3. Based on the evidence presented through verified quotes, Answer A is the most factually consistent summary with the information provided in the story transcript. It accurately captures the key details mentioned about President Obama's purpose of attending the interfaith vigil in Newtown to offer solace and support to the grieving community.<br><br>Answer A: The document mentions that President Obama will be attending an interfaith vigil in Newtown, Connecticut to provide comfort to the families and first responders affected by the shooting at Sandy Hook Elementary School.<br><br>Final Answer: &lt;A&gt;<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-201245_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-201245</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:New York City Mayor Michael Bloomberg is planning to ban the use of Styrofoam in stores and restaurants due to environmental concerns.<br> Answer B:What is the proposed ban on Styrofoam in stores?</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on quotes from the story to support their position that Answer A is the most factually consistent summary. However, none of the quotes are verified, so I cannot fully trust them as direct evidence from the story.<br><br>2. That said, the consultant makes a logical argument that Answer A captures the key details reported - that Mayor Bloomberg is planning to ban Styrofoam (or plastic foam) food packaging from stores and restaurants in New York City due to environmental concerns. This seems to align with the consultant's characterization of Bloomberg's history of regulating food packaging and ingredients for health and environmental reasons.<br><br>Answer B, on the other hand, is simply a vague question and does not appear to be a factual summary of the reported information.<br><br>3. While I cannot verify the quotes used by the consultant, their overall argument and reasoning seem sound based on the information provided. Therefore, I am inclined to believe that Answer A is the more factually consistent summary of the story's key details.<br><br>4. Answer A: \"New York City Mayor Michael Bloomberg is planning to ban the use of Styrofoam in stores and restaurants due to environmental concerns.\"<br>&lt;/thinking&gt;<br><br>Final Answer: &lt;A&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-229050_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-229050</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The physician describes the phenomenon that saved the boy's life as a perfect storm of disaster that allowed him to survive by slowing down his body's need for oxygen and putting him in a frozen state until his oxygen was replenished.<br> Answer B:He is shocked that the recent 15-year-old stowaway survived with no lasting side effects.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments are based on detailed quotes attributed to Dr. Armand Dorian, a physician who previously treated a similar stowaway case. The quotes describe a \"perfect storm of disaster\" where the lack of oxygen and freezing temperatures at high altitude allowed the boy's body to enter a frozen, suspended state that drastically reduced his need for oxygen until he could be revived. These quotes seem to support Answer A and provide a plausible medical explanation for the boy's survival.<br><br>However, the quotes are not enclosed in &lt;v quote&gt;&lt;/v quote&gt; tags, so they are unverified and cannot be fully trusted. The consultant argues persuasively that Dr. Dorian's expertise and first-hand experience lend credibility to his analysis, but without verified quotes, there is still some uncertainty.<br><br>2. Answer A provides a detailed summary that aligns with the consultant's arguments and the unverified quotes attributed to Dr. Dorian. It describes the phenomenon as a \"perfect storm of disaster\" that allowed the boy to survive by slowing down his body's need for oxygen and putting him in a frozen state until his oxygen was replenished. This matches the consultant's explanation and the quotes provided.<br><br>Answer B, on the other hand, is a very brief summary that simply states the physician was shocked the boy survived with no lasting side effects. It does not provide any explanation for how the survival was possible, which the consultant argues is a key part of Dr. Dorian's analysis.<br><br>3. Based on the evidence presented, even though the quotes are unverified, Answer A seems more likely to be correct as it aligns with the detailed medical explanation provided by the consultant and the quotes attributed to Dr. Dorian.<br><br>4. Answer A: The physician describes the phenomenon that saved the boy's life as a perfect storm of disaster that allowed him to survive by slowing down his body's need for oxygen and putting him in a frozen state until his oxygen was replenished.<br>&lt;/thinking&gt;<br><br>Final Answer: &lt;A&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_consultant :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_consultancy_CNN-239067_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-239067</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Currency uncertainty would be a major issue in the event of a yes vote in the Scotland referendum on independence.<br> Answer B:Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. The consultant's arguments rely heavily on quotes attributed to Harvard economist Kenneth Rogoff to support the claim that currency uncertainty is portrayed as the major issue for an independent Scotland. However, none of these quotes are verified, so I cannot fully trust them as direct evidence from the story.<br><br>2. Without verified quotes, I have to carefully consider the wording of the two answer choices:<br>Answer A: \"Currency uncertainty would be a major issue in the event of a yes vote in the Scotland referendum on independence.\"<br>This states that currency uncertainty would be a major issue, but does not necessarily claim it would be the paramount or biggest issue.<br>Answer B: \"Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility.\"<br>This states that currency uncertainty would arise and there would be questions about the currency and establishing credibility, but does not explicitly claim it would be the biggest or most pressing issue.<br><br>Based solely on the wording of the answer choices, without being able to rely on the unverified quotes, I cannot definitively conclude that Answer A (portraying currency uncertainty as the major issue) is more factually consistent than Answer B (stating currency uncertainty would arise with related questions). Both answers seem potentially consistent with the story based on their wording.<br><br>3. Answer text I choose as most likely to be correct: \"Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility.\"<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 117 ms, sys: 14.3 ms, total: 131 ms\n",
      "Wall time: 4min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "consultancy_judge_flipped_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------CONSULTANCY Flipped JUDGE Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    time.sleep(4) # sleep 4 seconds to fix timeout errors\n",
    "    judge_response = invoke_claude_judge_consultant(debate_id = debate_id + FLIPPED_FILE_SUFFIX,\n",
    "                                  question = question, \n",
    "                                  answer_a = answer_a,\n",
    "                                  answer_b = answer_b)\n",
    "\n",
    "    logger.info(f\" >>>>> Flipped invoke_titan_judge_consultant - judge_response  >>>>> {judge_response}\")\n",
    "    consultancy_judge_flipped_answers.append(extract_final_answer(judge_response, flipped=False))\n",
    "    \n",
    "    # Print the final response \n",
    "    format_final_response(debate_id, \n",
    "                          round_num=1, \n",
    "                          question=question, \n",
    "                          answer_a=answer_a, \n",
    "                          answer_b=answer_b, \n",
    "                          judge_response=judge_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d4dd10-f4f2-4d9f-b439-6e07c26833a7",
   "metadata": {},
   "source": [
    "### <a name=\"8\">Accuracy of LLM Consultancy</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0caf50eb-f514-4d70-86f6-e6fa8eb50cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, True, False, False, False, False, True, False, False, False]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consultancy_judge_regular_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "917a81b7-4feb-4243-a62e-153d14121b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, True, True, True, True, True, True, True, False]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consultancy_judge_flipped_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c28c3b-fa91-458b-a0d6-b70b8d22f6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_consultant_judge = find_num_matching_elements(consultancy_judge_regular_answers, consultancy_judge_flipped_answers)/total_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "276b2914-3dcd-438c-b63c-919f58464d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_consultant_judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d335b912-7541-43be-9e9c-186d5100dc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_naive_judge': 0.2, 'accuracy_expert_judge': 0.3, 'accuracy_consultant_judge': 0.3, 'accuracy_debate_judge': 0.7}\n",
      "notebook results saved in results folder\n"
     ]
    }
   ],
   "source": [
    "# save the results\n",
    "results_dict = {\"accuracy_consultant_judge\" : accuracy_consultant_judge}\n",
    "save_each_experiment_result(results_dict)\n",
    "print(\"notebook results saved in results folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fbd255-a593-438f-a44f-fce489056f26",
   "metadata": {},
   "source": [
    "## <a name=\"14\">Compare Accuracies across experiments/methods.</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97c205-fa79-4c09-b115-0cd3e9cae864",
   "metadata": {},
   "source": [
    "Here we compare the accuracies of each method/experiment to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39472b99-25dc-4d47-8242-450df4c13559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_naive_judge': 0.2, 'accuracy_expert_judge': 0.3, 'accuracy_consultant_judge': 0.3, 'accuracy_debate_judge': 0.7}\n",
      "{'accuracy_naive_judge': 0.2, 'accuracy_expert_judge': 0.3, 'accuracy_consultant_judge': 0.3, 'accuracy_debate_judge': 0.7}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Naive Judge</th>\n",
       "      <th>Expert Judge</th>\n",
       "      <th>LLM Consultancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_naive_judge = get_each_experiment_result(\"accuracy_naive_judge\")\n",
    "accuracy_expert_judge = get_each_experiment_result(\"accuracy_expert_judge\")\n",
    "\n",
    "final_accuracy_comparison_judge_and_consultant(\n",
    "    accuracy_naive_judge = accuracy_naive_judge,\n",
    "    accuracy_expert_judge = accuracy_expert_judge,\n",
    "    accuracy_consultant_judge = accuracy_consultant_judge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e751a8d-fd46-4192-b100-655e8342ffe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the plot\n",
    "%matplotlib inline\n",
    "x_values = [ \"Naive Judge\", \"Expert Judge\", \"LLM Consultant\"]\n",
    "y_values = [ accuracy_naive_judge, accuracy_expert_judge, accuracy_consultant_judge]\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Compare Accuracies across experiments')\n",
    "plt.xlabel('Experiment Type')\n",
    "plt.ylabel('Accuracy')\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
