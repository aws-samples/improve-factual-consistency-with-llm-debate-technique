{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604ee77c-0e19-4a36-9f70-e8ab02cfaf54",
   "metadata": {},
   "source": [
    "<center><img src=\"images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">Improve Factual Consistency Part 3 </a>\n",
    "## <a name=\"0\">Improving Factual Consistency and Explainability using LLM Debates </a>\n",
    "\n",
    "### Glossary of Terms\n",
    "- Naive Judge : This LLM has **no** access to transcript but only question and two summaries. Measure the baseline performance.\n",
    "- Expert Judge : This LLM has access to transcript along with question and two summaries\n",
    "- Question asked to LLM (in all experiments): It is always the same: `Which one of these summaries is the most factually consistent one?`\n",
    "\n",
    "## Dataset\n",
    "Our dataset is distilled from the Amazon Science evaluation benchmark dataset called <a href=\"https://github.com/amazon-science/tofueval\">TofuEval</a>. 10 summaries have been curated from the [MediaSum documents](https://github.com/zcgzcgzcg1/MediaSum) inside the tofueval dataset for this notebook. \n",
    "\n",
    "MediaSum is a large-scale media interview dataset contains 463.6K transcripts with abstractive summaries, collected from interview transcripts and overview / topic descriptions from NPR and CNN.\n",
    "\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "In this notebook, we navigate the LLM debating technique with more persuasive LLMs having two expert debater LLMs (Claude and Mixtral) and one judge (using Claude - we can use others like Mistral/Mixtral, Titan Premier) to measure, compare and contrast its performance against other techniques like self-consistency (with naive and expert judges) and LLM consultancy. This notebook is an adapted and partial implementation of one of the ICML 2024 best papers, <a href=\"https://arxiv.org/pdf/2402.06782\"> Debating with More Persuasive LLMs Leads to More Truthful Answers </a> on a new and different Amazon Science evaluation dataset <a href=\"https://github.com/amazon-science/tofueval\">TofuEval</a>. \n",
    "\n",
    "\n",
    "- Part 1.  Demonstrate typical Standalone LLM approach\n",
    "\n",
    "- Part 2.  Demonstrate the LLM Consultancy approach and compare with Part 1.\n",
    "\n",
    "- Part 3.  **[THIS notebook]**  Demonstrate the LLM Debate approach and compare with other methods.\n",
    "\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    While this notebook(part 1, 2 and 3) compares various methods and demonstrates the efficacy of LLM Debates in notebook part 3 with a supervised dataset, the greater benefit is possible in unsupervised scenarios where ground truth is unknown and ground truth alignment and/or curation is required. Human annotation can be expensive plus slow and agreement amongst human annotators adds another level of intricacy. A possible `scalable oversight direction could be this LLM debating technique to align on the ground truth options` via this debating and critique mechanism by establishing factual consistency(veracity). This alignment and curation of ground truth for unsupervised data could be a possible win direction for the debating technique in terms of cost versus benefit analysis.\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "\n",
    "#### Notebook Kernel\n",
    "Please choose `conda_python3` as the kernel type of the top right corner of the notebook if that does not appear by default.\n",
    "\n",
    "\n",
    "## LLM Access\n",
    "\n",
    "We will need access to Anthropic Claude v3 Sonnet, Mistral 7b and  Mixtral 8x7b LLMs for this notebook.\n",
    "\n",
    "[Anthropic Claude v3(Sonnet)](https://www.anthropic.com/news/claude-3-family) , [Mixtral 8X7B](https://mistral.ai/news/mixtral-of-experts/), [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) - all of them pre-trained on general text summarization tasks.\n",
    "\n",
    "## Use-Case Overview\n",
    "\n",
    "To demonstrate the measurement and improvement of factual consistency (veracity) with explainability in this notebook, we conduct a series of experiments to choose the best summary for each transcript. In each experiment, we measure the veracity and correctness of the summaries generated from transcripts and improve upon the decision to choose the correct one via methods like LLM consultancy and LLM debates.\n",
    "\n",
    "The <b>overall task in this notebook</b> is choose which one of the two summaries is most appropriate for a given transcript. There are a total of 10 transcripts and each transcript has 2 summaries - one correct and other incorrect. The incorrect summaries have various classes of errors like `Nuanced Meaning Shift`, `Extrinsic Information` and  `Reasoning errors`. \n",
    "\n",
    "In this notebook we will conduct the following set of experiment combinations to measure, compare and contrast LLM debating techniques with others.\n",
    "\n",
    "\n",
    "## Experiments\n",
    "For each of these experiments we flip the side of the argument the LLM takes to account for `position bias` and `verbosity bias` and re-run each experiment.\n",
    "\n",
    "**Note** We always use the same Judge LLM (Mistral 7B) across all the experiments in this notebook\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    If you see throttling exception, please increase timeout from 10 seconds in `time.sleep(10)` to say 20 and retry\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment 4: (LLM Debate) \n",
    "<center><img src=\"images/veracitylab01-llm-debate.png\" alt=\"In this image, we depict the flow of LLM Debate. First Debater LLMs like Claude and Mixtral argue their side\n",
    "based on transcript contents. Next each argument is saved to a file and\n",
    "the next debater picks up the entire argument history before posting their next argument. Finally, once all 3 rounds of arguments are over, the Judge LLM reads all the arguments and decides which summary is the most factually consistent answer.\"  height=\"700\" width=\"700\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "We use Claude 3 as first debater and Mixtral as second debater with Claude as Judge. We let the debater argue their sides and finally let the judge decide which argument is better. This continues for N(=3 in this notebook) rounds. We flip Claude and Mistral argument sides in experiments 4a and 4b and take average of the experiment results as final accuracy. This accounts for errors due to position (choosing an answer due to its order/position) and verbosity bias (one answer longer than the other)\n",
    "\n",
    "##### Experiment 4a: (Claude v3 argues for answer A, Mixtral argues for Answer B): \n",
    "Claude v3(Sonnet) argues for answer A(Ground Truth:False Answer) and generates rationale why that answer is correct. Mixtral 8X7B argues for answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of the debate, Claude as a judge adjudicates whether Claude's or Mixtral's rationale is correct and chooses a side to give the final answer.\n",
    "\n",
    "#####  Experiment 4b: (Claude v3 argues for answer B, Mixtral argues for Answer A): \n",
    "Claude v3(Sonnet) argues for answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. Mixtral 8X7B argues for answer A(Ground Truth:False Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of the debate, Claude as a judge adjudicates whether Claude's or Mixtral's rationale is correct and chooses a side to give the final answer.\n",
    "\n",
    "---\n",
    "## Evaluation Metrics\n",
    "For each type of experiment we evaluate the accuracy of the answers for that experiment/method type to compare and contrast each method at the end.\n",
    "\n",
    "For the final experiment on LLM Debate, we also calculate the `win rate` of the LLM debaters to evaluate which of the LLMs actually got most of the answers right as adjudicated by the judge. This can be considered a mechanism to choose one LLM over the other given this use-case.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This notebook notebook has the following sections:\n",
    "\n",
    "1. <a href=\"#1\">Dataset exploration</a>\n",
    "2. <a href=\"#2\">Accuracy of LLM Debate</a>\n",
    "3. <a href=\"#3\">Compare Accuracies across experiments</a>\n",
    "4. <a href=\"#4\">Choose expert LLM using Win Rate measured during LLM Debate (Experiment 4) </a>\n",
    "5. <a href=\"#5\">Challenge exercise and notebook quiz</a>\n",
    "    \n",
    "Please work top to bottom of this notebook and don't skip sections as this could lead to error messages due to missing code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7190cc0d-ec7d-42c4-bd87-f301e6db48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install setuptools==70.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2ad2f4-b720-48b9-bb5a-7b99bcafce8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q -U pip --root-user-action=ignore\n",
    "!pip3 install -q -r requirements.txt --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e6507-fc9a-4f1f-8535-7e2deb20a9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We load all prompts from a separate file prompts.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from prompts import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mlu_utils.veracity_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573ddc8-9290-484c-86f9-f16531648cac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_up_files_in_dir(\"./transcripts\")\n",
    "clear_file_contents(\"./log_files/notebook_run_logs.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c84b37-369c-405c-ac08-23be8dbb61a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re, time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import Markdown\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import logging\n",
    "import boto3, warnings\n",
    "import pandas as pd\n",
    "# Supress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(filename='log_files/notebook_run_logs.log', encoding='utf-8', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"----- Test logging setup -----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd02d8-4595-48de-b0cd-394002cbc17a",
   "metadata": {},
   "source": [
    "### Bedrock Model Access check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42bd90d-9cee-4534-90a4-c41bd2c07b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if all bedrock model access has been enabled \n",
    "test_llm_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c14a1-7d5b-48dc-a4a0-5c5e0a205f57",
   "metadata": {},
   "source": [
    "### Constants used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf134bf-7fbd-4c70-a84c-4b077f79f2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rounds = 3\n",
    "question = \"Which one of these summaries is the most factually consistent one?\"\n",
    "total_data_points = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829f829-7d19-420c-85ea-0e5c370304e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a name=\"1\">Dataset Exploration</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d79c9-c30d-4daa-9d25-4dac7de60e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pre-process the dataset\n",
    "answers_df = pd.read_csv(\"./tofueval_dataset/mediasum_dev_doc_id_group_final_dual_summaries_manual_final_dataset.csv\")\n",
    "#answers_df.head()\n",
    "interview_df = pd.read_csv(\"./tofueval_dataset/mediasum_dev_doc_complete_final.csv\")\n",
    "#interview_df.head()\n",
    "\n",
    "result = pd.merge(answers_df, interview_df, on=\"doc_id\")\n",
    "final_dataset = result[[\"doc_id\", \"topic\", \"summ_sent_incorrect_original\", \"summ_sent_correct_manual\", \"exp\", \"type\", \"source\"]]\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a89423-9733-498e-807c-ef7c28e3de1a",
   "metadata": {},
   "source": [
    "### <a name=\"2\">LLM Debate: 2 expert LLMs, 1 naive judge - LLM-1 arguing for 1st summary</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f1bc7-1f39-4907-9ea8-4e73c2e97402",
   "metadata": {},
   "source": [
    "In this LLM Debate - Claude(LLM-1) defends incorrect Summary and Mixtral(LLM-2) defends correct summary.\n",
    "\n",
    "Claude v3(Sonnet) argues for answer A(Ground Truth:False Answer) and generates rationale why that answer is correct. Mixtral 8X7B argues for answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of the debate, Claude as a judge adjudicates whether Claude's or Mixtral's rationale is correct and chooses a side to give the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70825e3-a6cb-4698-ba8c-4cf4b6bacb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(20) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    \n",
    "    #### defending false - Claude\n",
    "    claude_defending_summary=answer_b\n",
    "    claude_opposing_summary=answer_a\n",
    "\n",
    "    #### defending true - Mixtral\n",
    "    mixtral_defending_summary=answer_a\n",
    "    mixtral_opposing_summary=answer_b\n",
    "\n",
    "    logger.info(f\"-------------2 model Debate -> Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    #### Consultancy Claude - defending true - 3 rounds\n",
    "    delete_file(f\"./transcripts/full_transcript_debate_{debate_id}.log\")\n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        print(f\"=========== START OF 2 model DEBATE debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "        logger.info(f\"START Debate with Claude Debate_id {debate_id} Round #{round_number + 1} >>>>>> \\n\") \n",
    "        claude_debate_response = invoke_claude_v3(debate_id = debate_id,\n",
    "                         question=question,\n",
    "                         round_number = round_number + 1,\n",
    "                         summary_defending = claude_defending_summary, \n",
    "                         summary_opposing = claude_opposing_summary, \n",
    "                         complete_interview = complete_interview_transcript,\n",
    "                         debate=True\n",
    "                         )\n",
    "\n",
    "        logger.info(f\" >>>>> claude_debate_response Round #{round_number + 1} >>>>> {claude_debate_response}\")\n",
    "        logger.info(f\"END Debate with Claude Round #{round_number + 1} >>>>>> \\n\")\n",
    "\n",
    "        mixtral_debate_response = invoke_mistral(debate_id = debate_id,\n",
    "                     question=question,\n",
    "                     round_number = round_number + 1,\n",
    "                     summary_defending = mixtral_defending_summary, \n",
    "                     summary_opposing = mixtral_opposing_summary, \n",
    "                     complete_interview = complete_interview_transcript, \n",
    "                     )\n",
    "\n",
    "        logger.info(f\" >>>>> mixtral_debate_response Round #{round_number + 1} >>>>> {mixtral_debate_response}\")\n",
    "        logger.info(f\"END Debate with Mixtral Round #{round_number + 1} >>>>>> \\n\")\n",
    "    print(f\"=========== END OF 2 model DEBATE debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152677e-6d8d-434e-82bb-3db6742b0dc8",
   "metadata": {},
   "source": [
    "## JUDGE for Regular Debate : LLM-Claude arguing for 1st summary, LLM-Mixtral arguing for 2nd summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b4c7d-eff4-478a-90bf-898bfa490401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "debate_judge_regular_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------DEBATE  JUDGE Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    judge_response = invoke_claude_judge_debate(debate_id = debate_id,\n",
    "                              question=question,\n",
    "                 answer_a = answer_a,\n",
    "                 answer_b = answer_b)\n",
    "    debate_judge_regular_answers.append(extract_final_answer(judge_response, flipped=False))\n",
    "    logger.info(f\" >>>>> invoke_mistral_judge_debate - judge_response  >>>>> {judge_response}\")\n",
    "    # Print the final response \n",
    "    format_final_response(debate_id, \n",
    "                          round_num=1, \n",
    "                          question=question, \n",
    "                          answer_a=answer_a, \n",
    "                          answer_b=answer_b, \n",
    "                          judge_response=judge_response)\n",
    "print(debate_judge_regular_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a46272-45c5-4e0f-be8e-0a0850e5df8b",
   "metadata": {},
   "source": [
    "### <a name=\"3\">LLM Debate: 2 expert LLMs, 1 naive judge - LLM-1 arguing for 2nd summary</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7f903-969c-427d-a1aa-732d0db39ba2",
   "metadata": {},
   "source": [
    "In this **flipped LLM Debate** - Claude(LLM-1) defends correct Summary and Mixtral(LLM-2) defends incorrect summary.\n",
    "\n",
    "\n",
    "Claude v3(Sonnet) argues for answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. Mixtral 8X7B argues for answer A(Ground Truth:False Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of the debate, Claude as a judge adjudicates whether Claude's or Mixtral's rationale is correct and chooses a side to give the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd6c9f-880e-4a70-a625-8c6a964a4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(20) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    \n",
    "    #### defending True - Claude\n",
    "    claude_defending_summary=answer_a\n",
    "    claude_opposing_summary=answer_b\n",
    "\n",
    "    #### defending False - Mixtral\n",
    "    mixtral_defending_summary=answer_b\n",
    "    mixtral_opposing_summary=answer_a\n",
    "    \n",
    "    delete_file(f\"./transcripts/full_transcript_debate_{debate_id}{FLIPPED_FILE_SUFFIX}.log\")\n",
    "\n",
    "    logger.info(f\"-------------2 model Debate -> Debate_id {debate_id}-------------------\")\n",
    "    print(f\"=========== START OF 2 model FLIPPED DEBATE debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        logger.info(f\"START Debate with Claude Round #{round_number + 1} >>>>>> \\n\") \n",
    "        claude_debate_response = invoke_claude_v3(debate_id = debate_id + FLIPPED_FILE_SUFFIX,\n",
    "                         question=question,\n",
    "                         round_number = round_number + 1,\n",
    "                         summary_defending = claude_defending_summary, \n",
    "                         summary_opposing = claude_opposing_summary, \n",
    "                         complete_interview = complete_interview_transcript,\n",
    "                         debate=True\n",
    "                         )\n",
    "\n",
    "        logger.info(f\" >>>>> claude_debate_response Round #{round_number + 1} >>>>> {claude_debate_response}\")\n",
    "        logger.info(f\"END Debate with Claude Round #{round_number + 1} >>>>>> \\n\")\n",
    "\n",
    "        mixtral_debate_response = invoke_mistral(debate_id = debate_id + FLIPPED_FILE_SUFFIX,\n",
    "                     question=question,\n",
    "                     round_number = round_number + 1,\n",
    "                     summary_defending = mixtral_defending_summary, \n",
    "                     summary_opposing = mixtral_opposing_summary, \n",
    "                     complete_interview = complete_interview_transcript, \n",
    "                     )\n",
    "\n",
    "        logger.info(f\" >>>>> mixtral_debate_response Round #{round_number + 1} >>>>> {mixtral_debate_response}\")\n",
    "        logger.info(f\"END Debate with Mixtral Round #{round_number + 1} >>>>>> \\n\")\n",
    "    print(f\"=========== END OF 2 model FLIPPED DEBATE debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73c855-24d5-4893-8b10-afc91936bf04",
   "metadata": {},
   "source": [
    "## JUDGE for flipped LLM Debate:LLM-Claude arguing for 1st summary, LLM-Mixtral arguing for 2nd summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee103881-33b6-473f-8d9b-ad7a69aa20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "debate_judge_flipped_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------DEBATE FLIPPED JUDGE Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    judge_response = invoke_claude_judge_debate(debate_id = debate_id + FLIPPED_FILE_SUFFIX,\n",
    "                              question=question,\n",
    "                              answer_a = answer_a,\n",
    "                              answer_b = answer_b)\n",
    "    debate_judge_flipped_answers.append(extract_final_answer(judge_response, flipped=False))\n",
    "    logger.info(f\" >>>>> Flipped invoke_mistral_judge_debate - judge_response  >>>>> {judge_response}\")\n",
    "    \n",
    "    # Print the final response \n",
    "    format_final_response(debate_id, \n",
    "                          round_num=1, \n",
    "                          question=question, \n",
    "                          answer_a=answer_a, \n",
    "                          answer_b=answer_b, \n",
    "                          judge_response=judge_response)\n",
    "print(debate_judge_flipped_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd391f-2c5b-4385-84d9-38da4515699f",
   "metadata": {},
   "source": [
    "## <a name=\"4\">Accuracy of LLM Debate</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee9e72-42c2-4963-a68c-60941c472e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debate_judge_regular_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b689ae-7377-41db-afce-c80bf65fc1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debate_judge_flipped_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c47c3faa-1ddd-4f7b-864f-efac1c23a155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_debate_judge = find_num_matching_elements(debate_judge_regular_answers, debate_judge_flipped_answers)/total_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13e301-ce93-49d5-ab48-430245eff5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_debate_judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f33fc-52aa-442a-bc76-ed55f15d560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "results_dict = {\"accuracy_debate_judge\" : accuracy_debate_judge}\n",
    "save_each_experiment_result(results_dict)\n",
    "print(\"notebook results saved in results folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fbd255-a593-438f-a44f-fce489056f26",
   "metadata": {},
   "source": [
    "## <a name=\"5\">Compare Accuracies across experiments/methods.</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97c205-fa79-4c09-b115-0cd3e9cae864",
   "metadata": {},
   "source": [
    "Here we compare the accuracies of each method/experiment to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39472b99-25dc-4d47-8242-450df4c13559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_naive_judge = get_each_experiment_result(\"accuracy_naive_judge\")\n",
    "accuracy_expert_judge = get_each_experiment_result(\"accuracy_expert_judge\")\n",
    "accuracy_consultant_judge = get_each_experiment_result(\"accuracy_consultant_judge\")\n",
    "\n",
    "final_accuracy_comparison(\n",
    "    accuracy_naive_judge = accuracy_naive_judge,\n",
    "    accuracy_expert_judge = accuracy_expert_judge,\n",
    "    accuracy_consultant_judge = accuracy_consultant_judge,\n",
    "    accuracy_debate_judge = accuracy_debate_judge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e751a8d-fd46-4192-b100-655e8342ffe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the plot\n",
    "x_values = [ \"Naive Judge\", \"Expert Judge\", \"LLM Consultant\", \"LLM Debate\"]\n",
    "y_values = [ accuracy_naive_judge, accuracy_expert_judge, accuracy_consultant_judge, accuracy_debate_judge]\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Compare Accuracies across experiments')\n",
    "plt.xlabel('Experiment Type')\n",
    "plt.ylabel('Accuracy')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b11eae-cb6b-4196-a91e-56b2f3c4b076",
   "metadata": {},
   "source": [
    "### <a name=\"6\">Choose expert LLM using Win Rate measured during LLM Debate (Experiment 4) </a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0682b92-dcda-4949-b9ac-49c027e18226",
   "metadata": {},
   "source": [
    "With this win rate of expert models, we emprically understand which LLM as a debater is more successful than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2b8aa-0335-4d8a-811d-9da60dd69826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "claude_avg_win_rate, mixtral_avg_win_rate = get_win_rate_per_model(\n",
    "    debate_judge_regular_answers, \n",
    "    debate_judge_flipped_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719aa86-ca8b-436a-8b6d-9ca1a3ba08db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "win_rate_comparison(claude_avg_win_rate, mixtral_avg_win_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1319a9-e85c-4d29-92d7-60acd1a21e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the plot\n",
    "%matplotlib inline\n",
    "x_values = [ \"Claude v3 Sonnet\", \"Mixtral\"]\n",
    "y_values = [claude_avg_win_rate, mixtral_avg_win_rate]\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Compare average win-rate across expert LLMs')\n",
    "plt.xlabel('average win-rate')\n",
    "plt.ylabel('win-rate')\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
