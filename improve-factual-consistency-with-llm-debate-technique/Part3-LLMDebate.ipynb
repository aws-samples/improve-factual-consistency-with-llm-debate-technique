{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604ee77c-0e19-4a36-9f70-e8ab02cfaf54",
   "metadata": {},
   "source": [
    "<center><img src=\"images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">Improve Factual Consistency Part 3 </a>\n",
    "## <a name=\"0\">Improving Factual Consistency and Explainability using LLM Debates </a>\n",
    "\n",
    "### Glossary of Terms\n",
    "- Naive Judge : This LLM has **no** access to transcript but only question and two summaries. Measure the baseline performance.\n",
    "- Expert Judge : This LLM has access to transcript along with question and two summaries\n",
    "- Question asked to LLM (in all experiments): It is always the same: `Which one of these summaries is the most factually consistent one?`\n",
    "\n",
    "## Dataset\n",
    "Our dataset is distilled from the Amazon Science evaluation benchmark dataset called <a href=\"https://github.com/amazon-science/tofueval\">TofuEval</a>. 10 summaries have been curated from the [MediaSum documents](https://github.com/zcgzcgzcg1/MediaSum) inside the tofueval dataset for this notebook. \n",
    "\n",
    "MediaSum is a large-scale media interview dataset contains 463.6K transcripts with abstractive summaries, collected from interview transcripts and overview / topic descriptions from NPR and CNN.\n",
    "\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "In this notebook, we navigate the LLM debating technique with more persuasive LLMs having two expert debater LLMs (Claude and Mixtral) and one judge (using Claude - we can use others like Mistral/Mixtral, Titan Premier) to measure, compare and contrast its performance against other techniques like self-consistency (with naive and expert judges) and LLM consultancy. This notebook is an adapted and partial implementation of one of the ICML 2024 best papers, <a href=\"https://arxiv.org/pdf/2402.06782\"> Debating with More Persuasive LLMs Leads to More Truthful Answers </a> on a new and different Amazon Science evaluation dataset <a href=\"https://github.com/amazon-science/tofueval\">TofuEval</a>. \n",
    "\n",
    "\n",
    "- Part 1.  Demonstrate typical Standalone LLM approach\n",
    "\n",
    "- Part 2.  Demonstrate the LLM Consultancy approach and compare with Part 1.\n",
    "\n",
    "- Part 3.  **[THIS notebook]**  Demonstrate the LLM Debate approach and compare with other methods.\n",
    "\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    While this notebook(part 1, 2 and 3) compares various methods and demonstrates the efficacy of LLM Debates in notebook part 3 with a supervised dataset, the greater benefit is possible in unsupervised scenarios where ground truth is unknown and ground truth alignment and/or curation is required. Human annotation can be expensive plus slow and agreement amongst human annotators adds another level of intricacy. A possible `scalable oversight direction could be this LLM debating technique to align on the ground truth options` via this debating and critique mechanism by establishing factual consistency(veracity). This alignment and curation of ground truth for unsupervised data could be a possible win direction for the debating technique in terms of cost versus benefit analysis.\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "\n",
    "#### Notebook Kernel\n",
    "Please choose `conda_python3` as the kernel type of the top right corner of the notebook if that does not appear by default.\n",
    "\n",
    "\n",
    "## LLM Access\n",
    "\n",
    "We will need access to Anthropic Claude v3 Sonnet, Mistral 7b and  Mixtral 8x7b LLMs for this notebook.\n",
    "\n",
    "[Anthropic Claude v3(Sonnet)](https://www.anthropic.com/news/claude-3-family) , [Mixtral 8X7B](https://mistral.ai/news/mixtral-of-experts/), [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) - all of them pre-trained on general text summarization tasks.\n",
    "\n",
    "## Use-Case Overview\n",
    "\n",
    "To demonstrate the measurement and improvement of factual consistency (veracity) with explainability in this notebook, we conduct a series of experiments to choose the best summary for each transcript. In each experiment, we measure the veracity and correctness of the summaries generated from transcripts and improve upon the decision to choose the correct one via methods like LLM consultancy and LLM debates.\n",
    "\n",
    "The <b>overall task in this notebook</b> is choose which one of the two summaries is most appropriate for a given transcript. There are a total of 10 transcripts and each transcript has 2 summaries - one correct and other incorrect. The incorrect summaries have various classes of errors like `Nuanced Meaning Shift`, `Extrinsic Information` and  `Reasoning errors`. \n",
    "\n",
    "In this notebook we will conduct the following set of experiment combinations to measure, compare and contrast LLM debating techniques with others.\n",
    "\n",
    "\n",
    "## Experiments\n",
    "For each of these experiments we flip the side of the argument the LLM takes to account for `position bias` and `verbosity bias` and re-run each experiment.\n",
    "\n",
    "**Note** We always use the same Judge LLM (Mistral 7B) across all the experiments in this notebook\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    If you see throttling exception, please increase timeout from 10 seconds in `time.sleep(10)` to say 20 and retry\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment 4: (LLM Debate) \n",
    "<center><img src=\"images/veracitylab01-llm-debate.png\" alt=\"In this image, we depict the flow of LLM Debate. First Debater LLMs like Claude and Mixtral argue their side\n",
    "based on transcript contents. Next each argument is saved to a file and\n",
    "the next debater picks up the entire argument history before posting their next argument. Finally, once all 3 rounds of arguments are over, the Judge LLM reads all the arguments and decides which summary is the most factually consistent answer.\"  height=\"700\" width=\"700\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "We use Claude 3 as first debater and Mixtral as second debater with Claude as Judge. We let the debater argue their sides and finally let the judge decide which argument is better. This continues for N(=3 in this notebook) rounds. We flip Claude and Mistral argument sides in experiments 4a and 4b and take average of the experiment results as final accuracy. This accounts for errors due to position (choosing an answer due to its order/position) and verbosity bias (one answer longer than the other)\n",
    "\n",
    "##### Experiment 4a: (Claude v3 argues for answer A, Mixtral argues for Answer B): \n",
    "Claude v3(Sonnet) argues for answer A(Ground Truth:False Answer) and generates rationale why that answer is correct. Mixtral 8X7B argues for answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of the debate, Claude as a judge adjudicates whether Claude's or Mixtral's rationale is correct and chooses a side to give the final answer.\n",
    "\n",
    "#####  Experiment 4b: (Claude v3 argues for answer B, Mixtral argues for Answer A): \n",
    "Claude v3(Sonnet) argues for answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. Mixtral 8X7B argues for answer A(Ground Truth:False Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of the debate, Claude as a judge adjudicates whether Claude's or Mixtral's rationale is correct and chooses a side to give the final answer.\n",
    "\n",
    "---\n",
    "## Evaluation Metrics\n",
    "For each type of experiment we evaluate the accuracy of the answers for that experiment/method type to compare and contrast each method at the end.\n",
    "\n",
    "For the final experiment on LLM Debate, we also calculate the `win rate` of the LLM debaters to evaluate which of the LLMs actually got most of the answers right as adjudicated by the judge. This can be considered a mechanism to choose one LLM over the other given this use-case.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This notebook notebook has the following sections:\n",
    "\n",
    "1. <a href=\"#1\">Dataset exploration</a>\n",
    "2. <a href=\"#2\">Accuracy of LLM Debate</a>\n",
    "3. <a href=\"#3\">Compare Accuracies across experiments</a>\n",
    "4. <a href=\"#4\">Choose expert LLM using Win Rate measured during LLM Debate (Experiment 4) </a>\n",
    "5. <a href=\"#5\">Challenge exercise and notebook quiz</a>\n",
    "    \n",
    "Please work top to bottom of this notebook and don't skip sections as this could lead to error messages due to missing code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7190cc0d-ec7d-42c4-bd87-f301e6db48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install setuptools==70.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2ad2f4-b720-48b9-bb5a-7b99bcafce8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q -U pip --root-user-action=ignore\n",
    "!pip3 install -q -r requirements.txt --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8e6507-fc9a-4f1f-8535-7e2deb20a9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# We load all prompts from a separate file prompts.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from prompts import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mlu_utils.veracity_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f573ddc8-9290-484c-86f9-f16531648cac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear_file_contents dir :: <built-in function dir>\n"
     ]
    }
   ],
   "source": [
    "clean_up_files_in_dir(\"./transcripts\")\n",
    "clear_file_contents(\"./log_files/notebook_run_logs.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c84b37-369c-405c-ac08-23be8dbb61a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re, time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import Markdown\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import logging\n",
    "import boto3, warnings\n",
    "import pandas as pd\n",
    "# Supress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(filename='log_files/notebook_run_logs.log', encoding='utf-8', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"----- Test logging setup -----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd02d8-4595-48de-b0cd-394002cbc17a",
   "metadata": {},
   "source": [
    "### Bedrock Model Access check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42bd90d-9cee-4534-90a4-c41bd2c07b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude v3 sonnet looks good\n",
      "Mixtral 8X7B looks good\n",
      "Mistral 7B looks good\n",
      "All required model access look good\n"
     ]
    }
   ],
   "source": [
    "#test if all bedrock model access has been enabled \n",
    "test_llm_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c14a1-7d5b-48dc-a4a0-5c5e0a205f57",
   "metadata": {},
   "source": [
    "### Constants used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf134bf-7fbd-4c70-a84c-4b077f79f2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rounds = 3\n",
    "question = \"Which one of these summaries is the most factually consistent one?\"\n",
    "total_data_points = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829f829-7d19-420c-85ea-0e5c370304e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a name=\"1\">Dataset Exploration</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b67d79c9-c30d-4daa-9d25-4dac7de60e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>summ_sent_incorrect_original</th>\n",
       "      <th>summ_sent_correct_manual</th>\n",
       "      <th>exp</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-104129</td>\n",
       "      <td>Decline of American automobile industry</td>\n",
       "      <td>GM lost $10B in 2005, continues losing market ...</td>\n",
       "      <td>GM lost $10.6B in 2005, continues losing marke...</td>\n",
       "      <td>It's not \"$10B\" but \"$10.6B\"</td>\n",
       "      <td>Nuanced Meaning Shift</td>\n",
       "      <td>DOBBS: General Motors today announced it will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN-138971</td>\n",
       "      <td>Diplomatic efforts</td>\n",
       "      <td>North Korea has announced plans to launch a sa...</td>\n",
       "      <td>Diplomatic efforts to secure the release of Am...</td>\n",
       "      <td>The launch of a satellite is not mentioned, bu...</td>\n",
       "      <td>Extrinsic Information</td>\n",
       "      <td>ROBERTS: Welcome back to the Most News in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN-139946</td>\n",
       "      <td>Filibuster-Proof Majority</td>\n",
       "      <td>This filibuster-proof majority means Democrats...</td>\n",
       "      <td>Democrats gain 60 seats in Senate, giving them...</td>\n",
       "      <td>This is an unsupported statement</td>\n",
       "      <td>Extrinsic Information</td>\n",
       "      <td>ANNOUNCER: This is CNN breaking news.\\nMALVEAU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN-145383</td>\n",
       "      <td>Educate to Innovate Campaign</td>\n",
       "      <td>The private sector has committed over $260 mil...</td>\n",
       "      <td>Over $260 million in private funding will supp...</td>\n",
       "      <td>The document does not state that \"reaching you...</td>\n",
       "      <td>Reasoning Error</td>\n",
       "      <td>HARRIS: And President Obama in the Eisenhower ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN-164885</td>\n",
       "      <td>Cuban celebration and government gathering</td>\n",
       "      <td>170,000 Cubans have private businesses.</td>\n",
       "      <td>Cuba celebrated the 50th anniversary of their ...</td>\n",
       "      <td>The document says that 170,000 Cubans have app...</td>\n",
       "      <td>Nuanced Meaning Shift</td>\n",
       "      <td>FEYERICK: We'll get to Donald Trump's campaign...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id                                       topic  \\\n",
       "0  CNN-104129     Decline of American automobile industry   \n",
       "1  CNN-138971                          Diplomatic efforts   \n",
       "2  CNN-139946                   Filibuster-Proof Majority   \n",
       "3  CNN-145383                Educate to Innovate Campaign   \n",
       "4  CNN-164885  Cuban celebration and government gathering   \n",
       "\n",
       "                        summ_sent_incorrect_original  \\\n",
       "0  GM lost $10B in 2005, continues losing market ...   \n",
       "1  North Korea has announced plans to launch a sa...   \n",
       "2  This filibuster-proof majority means Democrats...   \n",
       "3  The private sector has committed over $260 mil...   \n",
       "4            170,000 Cubans have private businesses.   \n",
       "\n",
       "                            summ_sent_correct_manual  \\\n",
       "0  GM lost $10.6B in 2005, continues losing marke...   \n",
       "1  Diplomatic efforts to secure the release of Am...   \n",
       "2  Democrats gain 60 seats in Senate, giving them...   \n",
       "3  Over $260 million in private funding will supp...   \n",
       "4  Cuba celebrated the 50th anniversary of their ...   \n",
       "\n",
       "                                                 exp                   type  \\\n",
       "0                       It's not \"$10B\" but \"$10.6B\"  Nuanced Meaning Shift   \n",
       "1  The launch of a satellite is not mentioned, bu...  Extrinsic Information   \n",
       "2                   This is an unsupported statement  Extrinsic Information   \n",
       "3  The document does not state that \"reaching you...        Reasoning Error   \n",
       "4  The document says that 170,000 Cubans have app...  Nuanced Meaning Shift   \n",
       "\n",
       "                                              source  \n",
       "0  DOBBS: General Motors today announced it will ...  \n",
       "1  ROBERTS: Welcome back to the Most News in the ...  \n",
       "2  ANNOUNCER: This is CNN breaking news.\\nMALVEAU...  \n",
       "3  HARRIS: And President Obama in the Eisenhower ...  \n",
       "4  FEYERICK: We'll get to Donald Trump's campaign...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-process the dataset\n",
    "answers_df = pd.read_csv(\"./tofueval_dataset/mediasum_dev_doc_id_group_final_dual_summaries_manual_final_dataset.csv\")\n",
    "#answers_df.head()\n",
    "interview_df = pd.read_csv(\"./tofueval_dataset/mediasum_dev_doc_complete_final.csv\")\n",
    "#interview_df.head()\n",
    "\n",
    "result = pd.merge(answers_df, interview_df, on=\"doc_id\")\n",
    "final_dataset = result[[\"doc_id\", \"topic\", \"summ_sent_incorrect_original\", \"summ_sent_correct_manual\", \"exp\", \"type\", \"source\"]]\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a89423-9733-498e-807c-ef7c28e3de1a",
   "metadata": {},
   "source": [
    "### <a name=\"2\">LLM Debate: 2 expert LLMs, 1 naive judge - LLM-1 arguing for 1st summary</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f1bc7-1f39-4907-9ea8-4e73c2e97402",
   "metadata": {},
   "source": [
    "In this LLM Debate - Claude(LLM-1) defends incorrect Summary and Mixtral(LLM-2) defends correct summary.\n",
    "\n",
    "Claude v3(Sonnet) argues for answer A(Ground Truth:False Answer) and generates rationale why that answer is correct. Mixtral 8X7B argues for answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of the debate, Claude as a judge adjudicates whether Claude's or Mixtral's rationale is correct and chooses a side to give the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a70825e3-a6cb-4698-ba8c-4cf4b6bacb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== START OF 2 model DEBATE debate_id CNN-104129 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-104129 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-104129 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-104129 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-138971 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-138971 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-138971 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-138971 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-139946 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-139946 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-139946 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-139946 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-145383 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-145383 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-145383 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-145383 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-164885 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-164885 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-164885 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-164885 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-173359 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-173359 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-173359 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-173359 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-197627 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-197627 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-197627 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-197627 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-201245 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-201245 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-201245 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-201245 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-229050 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-229050 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-229050 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-229050 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-239067 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-239067 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-239067 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-239067 Round #1..3 ======= \n",
      "\n",
      "CPU times: user 785 ms, sys: 96.7 ms, total: 882 ms\n",
      "Wall time: 15min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(20) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    \n",
    "    #### defending false - Claude\n",
    "    claude_defending_summary=answer_b\n",
    "    claude_opposing_summary=answer_a\n",
    "\n",
    "    #### defending true - Mixtral\n",
    "    mixtral_defending_summary=answer_a\n",
    "    mixtral_opposing_summary=answer_b\n",
    "\n",
    "    logger.info(f\"-------------2 model Debate -> Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    #### Consultancy Claude - defending true - 3 rounds\n",
    "    delete_file(f\"./transcripts/full_transcript_debate_{debate_id}.log\")\n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        print(f\"=========== START OF 2 model DEBATE debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "        logger.info(f\"START Debate with Claude Debate_id {debate_id} Round #{round_number + 1} >>>>>> \\n\") \n",
    "        claude_debate_response = invoke_claude_v3(debate_id = debate_id,\n",
    "                         question=question,\n",
    "                         round_number = round_number + 1,\n",
    "                         summary_defending = claude_defending_summary, \n",
    "                         summary_opposing = claude_opposing_summary, \n",
    "                         complete_interview = complete_interview_transcript,\n",
    "                         debate=True\n",
    "                         )\n",
    "\n",
    "        logger.info(f\" >>>>> claude_debate_response Round #{round_number + 1} >>>>> {claude_debate_response}\")\n",
    "        logger.info(f\"END Debate with Claude Round #{round_number + 1} >>>>>> \\n\")\n",
    "\n",
    "        mixtral_debate_response = invoke_mistral(debate_id = debate_id,\n",
    "                     question=question,\n",
    "                     round_number = round_number + 1,\n",
    "                     summary_defending = mixtral_defending_summary, \n",
    "                     summary_opposing = mixtral_opposing_summary, \n",
    "                     complete_interview = complete_interview_transcript, \n",
    "                     )\n",
    "\n",
    "        logger.info(f\" >>>>> mixtral_debate_response Round #{round_number + 1} >>>>> {mixtral_debate_response}\")\n",
    "        logger.info(f\"END Debate with Mixtral Round #{round_number + 1} >>>>>> \\n\")\n",
    "    print(f\"=========== END OF 2 model DEBATE debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152677e-6d8d-434e-82bb-3db6742b0dc8",
   "metadata": {},
   "source": [
    "## JUDGE for Regular Debate : LLM-Claude arguing for 1st summary, LLM-Mixtral arguing for 2nd summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "667b4c7d-eff4-478a-90bf-898bfa490401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-104129.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-104129</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:GM lost \\$10.6B in 2005, continues losing market share.<br> Answer B:GM lost \\$10B in 2005, continues losing market share.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Strengths and weaknesses of arguments:<br>- Both debaters make valid points about the relative accuracy of approximating \\$10.6B to \\$10B for a high-level summary.<br>- The key verified quote is: \"General Motors reporting a \\$10.6 billion loss in 2005\". This supports Answer A being more precise.<br>- However, both debaters agree that a \\$0.6B difference is relatively minor in the context of multi-billion dollar losses, and that conveying the overall scale is more important than exact precision for a summary.<br><br>2. Evaluating which answer is likely correct:<br>- Answer A (\"GM lost \\$10.6B in 2005, continues losing market share\") is a direct quote from the story, making it the most factually precise summary.<br>- However, Answer B (\"GM lost \\$10B in 2005, continues losing market share\") is also a reasonable approximation that captures the essence of GM's massive losses, as both debaters argued.<br>- For a high-level summary, the \\$0.6B difference is relatively minor compared to losses in the billions. Both answers convey the key point of GM's struggles and losses continuing.<br>- Therefore, while Answer A is technically more precise, Answer B can also be considered a factually consistent summary given the context.<br><br>3. Answer chosen as correct: Both Answer A and Answer B can be considered factually consistent summaries, with A being more precise but B still being a reasonable approximation that captures the overall point.<br><br>4. Final Answer: Both A and B are factually consistent summaries.<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-138971.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-138971</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Diplomatic efforts to secure the release of American journalists Laura Ling and Euna Lee, who are accused of spying in North Korea and face trial, are ongoing.<br> Answer B:North Korea has announced plans to launch a satellite into orbit, which the US sees as a cover for testing ballistic missile technology.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 argues that Answer B (\"North Korea has announced plans to launch a satellite into orbit, which the US sees as a cover for testing ballistic missile technology\") is more consistent with the story. They cite the lead paragraph mentioning the satellite launch and provide a verified quote about North Korea's recent nuclear test and missile launches complicating diplomacy. This supports the idea that the satellite launch is seen as a cover for missile testing.<br>- expert_debater_mixtral_8_7B argues that Answer A (\"Diplomatic efforts to secure the release of American journalists Laura Ling and Euna Lee, who are accused of spying in North Korea and face trial, are ongoing\") is more consistent. They provide verified quotes showing the story focuses extensively on the journalists' situation, potential consequences, and diplomatic efforts for their release.<br><br>2. Evaluating which answer is more likely correct:<br>- While the satellite launch is mentioned in the lead, the bulk of the story does seem to revolve around the journalists' situation based on the verified quotes provided.<br>- However, the verified quote about North Korea's recent actions complicating diplomacy does lend credence to the idea that the satellite launch is a major issue related to the journalists' predicament.<br>- On balance, the story appears to be framed around the journalists as the central issue, with the satellite launch providing important context and complications for diplomacy.<br><br>3. My chosen answer is A: \"Diplomatic efforts to secure the release of American journalists Laura Ling and Euna Lee, who are accused of spying in North Korea and face trial, are ongoing.\"<br><br>4. Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-139946.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-139946</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Democrats gain 60 seats in Senate, giving them a filibuster-proof majority.<br> Answer B:This filibuster-proof majority means Democrats can pass legislation without Republican support.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 relies heavily on the verified quote from Gloria Borger: \"Well, it means that they've got 60 seats, which would give them a filibuster-proof majority.\" This quote supports the idea that Democrats having 60 seats allows them to pass legislation without Republican support.<br>- expert_debater_mixtral_8_7B argues that a filibuster-proof majority does not necessarily mean Democrats can pass legislation without any Republican support, as Democrats may not always agree. However, this debater does not provide verified quotes to back up this claim.<br><br>2. Analyzing the answers:<br>- Answer A: \"Democrats gain 60 seats in Senate, giving them a filibuster-proof majority.\" This is not fully supported by the verified quote, which states they \"got\" 60 seats, not that they \"gained\" 60 seats.<br>- Answer B: \"This filibuster-proof majority means Democrats can pass legislation without Republican support.\" This is directly supported by the verified quote from Gloria Borger, which states that having 60 seats gives them a filibuster-proof majority to pass bills without needing Republican votes.<br><br>3. Based on the verified quote and the evidence presented, Answer B is more likely to be correct. While Democrats may not always agree, the filibuster-proof majority does give them the ability to pass legislation without needing Republican support, as stated by Gloria Borger.<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-145383.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-145383</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Over \\$260 million in private funding will support programs to engage students, improve teacher effectiveness, and promote hands-on learning.<br> Answer B:The private sector has committed over \\$260 million to the campaign, and the initial focus is on reaching young students with interactive science content.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 provides several verified quotes that support the focus on engaging young students with interactive science content, such as:<br>&lt;v quote&gt;\"The initial commitment of the private sector to this campaign is more than \\$260 million, and we only expect the campaign to grow.\"&lt;/v quote&gt;<br>&lt;v quote&gt;\"Discovery Communications is going to deliver interactive science content to 60,000 schools, reaching 35 million students.\"&lt;/v quote&gt;<br>&lt;v quote&gt;\"Students will launch rockets, construct miniature windmills and get their hands dirty. They'll have the chance to build and create, and maybe destroy just a little bit...\"&lt;/v quote&gt;<br>These quotes directly support Answer B's claim about the initial focus on reaching young students with interactive content.<br><br>- expert_debater_mixtral_8_7B argues for Answer A, citing that the funding supports programs to engage students, improve teacher effectiveness, and promote hands-on learning. However, the argument does not provide verified quotes to support the claim about improving teacher effectiveness.<br><br>2. Evaluating the answers:<br>- Answer A states that the funding will support programs to engage students, improve teacher effectiveness, and promote hands-on learning. While the verified quotes support engaging students and promoting hands-on learning, there is no direct evidence provided about improving teacher effectiveness.<br>- Answer B states that the initial focus is on reaching young students with interactive science content. This is directly supported by the verified quotes provided by expert_debater_sonnet_v3.<br><br>3. Based on the verified quotes and evidence presented, Answer B (\"The private sector has committed over \\$260 million to the campaign, and the initial focus is on reaching young students with interactive science content\") appears to be the most factually consistent with the story.<br><br>Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-164885.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-164885</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Cuba celebrated the 50th anniversary of their victory over a US-backed invasion at the Bay of Pigs.<br> Answer B:170,000 Cubans have private businesses.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 provides a verified quote directly stating the number of private businesses: \"&lt;v quote&gt;I read something like 170,000 Cubans have actually taken out licenses to open their own business.&lt;/v quote&gt;\" This quote supports Answer B and is a strong piece of evidence.<br>- expert_debater_sonnet_v3 also provides verified quotes about Cuba's economic reforms allowing more private enterprise, further supporting Answer B.<br>- expert_debater_mixtral_8_7B focuses on the Bay of Pigs anniversary celebrations but does not provide any verified quotes directly related to Answer A.<br>- The quotes expert_debater_mixtral_8_7B provides are either unverified or do not directly support Answer A over Answer B.<br><br>2. Considering which answer is more likely to be correct:<br>- Answer B has direct evidence from a verified quote stating the specific number of private businesses.<br>- Answer A does not have any verified quotes directly supporting it as the most factually consistent summary.<br>- While the Bay of Pigs anniversary is discussed, the evidence for the number of private businesses seems more concrete and factually consistent with the story based on the verified quotes provided.<br><br>3. The answer I choose as most likely to be correct is Answer B: \"170,000 Cubans have private businesses.\"<br><br>Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-173359.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-173359</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting.<br> Answer B:Though Jackson was in good health, these sedatives and anesthetics stopped his breathing and led to his death.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Strengths and weaknesses of arguments:<br>- expert_debater_sonnet_v3 provides verified quotes showing that Jackson was in good health, the drugs stopped his breathing, and Propofol should not be used outside a hospital setting. These quotes support Answer A.<br>- expert_debater_mixtral_8_7B acknowledges the verified quotes but argues that the key point is the inappropriate use of drugs outside a hospital setting, which also supports Answer A.<br>- Neither debater provides verified quotes directly contradicting the other's arguments.<br><br>2. Evaluating which answer is likely correct:<br>- Both debaters agree that the drugs, including Propofol, were used inappropriately outside a hospital setting, which led to Jackson's death despite his good health.<br>- The verified quotes support the notion that the use of multiple drugs together, including Propofol, outside a hospital setting was the key factor in Jackson's death.<br>- Therefore, Answer A: \"The use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting\" is the most factually consistent summary based on the verified evidence presented.<br><br>3. Answer chosen as correct: A<br><br>Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-197627.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-197627</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The document mentions that President Obama will be attending an interfaith vigil in Newtown, Connecticut to provide comfort to the families and first responders affected by the shooting at Sandy Hook Elementary School.<br> Answer B:Connecticut police confirmed Adam Lanza fired hundreds of bullets, killing 26.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 provides verified quotes that directly state details about the shooting, such as the number of bullets fired, the weapon used, and the number of victims. These quotes support Answer B.<br>- expert_debater_mixtral_8_7B provides a verified quote about President Obama attending a vigil to provide comfort to the families and first responders. This quote supports Answer A.<br>- Neither debater provides verified quotes that fully match the wording of the answer choices, so I cannot rely solely on direct quote matches.<br><br>2. Analyzing the answers step-by-step:<br>- Answer A focuses on President Obama attending a vigil, which is supported by the verified quote provided by expert_debater_mixtral_8_7B.<br>- Answer B focuses on details about the shooting itself, such as the number of bullets fired, the weapon used, and the number of victims, which is supported by the verified quotes provided by expert_debater_sonnet_v3.<br>- The story seems to be primarily about reporting the details of the shooting, with President Obama's attendance at the vigil being a secondary detail.<br>- Therefore, Answer B, which summarizes the core details of the shooting, is likely to be more factually consistent with the main focus of the story.<br><br>3. The answer I choose as correct is Answer B: \"Connecticut police confirmed Adam Lanza fired hundreds of bullets, killing 26.\"<br><br>Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-201245.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-201245</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:New York City Mayor Michael Bloomberg is planning to ban the use of Styrofoam in stores and restaurants due to environmental concerns.<br> Answer B:What is the proposed ban on Styrofoam in stores?</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 relies heavily on the verified quote to argue that the story only mentions a proposed ban on Styrofoam/plastic foam packaging in stores, not restaurants, and does not state any environmental motivations. This argument is supported by evidence from the verified quote.<br>- expert_debater_mixtral_8_7B argues that while environmental motivations are not explicitly stated, Bloomberg's history of public health and environmental initiatives suggests the ban is likely motivated by environmental concerns. However, this is an inference not directly supported by verified quotes from the story.<br><br>2. Analyzing which answer is more likely correct:<br>- Answer A states that the proposed ban is due to environmental concerns, which is not directly supported by verified quotes from the story.<br>- Answer B simply asks about the proposed ban on Styrofoam in stores, which is directly supported by the verified quote mentioning Bloomberg's plan to \"ban Styrofoam food packaging in stores.\"<br>- While environmental motivations are plausible given Bloomberg's history, the story itself does not provide verified evidence to support this claim. It only states the details of the proposed ban.<br>- Therefore, based on the verified evidence presented, Answer B is more factually consistent with the story by simply asking about the proposed ban without making unsupported claims about the motivations.<br><br>3. The answer I choose as most likely correct is Answer B: \"What is the proposed ban on Styrofoam in stores?\"<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-229050.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-229050</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The physician describes the phenomenon that saved the boy's life as a perfect storm of disaster that allowed him to survive by slowing down his body's need for oxygen and putting him in a frozen state until his oxygen was replenished.<br> Answer B:He is shocked that the recent 15-year-old stowaway survived with no lasting side effects.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Strengths and weaknesses of arguments:<br>- expert_debater_sonnet_v3 provides verified quotes showing the doctor's surprise at the stowaway having no obvious effects upon discharge, contrasting with a previous patient. This supports Answer B.<br>- expert_debater_mixtral_8_7B provides a verified quote of the doctor describing the \"perfect storm\" phenomenon that allowed the stowaway to survive, supporting Answer A. However, this quote does not directly contradict the lack of lasting effects.<br><br>2. Evaluating which answer is more likely correct:<br>- Answer A focuses on the survival mechanism, which the verified quote supports, but does not directly address lasting effects.<br>- Answer B focuses on the lack of lasting effects, which the verified quotes more directly support by contrasting this case with a previous patient who did have effects.<br>- While the survival mechanism is important context, the question asks about the most factually consistent summary, which seems to be Answer B based on the verified evidence provided.<br><br>3. The answer I believe is most likely correct is Answer B: \"He is shocked that the recent 15-year-old stowaway survived with no lasting side effects.\"<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-239067.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-239067</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Currency uncertainty would be a major issue in the event of a yes vote in the Scotland referendum on independence.<br> Answer B:Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the debaters' arguments:<br><br>expert_debater_sonnet_v3:<br>- Correctly points out that the transcript does not use strong language like \"major issue\" regarding currency uncertainty, only that it is \"one of the first questions\" for investors.<br>- Highlights the challenge of establishing credibility for a new Scottish currency, which is supported by the verified quote: \"And they could have their own currency, but that's not a magic elixir when you don't have long credibility.\"<br>- The summary \"Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility\" aligns well with the verified quotes.<br><br>expert_debater_mixtral_8_7B:<br>- Argues that currency uncertainty is a \"major issue\" based on the verified quote stating it is \"one of the first questions people will have, it's one of the first questions investors have.\"<br>- However, the quote does not directly support labeling it as a \"major issue\" - it only states that it is one of the first questions, not that it is a major issue.<br>- The summary \"Currency uncertainty would be a major issue in the event of a yes vote in the Scotland referendum on independence\" is an overstatement not fully supported by the verified quotes.<br><br>2. Evaluating which answer is likely correct:<br>Answer A: \"Currency uncertainty would be a major issue in the event of a yes vote in the Scotland referendum on independence.\"<br>- This answer is an overstatement not fully supported by the verified quotes, which only state that currency uncertainty is one of the first questions, not that it is a major issue.<br><br>Answer B: \"Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility.\"<br>- This answer is more consistent with the verified quotes, which highlight the uncertainty around what currency would be used and the challenge of establishing credibility for a new currency.<br><br>3. Based on the evidence from the verified quotes and the debaters' arguments, Answer B is more likely to be correct.<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, False, False, False, True, False, False, False, False]\n",
      "CPU times: user 177 ms, sys: 12.9 ms, total: 190 ms\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "debate_judge_regular_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------DEBATE  JUDGE Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    judge_response = invoke_claude_judge_debate(debate_id = debate_id,\n",
    "                              question=question,\n",
    "                 answer_a = answer_a,\n",
    "                 answer_b = answer_b)\n",
    "    debate_judge_regular_answers.append(extract_final_answer(judge_response, flipped=False))\n",
    "    logger.info(f\" >>>>> invoke_mistral_judge_debate - judge_response  >>>>> {judge_response}\")\n",
    "    # Print the final response \n",
    "    format_final_response(debate_id, \n",
    "                          round_num=1, \n",
    "                          question=question, \n",
    "                          answer_a=answer_a, \n",
    "                          answer_b=answer_b, \n",
    "                          judge_response=judge_response)\n",
    "print(debate_judge_regular_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a46272-45c5-4e0f-be8e-0a0850e5df8b",
   "metadata": {},
   "source": [
    "### <a name=\"3\">LLM Debate: 2 expert LLMs, 1 naive judge - LLM-1 arguing for 2nd summary</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7f903-969c-427d-a1aa-732d0db39ba2",
   "metadata": {},
   "source": [
    "In this **flipped LLM Debate** - Claude(LLM-1) defends correct Summary and Mixtral(LLM-2) defends incorrect summary.\n",
    "\n",
    "\n",
    "Claude v3(Sonnet) argues for answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. Mixtral 8X7B argues for answer A(Ground Truth:False Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of the debate, Claude as a judge adjudicates whether Claude's or Mixtral's rationale is correct and chooses a side to give the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0cd6c9f-880e-4a70-a625-8c6a964a4947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-104129 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-104129 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-138971 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-138971 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-139946 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-139946 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-145383 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-145383 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-164885 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-164885 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-173359 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-173359 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-197627 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-197627 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-201245 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-201245 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-229050 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-229050 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-239067 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-239067 Round #1..3 ======= \n",
      "\n",
      "CPU times: user 838 ms, sys: 48.7 ms, total: 887 ms\n",
      "Wall time: 16min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(20) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    \n",
    "    #### defending True - Claude\n",
    "    claude_defending_summary=answer_a\n",
    "    claude_opposing_summary=answer_b\n",
    "\n",
    "    #### defending False - Mixtral\n",
    "    mixtral_defending_summary=answer_b\n",
    "    mixtral_opposing_summary=answer_a\n",
    "    \n",
    "    delete_file(f\"./transcripts/full_transcript_debate_{debate_id}{FLIPPED_FILE_SUFFIX}.log\")\n",
    "\n",
    "    logger.info(f\"-------------2 model Debate -> Debate_id {debate_id}-------------------\")\n",
    "    print(f\"=========== START OF 2 model FLIPPED DEBATE debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        logger.info(f\"START Debate with Claude Round #{round_number + 1} >>>>>> \\n\") \n",
    "        claude_debate_response = invoke_claude_v3(debate_id = debate_id + FLIPPED_FILE_SUFFIX,\n",
    "                         question=question,\n",
    "                         round_number = round_number + 1,\n",
    "                         summary_defending = claude_defending_summary, \n",
    "                         summary_opposing = claude_opposing_summary, \n",
    "                         complete_interview = complete_interview_transcript,\n",
    "                         debate=True\n",
    "                         )\n",
    "\n",
    "        logger.info(f\" >>>>> claude_debate_response Round #{round_number + 1} >>>>> {claude_debate_response}\")\n",
    "        logger.info(f\"END Debate with Claude Round #{round_number + 1} >>>>>> \\n\")\n",
    "\n",
    "        mixtral_debate_response = invoke_mistral(debate_id = debate_id + FLIPPED_FILE_SUFFIX,\n",
    "                     question=question,\n",
    "                     round_number = round_number + 1,\n",
    "                     summary_defending = mixtral_defending_summary, \n",
    "                     summary_opposing = mixtral_opposing_summary, \n",
    "                     complete_interview = complete_interview_transcript, \n",
    "                     )\n",
    "\n",
    "        logger.info(f\" >>>>> mixtral_debate_response Round #{round_number + 1} >>>>> {mixtral_debate_response}\")\n",
    "        logger.info(f\"END Debate with Mixtral Round #{round_number + 1} >>>>>> \\n\")\n",
    "    print(f\"=========== END OF 2 model FLIPPED DEBATE debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73c855-24d5-4893-8b10-afc91936bf04",
   "metadata": {},
   "source": [
    "## JUDGE for flipped LLM Debate:LLM-Claude arguing for 1st summary, LLM-Mixtral arguing for 2nd summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee103881-33b6-473f-8d9b-ad7a69aa20df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-104129_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-104129</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:GM lost \\$10.6B in 2005, continues losing market share.<br> Answer B:GM lost \\$10B in 2005, continues losing market share.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Strengths and weaknesses of arguments:<br>- expert_debater_sonnet_v3 correctly points out that the story explicitly states GM's 2005 loss as \"\\$10.6 billion\", which is a verified quote. This supports Answer A being more factually consistent.<br>- expert_debater_mixtral_8_7B acknowledges the discrepancy in the loss figure but argues that the overall message of GM's struggles and market share decline is more important than the precise number. However, this argument is weaker since the question asks for the \"most factually consistent\" summary.<br><br>2. Evaluating which answer is likely correct:<br>- The verified quote \"&lt;v quote&gt;General Motors reporting a \\$10.6 billion loss in 2005&lt;/v quote&gt;\" directly contradicts Answer B's claim of a \\$10 billion loss.<br>- While the overall message of GM's struggles is important, the question specifically asks for factual consistency, which Answer A provides by accurately stating the \\$10.6 billion loss figure from the story.<br>- There are no verified quotes supporting Answer B's \\$10 billion loss figure.<br><br>3. Answer chosen as correct: Answer A (\"GM lost \\$10.6B in 2005, continues losing market share\") is more factually consistent with the verified quote from the story.<br><br>Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-138971_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-138971</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Diplomatic efforts to secure the release of American journalists Laura Ling and Euna Lee, who are accused of spying in North Korea and face trial, are ongoing.<br> Answer B:North Korea has announced plans to launch a satellite into orbit, which the US sees as a cover for testing ballistic missile technology.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Strengths and weaknesses of arguments:<br>- expert_debater_sonnet_v3 provides verified quotes directly related to the detained journalists and diplomatic efforts for their release, supporting Answer A. However, the quotes do not explicitly rule out Answer B.<br>- expert_debater_mixtral_8_7B provides verified quotes about North Korea's recent provocative actions and lack of transparency, which lend context to the US's concerns about the satellite launch being a cover for missile testing. However, there are no verified quotes directly mentioning the satellite launch or linking it to the US's concerns.<br><br>2. Evaluating which answer is likely correct:<br>- Answer A is directly supported by verified quotes about the detained journalists and diplomatic efforts for their release. This is the central focus of the story.<br>- While Answer B captures relevant context about North Korea's behavior and the US's skepticism, there is no verified evidence in the story explicitly confirming the summary about the satellite launch and the US's stance on it.<br>- Without a verified quote directly supporting Answer B, Answer A appears to be the most factually consistent summary based on the verified evidence provided.<br><br>3. Answer chosen as correct: Answer A<br><br>4. Final Answer: A<br>&lt;/thinking&gt;<br><br>Final Answer: A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-139946_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-139946</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Democrats gain 60 seats in Senate, giving them a filibuster-proof majority.<br> Answer B:This filibuster-proof majority means Democrats can pass legislation without Republican support.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Strengths and weaknesses of arguments:<br>- Both debaters acknowledge that the story does not definitively confirm a filibuster-proof majority for Democrats, which weakens the case for Answer B.<br>- However, the verified quote from Gloria Borger suggests that Franken's liberal politics make him more likely to support Obama's agenda, implying Democrats can pass legislation more easily with 60 seats, even if not guaranteed.<br>- Neither debater provides verified quotes that directly confirm or deny Answer A, so it remains a possibility based on the information given.<br><br>2. Evaluating which answer is likely correct:<br>- Answer A (\"Democrats gain 60 seats in Senate, giving them a filibuster-proof majority\") is not definitively confirmed or denied by the verified quotes. It remains a possibility based on the information provided.<br>- Answer B (\"This filibuster-proof majority means Democrats can pass legislation without Republican support\") is weakened by the acknowledgment that a filibuster-proof majority is not guaranteed, and the lack of verified quotes directly confirming this claim.<br>- Based on the verified quote suggesting Franken's liberal politics align with Obama's agenda, and the lack of evidence definitively ruling out Answer A, Answer A seems more likely to be correct given the information provided.<br><br>3. Answer chosen as correct: Answer A (\"Democrats gain 60 seats in Senate, giving them a filibuster-proof majority\")<br><br>Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-145383_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-145383</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Over \\$260 million in private funding will support programs to engage students, improve teacher effectiveness, and promote hands-on learning.<br> Answer B:The private sector has committed over \\$260 million to the campaign, and the initial focus is on reaching young students with interactive science content.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments and verified quotes:<br><br>expert_debater_sonnet_v3's arguments:<br>- Cites verified quotes that support a broader scope of initiatives beyond just reaching young students, including improving teacher effectiveness, promoting hands-on learning, and engaging students in general.<br>- Verified quotes align with the key points in Answer A about engaging students, improving teacher effectiveness, and promoting hands-on learning.<br><br>expert_debater_mixtral_8_7B's arguments:<br>- Focuses heavily on the initiative of reaching young students with interactive science content, citing the verified quote about Sesame Street's initiative.<br>- Acknowledges other aspects like teacher effectiveness and hands-on learning, but does not provide verified quotes to support those points.<br>- Answer B is narrower in scope, focusing primarily on reaching young students with interactive content.<br><br>2. Evaluating which answer is more likely correct:<br>- Answer A covers a broader range of initiatives mentioned in the verified quotes, including engaging students, improving teacher effectiveness, and promoting hands-on learning.<br>- While reaching young students is a key focus, the verified quotes suggest a more comprehensive approach beyond just interactive content for that age group.<br>- Answer B is too narrow in scope, focusing primarily on reaching young students with interactive content, and lacking verified quote support for the other initiatives.<br><br>3. Based on the verified quotes and the broader scope of initiatives they support, Answer A seems to be the more factually consistent summary.<br><br>Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-164885_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-164885</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Cuba celebrated the 50th anniversary of their victory over a US-backed invasion at the Bay of Pigs.<br> Answer B:170,000 Cubans have private businesses.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the debaters' arguments:<br><br>expert_debater_sonnet_v3:<br>- Provides a verified quote supporting the celebration of the Bay of Pigs anniversary: &lt;v quote&gt;It was 50 years ago this small Caribbean island defeated a U.S.-backed invasion of the Bay of Pigs.&lt;/v quote&gt;<br>- Claims the segment focuses on the anniversary celebration, making Answer A more factually consistent.<br>- Later acknowledges the mention of private businesses but argues it is speculative and limited compared to the extensive coverage of the anniversary.<br><br>expert_debater_mixtral_8_7B:<br>- Initially argues correctly that the segment supports Answer A about the Bay of Pigs anniversary celebration.<br>- However, in round 3, makes an incorrect claim that the story is about a NATO bombing in Afghanistan, which is not supported by any verified quotes.<br>- In round 4, contradicts their previous argument by claiming Answer A is more factually consistent, despite earlier arguing for Answer B.<br><br>2. Evaluating which answer is more likely correct:<br>- Answer A is supported by a verified quote directly mentioning the celebration of the Bay of Pigs anniversary victory.<br>- While the segment mentions private businesses, this appears to be a secondary focus compared to the extensive coverage of the anniversary celebration.<br>- There are no verified quotes directly supporting the specific claim of \"170,000 Cubans have private businesses\" in Answer B.<br><br>3. Based on the verified evidence presented, Answer A seems more factually consistent with the overall content described in the segment.<br><br>4. Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-173359_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-173359</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting.<br> Answer B:Though Jackson was in good health, these sedatives and anesthetics stopped his breathing and led to his death.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments and verified quotes:<br><br>expert_debater_sonnet_v3's arguments:<br>- Relies heavily on the verified quote from Dr. Cain: &lt;v quote&gt;\"Propofol should not be used in home settings\"&lt;/v quote&gt;<br>- Emphasizes that the key issue was using Propofol and other sedatives together outside of a hospital setting, which is not recommended.<br>- Supports Answer A: \"The use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting.\"<br><br>expert_debater_mixtral_8_7B's arguments:<br>- Cites the verified quote from Dr. Cain confirming the drugs found in Jackson's system, including Propofol.<br>- Highlights the large quantity of Propofol found, enough for \"500 people\" or \"almost two months\" of use.<br>- Argues that Jackson died due to an overdose of these drugs, supporting Answer B: \"Though Jackson was in good health, these sedatives and anesthetics stopped his breathing and led to his death.\"<br><br>2. Evaluating which answer is more likely correct:<br>- Both debaters provide valid points based on verified quotes from the expert, Dr. Cain.<br>- However, expert_debater_sonnet_v3's arguments focus more directly on Dr. Cain's central warning about the inappropriate use of Propofol and other sedatives outside of a hospital setting, which aligns closely with Answer A.<br>- While expert_debater_mixtral_8_7B highlights the role of the drug overdose in Jackson's death, the verified quotes do not explicitly state that Jackson was in good health, as mentioned in Answer B.<br><br>3. Based on the evidence presented and the verified quotes, Answer A seems to be the more factually consistent summary of Dr. Cain's statements.<br><br>Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-197627_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-197627</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The document mentions that President Obama will be attending an interfaith vigil in Newtown, Connecticut to provide comfort to the families and first responders affected by the shooting at Sandy Hook Elementary School.<br> Answer B:Connecticut police confirmed Adam Lanza fired hundreds of bullets, killing 26.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 provides verified quotes that directly support Answer A about President Obama attending an interfaith vigil in Newtown to comfort families affected by the Sandy Hook shooting. These quotes seem trustworthy and relevant to the question.<br>- expert_debater_mixtral_8_7B provides one verified quote that directly supports Answer B about Adam Lanza firing hundreds of bullets and killing 26 people. This quote is factually accurate but narrowly focused on shooting details.<br><br>2. Considering which answer is more likely correct:<br>- Answer A captures a key aspect of the story - President Obama's visit and the interfaith vigil, which the verified quotes suggest is a central focus of the narrative.<br>- Answer B is factually accurate based on the verified quote but seems to miss the broader context and focus of the story on Obama's visit and the vigil.<br>- The question asks for the \"most factually consistent\" summary, not necessarily the most significant or newsworthy detail.<br>- However, a summary that captures the overall narrative and key focus is likely to be more factually consistent with the story as a whole.<br><br>3. Based on the verified evidence presented, Answer A seems to provide a more complete and factually consistent summary of the story by capturing its central focus on President Obama's upcoming visit and the interfaith vigil, while also being consistent with the shooting details mentioned in Answer B.<br><br>Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-201245_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-201245</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:New York City Mayor Michael Bloomberg is planning to ban the use of Styrofoam in stores and restaurants due to environmental concerns.<br> Answer B:What is the proposed ban on Styrofoam in stores?</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments and verified quotes:<br>- The only verified quote is: &lt;v quote&gt;\"Now, Mayor Michael Bloomberg is out to ban Styrofoam food packaging in stores. It's actually not Styrofoam technically, it's plastic foam. He's looking to ban that in stores and food restaurants.\"&lt;/v quote&gt;<br>- This quote directly supports Answer B (\"What is the proposed ban on Styrofoam in stores?\") by stating that Bloomberg is proposing to ban plastic foam packaging in stores.<br>- The quote also mentions restaurants, but Answer B does not explicitly contradict this by saying the ban is only for stores.<br>- Answer A (\"New York City Mayor Michael Bloomberg is planning to ban the use of Styrofoam in stores and restaurants due to environmental concerns.\") goes beyond the verified quote by specifying the reason as environmental concerns, which is not stated in the quote.<br><br>2. Thinking step-by-step about the correct answer:<br>- The verified quote provides factual evidence about the proposed ban on plastic foam packaging in stores.<br>- While the quote also mentions restaurants, Answer B does not definitively exclude restaurants from the proposed ban.<br>- Answer A makes an additional claim about the reason being environmental concerns, which is not supported by the verified quote.<br>- Therefore, based on the verified quote, Answer B is more factually consistent as it accurately reflects the proposed ban in stores without making unsupported claims about the reason or including restaurants.<br><br>3. The answer I choose as correct is Answer B: \"What is the proposed ban on Styrofoam in stores?\"<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-229050_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-229050</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The physician describes the phenomenon that saved the boy's life as a perfect storm of disaster that allowed him to survive by slowing down his body's need for oxygen and putting him in a frozen state until his oxygen was replenished.<br> Answer B:He is shocked that the recent 15-year-old stowaway survived with no lasting side effects.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments and verified quotes:<br><br>expert_debater_sonnet_v3's arguments:<br>- Relies heavily on unverified quotes like \"perfect storm of disaster\", \"slowing down your body's need for oxygen\", and \"suspended in time until your oxygen is replenished\". These cannot be fully trusted without verification.<br>- The only verified quote is: &lt;v quote&gt;\"puts you in a frozen or some people termed it as a suspended state\"&lt;/v quote&gt;. This supports the idea of the boy entering a suspended state, but does not fully confirm Answer A's description of the phenomenon.<br><br>expert_debater_mixtral_8_7B's arguments:<br>- Uses the verified quote: &lt;v quote&gt;\"I thought in my lifetime I would never even hear of another case like this, let alone hear of a case where the gentleman who was a stowaway walks away from the incident.\"&lt;/v quote&gt; This directly supports the doctor's shock at the stowaway surviving without lasting side effects, as stated in Answer B.<br>- Also cites the verified quote: &lt;v quote&gt;\"Back in 2000, the patient that rolled in by paramedics was not in the same state as this young boy this year. He was literally frozen, almost cartoonishly frozen.\"&lt;/v quote&gt; This contrasts the 2000 case with the recent one, further supporting the remarkable nature of the 15-year-old's survival without side effects.<br><br>2. Evaluating which answer is more likely correct:<br>Answer A provides a detailed physiological explanation, but relies heavily on unverified quotes that cannot be fully trusted. The one verified quote supports the idea of a suspended state, but does not confirm all the specifics described in Answer A.<br><br>Answer B is more directly supported by the verified quotes, which clearly show the doctor's shock at the stowaway surviving without lasting side effects, and contrast this case with the 2000 one where the stowaway was in critical condition.<br><br>3. Based on the evidence presented and the verified quotes, Answer B seems more likely to be the correct, factually consistent summary.<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-239067_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-239067</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Currency uncertainty would be a major issue in the event of a yes vote in the Scotland referendum on independence.<br> Answer B:Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 relies on the verified quote from Kenneth Rogoff: &lt;v quote&gt;When you got through with this divorce, suddenly you don\\'t have the currency. What currency do you have? That\\'s one of the first questions people will have, it\\'s one of the first questions investors have.&lt;/v quote&gt; This quote supports the idea that currency uncertainty would be a major issue for an independent Scotland.<br>- expert_debater_mixtral_8_7B also uses the same verified quote from Rogoff, but argues that it shows there are specific questions about what currency would be used and how to establish credibility, which Answer B captures.<br>- Both debaters use unverified quotes that cannot be fully trusted.<br><br>2. Analyzing the answers:<br>- Answer A: \"Currency uncertainty would be a major issue in the event of a yes vote in the Scotland referendum on independence.\" This answer is supported by Rogoff's verified quote, which highlights the uncertainty around currency as a major issue.<br>- Answer B: \"Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility.\" This answer is also supported by Rogoff's quote, as it raises specific questions about the currency and establishing credibility.<br><br>However, Rogoff's quote seems to emphasize the overall currency uncertainty as the major issue, rather than just the specific questions raised. Answer A captures this core point more directly.<br><br>3. Answer chosen as correct: Answer A<br><br>4. Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, False, False, True]\n",
      "CPU times: user 130 ms, sys: 9.11 ms, total: 139 ms\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "debate_judge_flipped_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------DEBATE FLIPPED JUDGE Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    judge_response = invoke_claude_judge_debate(debate_id = debate_id + FLIPPED_FILE_SUFFIX,\n",
    "                              question=question,\n",
    "                              answer_a = answer_a,\n",
    "                              answer_b = answer_b)\n",
    "    debate_judge_flipped_answers.append(extract_final_answer(judge_response, flipped=False))\n",
    "    logger.info(f\" >>>>> Flipped invoke_mistral_judge_debate - judge_response  >>>>> {judge_response}\")\n",
    "    \n",
    "    # Print the final response \n",
    "    format_final_response(debate_id, \n",
    "                          round_num=1, \n",
    "                          question=question, \n",
    "                          answer_a=answer_a, \n",
    "                          answer_b=answer_b, \n",
    "                          judge_response=judge_response)\n",
    "print(debate_judge_flipped_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd391f-2c5b-4385-84d9-38da4515699f",
   "metadata": {},
   "source": [
    "## <a name=\"4\">Accuracy of LLM Debate</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dee9e72-42c2-4963-a68c-60941c472e29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, False, False, False, True, False, False, False, False]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debate_judge_regular_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26b689ae-7377-41db-afce-c80bf65fc1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True, True, False, False, True]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debate_judge_flipped_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c47c3faa-1ddd-4f7b-864f-efac1c23a155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_debate_judge = find_num_matching_elements(debate_judge_regular_answers, debate_judge_flipped_answers)/total_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f13e301-ce93-49d5-ab48-430245eff5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_debate_judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab6f33fc-52aa-442a-bc76-ed55f15d560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_naive_judge': 0.2, 'accuracy_expert_judge': 0.4, 'accuracy_consultant_judge': 0.4, 'accuracy_debate_judge': 0.4}\n",
      "notebook results saved in results folder\n"
     ]
    }
   ],
   "source": [
    "# save the results\n",
    "results_dict = {\"accuracy_debate_judge\" : accuracy_debate_judge}\n",
    "save_each_experiment_result(results_dict)\n",
    "print(\"notebook results saved in results folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fbd255-a593-438f-a44f-fce489056f26",
   "metadata": {},
   "source": [
    "## <a name=\"5\">Compare Accuracies across experiments/methods.</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97c205-fa79-4c09-b115-0cd3e9cae864",
   "metadata": {},
   "source": [
    "Here we compare the accuracies of each method/experiment to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39472b99-25dc-4d47-8242-450df4c13559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_naive_judge': 0.2, 'accuracy_expert_judge': 0.4, 'accuracy_consultant_judge': 0.4, 'accuracy_debate_judge': 0.4}\n",
      "{'accuracy_naive_judge': 0.2, 'accuracy_expert_judge': 0.4, 'accuracy_consultant_judge': 0.4, 'accuracy_debate_judge': 0.4}\n",
      "{'accuracy_naive_judge': 0.2, 'accuracy_expert_judge': 0.4, 'accuracy_consultant_judge': 0.4, 'accuracy_debate_judge': 0.4}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Naive Judge</th>\n",
       "      <th>Expert Judge</th>\n",
       "      <th>LLM Consultancy</th>\n",
       "      <th>LLM Debate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_naive_judge = get_each_experiment_result(\"accuracy_naive_judge\")\n",
    "accuracy_expert_judge = get_each_experiment_result(\"accuracy_expert_judge\")\n",
    "accuracy_consultant_judge = get_each_experiment_result(\"accuracy_consultant_judge\")\n",
    "\n",
    "final_accuracy_comparison(\n",
    "    accuracy_naive_judge = accuracy_naive_judge,\n",
    "    accuracy_expert_judge = accuracy_expert_judge,\n",
    "    accuracy_consultant_judge = accuracy_consultant_judge,\n",
    "    accuracy_debate_judge = accuracy_debate_judge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e751a8d-fd46-4192-b100-655e8342ffe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the plot\n",
    "x_values = [ \"Naive Judge\", \"Expert Judge\", \"LLM Consultant\", \"LLM Debate\"]\n",
    "y_values = [ accuracy_naive_judge, accuracy_expert_judge, accuracy_consultant_judge, accuracy_debate_judge]\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Compare Accuracies across experiments')\n",
    "plt.xlabel('Experiment Type')\n",
    "plt.ylabel('Accuracy')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b11eae-cb6b-4196-a91e-56b2f3c4b076",
   "metadata": {},
   "source": [
    "### <a name=\"6\">Choose expert LLM using Win Rate measured during LLM Debate (Experiment 4) </a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0682b92-dcda-4949-b9ac-49c027e18226",
   "metadata": {},
   "source": [
    "With this win rate of expert models, we emprically understand which LLM as a debater is more successful than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cc2b8aa-0335-4d8a-811d-9da60dd69826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_common_regular_value =False , regular_count = 8\n",
      "most_common_flipped_value =True , flipped_count = 8\n",
      "\n",
      " claude_regular_win_rate :: 0.8 \n",
      "                \n",
      " mistral_regular_win_rate :: 0.19999999999999996 \n",
      "                \n",
      " claude_flipped_win_rate :: 0.8\n",
      "                \n",
      " mistral_flipped_win_rate :: 0.19999999999999996 \n"
     ]
    }
   ],
   "source": [
    "claude_avg_win_rate, mixtral_avg_win_rate = get_win_rate_per_model(\n",
    "    debate_judge_regular_answers, \n",
    "    debate_judge_flipped_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d719aa86-ca8b-436a-8b6d-9ca1a3ba08db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claude Win Rate</th>\n",
       "      <th>Mixtral Win Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "win_rate_comparison(claude_avg_win_rate, mixtral_avg_win_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc1319a9-e85c-4d29-92d7-60acd1a21e51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the plot\n",
    "%matplotlib inline\n",
    "x_values = [ \"Claude v3 Sonnet\", \"Mixtral\"]\n",
    "y_values = [claude_avg_win_rate, mixtral_avg_win_rate]\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Compare average win-rate across expert LLMs')\n",
    "plt.xlabel('average win-rate')\n",
    "plt.ylabel('win-rate')\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
