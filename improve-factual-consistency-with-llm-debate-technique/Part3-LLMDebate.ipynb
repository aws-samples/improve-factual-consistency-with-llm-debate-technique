{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604ee77c-0e19-4a36-9f70-e8ab02cfaf54",
   "metadata": {},
   "source": [
    "<center><img src=\"images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">Improve Factual Consistency Part 3 </a>\n",
    "## <a name=\"0\">Improving Factual Consistency and Explainability using LLM Debates </a>\n",
    "\n",
    "### Glossary of Terms\n",
    "- Naive Judge : This LLM has **no** access to transcript but only question and two summaries. Measure the baseline performance.\n",
    "- Expert Judge : This LLM has access to transcript along with question and two summaries\n",
    "- Question asked to LLM (in all experiments): It is always the same: `Which one of these summaries is the most factually consistent one?`\n",
    "\n",
    "## Dataset\n",
    "Our dataset is distilled from the Amazon Science evaluation benchmark dataset called <a href=\"https://github.com/amazon-science/tofueval\">TofuEval</a>. 10 summaries have been curated from the [MediaSum documents](https://github.com/zcgzcgzcg1/MediaSum) inside the tofueval dataset for this notebook. \n",
    "\n",
    "MediaSum is a large-scale media interview dataset contains 463.6K transcripts with abstractive summaries, collected from interview transcripts and overview / topic descriptions from NPR and CNN.\n",
    "\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "In this notebook, we navigate the LLM debating technique with more persuasive LLMs having two expert debater LLMs (Claude and Mixtral) and one judge (using Claude - we can use others like Mistral/Mixtral, Titan Premier) to measure, compare and contrast its performance against other techniques like self-consistency (with naive and expert judges) and LLM consultancy. This notebook is an adapted and partial implementation of one of the ICML 2024 best papers, <a href=\"https://arxiv.org/pdf/2402.06782\"> Debating with More Persuasive LLMs Leads to More Truthful Answers </a> on a new and different Amazon Science evaluation dataset <a href=\"https://github.com/amazon-science/tofueval\">TofuEval</a>. \n",
    "\n",
    "\n",
    "- Part 1.  Demonstrate typical Standalone LLM approach\n",
    "\n",
    "- Part 2.  Demonstrate the LLM Consultancy approach and compare with Part 1.\n",
    "\n",
    "- Part 3.  **[THIS notebook]**  Demonstrate the LLM Debate approach and compare with other methods.\n",
    "\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    While this notebook(part 1, 2 and 3) compares various methods and demonstrates the efficacy of LLM Debates in notebook part 3 with a supervised dataset, the greater benefit is possible in unsupervised scenarios where ground truth is unknown and ground truth alignment and/or curation is required. Human annotation can be expensive plus slow and agreement amongst human annotators adds another level of intricacy. A possible `scalable oversight direction could be this LLM debating technique to align on the ground truth options` via this debating and critique mechanism by establishing factual consistency(veracity). This alignment and curation of ground truth for unsupervised data could be a possible win direction for the debating technique in terms of cost versus benefit analysis.\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "\n",
    "#### Notebook Kernel\n",
    "Please choose `conda_python3` as the kernel type of the top right corner of the notebook if that does not appear by default.\n",
    "\n",
    "\n",
    "## LLM Access\n",
    "\n",
    "We will need access to Anthropic Claude v3 Sonnet, Mistral 7b and  Mixtral 8x7b LLMs for this notebook.\n",
    "\n",
    "[Anthropic Claude v3(Sonnet)](https://www.anthropic.com/news/claude-3-family) , [Mixtral 8X7B](https://mistral.ai/news/mixtral-of-experts/), [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) - all of them pre-trained on general text summarization tasks.\n",
    "\n",
    "## Use-Case Overview\n",
    "\n",
    "To demonstrate the measurement and improvement of factual consistency (veracity) with explainability in this notebook, we conduct a series of experiments to choose the best summary for each transcript. In each experiment, we measure the veracity and correctness of the summaries generated from transcripts and improve upon the decision to choose the correct one via methods like LLM consultancy and LLM debates.\n",
    "\n",
    "The <b>overall task in this notebook</b> is choose which one of the two summaries is most appropriate for a given transcript. There are a total of 10 transcripts and each transcript has 2 summaries - one correct and other incorrect. The incorrect summaries have various classes of errors like `Nuanced Meaning Shift`, `Extrinsic Information` and  `Reasoning errors`. \n",
    "\n",
    "In this notebook we will conduct the following set of experiment combinations to measure, compare and contrast LLM debating techniques with others.\n",
    "\n",
    "\n",
    "## Experiments\n",
    "For each of these experiments we flip the side of the argument the LLM takes to account for `position bias` and `verbosity bias` and re-run each experiment.\n",
    "\n",
    "**Note** We always use the same Judge LLM (Mistral 7B) across all the experiments in this notebook\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment 4: (LLM Debate) \n",
    "<center><img src=\"images/veracitylab01-llm-debate.png\" alt=\"In this image, we depict the flow of LLM Debate. First Debater LLMs like Claude and Mixtral argue their side\n",
    "based on transcript contents. Next each argument is saved to a file and\n",
    "the next debater picks up the entire argument history before posting their next argument. Finally, once all 3 rounds of arguments are over, the Judge LLM reads all the arguments and decides which summary is the most factually consistent answer.\"  height=\"700\" width=\"700\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "We use Claude 3 as first debater and Mixtral as second debater with Claude as Judge. We let the debater argue their sides and finally let the judge decide which argument is better. This continues for N(=3 in this notebook) rounds. We flip Claude and Mistral argument sides in experiments 4a and 4b and take average of the experiment results as final accuracy. This accounts for errors due to position (choosing an answer due to its order/position) and verbosity bias (one answer longer than the other)\n",
    "\n",
    "##### Experiment 4a: (Claude v3 argues for answer A, Mixtral argues for Answer B): \n",
    "Claude v3(Sonnet) argues for answer A(Ground Truth:False Answer) and generates rationale why that answer is correct. Mixtral 8X7B argues for answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of the debate, Claude as a judge adjudicates whether Claude's or Mixtral's rationale is correct and chooses a side to give the final answer.\n",
    "\n",
    "#####  Experiment 4b: (Claude v3 argues for answer B, Mixtral argues for Answer A): \n",
    "Claude v3(Sonnet) argues for answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. Mixtral 8X7B argues for answer A(Ground Truth:False Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of the debate, Claude as a judge adjudicates whether Claude's or Mixtral's rationale is correct and chooses a side to give the final answer.\n",
    "\n",
    "---\n",
    "## Evaluation Metrics\n",
    "For each type of experiment we evaluate the accuracy of the answers for that experiment/method type to compare and contrast each method at the end.\n",
    "\n",
    "For the final experiment on LLM Debate, we also calculate the `win rate` of the LLM debaters to evaluate which of the LLMs actually got most of the answers right as adjudicated by the judge. This can be considered a mechanism to choose one LLM over the other given this use-case.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This notebook notebook has the following sections:\n",
    "\n",
    "1. <a href=\"#1\">Dataset exploration</a>\n",
    "2. <a href=\"#2\">Accuracy of LLM Debate</a>\n",
    "3. <a href=\"#3\">Compare Accuracies across experiments</a>\n",
    "4. <a href=\"#4\">Choose expert LLM using Win Rate measured during LLM Debate (Experiment 4) </a>\n",
    "5. <a href=\"#5\">Challenge exercise and notebook quiz</a>\n",
    "    \n",
    "Please work top to bottom of this notebook and don't skip sections as this could lead to error messages due to missing code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7190cc0d-ec7d-42c4-bd87-f301e6db48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install setuptools==70.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2ad2f4-b720-48b9-bb5a-7b99bcafce8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q -U pip --root-user-action=ignore\n",
    "!pip3 install -q -r requirements.txt --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8e6507-fc9a-4f1f-8535-7e2deb20a9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# We load all prompts from a separate file prompts.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from prompts import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mlu_utils.veracity_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f573ddc8-9290-484c-86f9-f16531648cac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear_file_contents dir :: <built-in function dir>\n"
     ]
    }
   ],
   "source": [
    "clean_up_files_in_dir(\"./transcripts\")\n",
    "clear_file_contents(\"./log_files/notebook_run_logs.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c84b37-369c-405c-ac08-23be8dbb61a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re, time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import Markdown\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import logging\n",
    "import boto3, warnings\n",
    "import pandas as pd\n",
    "# Supress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(filename='log_files/notebook_run_logs.log', encoding='utf-8', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"----- Test logging setup -----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd02d8-4595-48de-b0cd-394002cbc17a",
   "metadata": {},
   "source": [
    "### Bedrock Model Access check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42bd90d-9cee-4534-90a4-c41bd2c07b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude v3 sonnet looks good\n",
      "Mixtral 8X7B looks good\n",
      "Titan Express looks good\n",
      "Mistral 7B looks good\n",
      "All required model access look good\n"
     ]
    }
   ],
   "source": [
    "#test if all bedrock model access has been enabled \n",
    "test_llm_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c14a1-7d5b-48dc-a4a0-5c5e0a205f57",
   "metadata": {},
   "source": [
    "### Constants used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf134bf-7fbd-4c70-a84c-4b077f79f2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rounds = 3\n",
    "question = \"Which one of these summaries is the most factually consistent one?\"\n",
    "total_data_points = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829f829-7d19-420c-85ea-0e5c370304e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a name=\"1\">Dataset Exploration</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b67d79c9-c30d-4daa-9d25-4dac7de60e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>summ_sent_incorrect_original</th>\n",
       "      <th>summ_sent_correct_manual</th>\n",
       "      <th>exp</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-104129</td>\n",
       "      <td>Decline of American automobile industry</td>\n",
       "      <td>GM lost $10B in 2005, continues losing market ...</td>\n",
       "      <td>GM lost $10.6B in 2005, continues losing marke...</td>\n",
       "      <td>It's not \"$10B\" but \"$10.6B\"</td>\n",
       "      <td>Nuanced Meaning Shift</td>\n",
       "      <td>DOBBS: General Motors today announced it will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN-138971</td>\n",
       "      <td>Diplomatic efforts</td>\n",
       "      <td>North Korea has announced plans to launch a sa...</td>\n",
       "      <td>Diplomatic efforts to secure the release of Am...</td>\n",
       "      <td>The launch of a satellite is not mentioned, bu...</td>\n",
       "      <td>Extrinsic Information</td>\n",
       "      <td>ROBERTS: Welcome back to the Most News in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN-139946</td>\n",
       "      <td>Filibuster-Proof Majority</td>\n",
       "      <td>This filibuster-proof majority means Democrats...</td>\n",
       "      <td>Democrats gain 60 seats in Senate, giving them...</td>\n",
       "      <td>This is an unsupported statement</td>\n",
       "      <td>Extrinsic Information</td>\n",
       "      <td>ANNOUNCER: This is CNN breaking news.\\nMALVEAU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN-145383</td>\n",
       "      <td>Educate to Innovate Campaign</td>\n",
       "      <td>The private sector has committed over $260 mil...</td>\n",
       "      <td>Over $260 million in private funding will supp...</td>\n",
       "      <td>The document does not state that \"reaching you...</td>\n",
       "      <td>Reasoning Error</td>\n",
       "      <td>HARRIS: And President Obama in the Eisenhower ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN-164885</td>\n",
       "      <td>Cuban celebration and government gathering</td>\n",
       "      <td>170,000 Cubans have private businesses.</td>\n",
       "      <td>Cuba celebrated the 50th anniversary of their ...</td>\n",
       "      <td>The document says that 170,000 Cubans have app...</td>\n",
       "      <td>Nuanced Meaning Shift</td>\n",
       "      <td>FEYERICK: We'll get to Donald Trump's campaign...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id                                       topic  \\\n",
       "0  CNN-104129     Decline of American automobile industry   \n",
       "1  CNN-138971                          Diplomatic efforts   \n",
       "2  CNN-139946                   Filibuster-Proof Majority   \n",
       "3  CNN-145383                Educate to Innovate Campaign   \n",
       "4  CNN-164885  Cuban celebration and government gathering   \n",
       "\n",
       "                        summ_sent_incorrect_original  \\\n",
       "0  GM lost $10B in 2005, continues losing market ...   \n",
       "1  North Korea has announced plans to launch a sa...   \n",
       "2  This filibuster-proof majority means Democrats...   \n",
       "3  The private sector has committed over $260 mil...   \n",
       "4            170,000 Cubans have private businesses.   \n",
       "\n",
       "                            summ_sent_correct_manual  \\\n",
       "0  GM lost $10.6B in 2005, continues losing marke...   \n",
       "1  Diplomatic efforts to secure the release of Am...   \n",
       "2  Democrats gain 60 seats in Senate, giving them...   \n",
       "3  Over $260 million in private funding will supp...   \n",
       "4  Cuba celebrated the 50th anniversary of their ...   \n",
       "\n",
       "                                                 exp                   type  \\\n",
       "0                       It's not \"$10B\" but \"$10.6B\"  Nuanced Meaning Shift   \n",
       "1  The launch of a satellite is not mentioned, bu...  Extrinsic Information   \n",
       "2                   This is an unsupported statement  Extrinsic Information   \n",
       "3  The document does not state that \"reaching you...        Reasoning Error   \n",
       "4  The document says that 170,000 Cubans have app...  Nuanced Meaning Shift   \n",
       "\n",
       "                                              source  \n",
       "0  DOBBS: General Motors today announced it will ...  \n",
       "1  ROBERTS: Welcome back to the Most News in the ...  \n",
       "2  ANNOUNCER: This is CNN breaking news.\\nMALVEAU...  \n",
       "3  HARRIS: And President Obama in the Eisenhower ...  \n",
       "4  FEYERICK: We'll get to Donald Trump's campaign...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-process the dataset\n",
    "answers_df = pd.read_csv(\"./tofueval_dataset/mediasum_dev_doc_id_group_final_dual_summaries_manual_final_dataset.csv\")\n",
    "#answers_df.head()\n",
    "interview_df = pd.read_csv(\"./tofueval_dataset/mediasum_dev_doc_complete_final.csv\")\n",
    "#interview_df.head()\n",
    "\n",
    "result = pd.merge(answers_df, interview_df, on=\"doc_id\")\n",
    "final_dataset = result[[\"doc_id\", \"topic\", \"summ_sent_incorrect_original\", \"summ_sent_correct_manual\", \"exp\", \"type\", \"source\"]]\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a89423-9733-498e-807c-ef7c28e3de1a",
   "metadata": {},
   "source": [
    "### <a name=\"2\">LLM Debate: 2 expert LLMs, 1 naive judge - LLM-1 arguing for 1st summary</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f1bc7-1f39-4907-9ea8-4e73c2e97402",
   "metadata": {},
   "source": [
    "In this LLM Debate - Claude(LLM-1) defends incorrect Summary and Mixtral(LLM-2) defends correct summary.\n",
    "\n",
    "Claude v3(Sonnet) argues for answer A(Ground Truth:False Answer) and generates rationale why that answer is correct. Mixtral 8X7B argues for answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of the debate, Claude as a judge adjudicates whether Claude's or Mixtral's rationale is correct and chooses a side to give the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a70825e3-a6cb-4698-ba8c-4cf4b6bacb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== START OF 2 model DEBATE debate_id CNN-104129 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-104129 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-104129 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-104129 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-138971 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-138971 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-138971 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-138971 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-139946 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-139946 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-139946 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-139946 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-145383 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-145383 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-145383 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-145383 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-164885 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-164885 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-164885 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-164885 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-173359 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-173359 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-173359 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-173359 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-197627 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-197627 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-197627 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-197627 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-201245 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-201245 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-201245 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-201245 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-229050 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-229050 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-229050 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-229050 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-239067 Round #1..1 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-239067 Round #1..2 ======= \n",
      "\n",
      "=========== START OF 2 model DEBATE debate_id CNN-239067 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model DEBATE debate_id CNN-239067 Round #1..3 ======= \n",
      "\n",
      "CPU times: user 773 ms, sys: 63 ms, total: 836 ms\n",
      "Wall time: 18min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    \n",
    "    #### defending false - Claude\n",
    "    claude_defending_summary=answer_b\n",
    "    claude_opposing_summary=answer_a\n",
    "\n",
    "    #### defending true - Mixtral\n",
    "    mixtral_defending_summary=answer_a\n",
    "    mixtral_opposing_summary=answer_b\n",
    "\n",
    "    logger.info(f\"-------------2 model Debate -> Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    #### Consultancy Claude - defending true - 3 rounds\n",
    "    delete_file(f\"./transcripts/full_transcript_debate_{debate_id}.log\")\n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        print(f\"=========== START OF 2 model DEBATE debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "        logger.info(f\"START Debate with Claude Debate_id {debate_id} Round #{round_number + 1} >>>>>> \\n\") \n",
    "        claude_debate_response = invoke_claude_v3(debate_id = debate_id,\n",
    "                         question=question,\n",
    "                         round_number = round_number + 1,\n",
    "                         summary_defending = claude_defending_summary, \n",
    "                         summary_opposing = claude_opposing_summary, \n",
    "                         complete_interview = complete_interview_transcript,\n",
    "                         debate=True\n",
    "                         )\n",
    "\n",
    "        logger.info(f\" >>>>> claude_debate_response Round #{round_number + 1} >>>>> {claude_debate_response}\")\n",
    "        logger.info(f\"END Debate with Claude Round #{round_number + 1} >>>>>> \\n\")\n",
    "\n",
    "        mixtral_debate_response = invoke_mistral(debate_id = debate_id,\n",
    "                     question=question,\n",
    "                     round_number = round_number + 1,\n",
    "                     summary_defending = mixtral_defending_summary, \n",
    "                     summary_opposing = mixtral_opposing_summary, \n",
    "                     complete_interview = complete_interview_transcript, \n",
    "                     )\n",
    "\n",
    "        logger.info(f\" >>>>> mixtral_debate_response Round #{round_number + 1} >>>>> {mixtral_debate_response}\")\n",
    "        logger.info(f\"END Debate with Mixtral Round #{round_number + 1} >>>>>> \\n\")\n",
    "    print(f\"=========== END OF 2 model DEBATE debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152677e-6d8d-434e-82bb-3db6742b0dc8",
   "metadata": {},
   "source": [
    "## JUDGE for Regular Debate : LLM-Claude arguing for 1st summary, LLM-Mixtral arguing for 2nd summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "667b4c7d-eff4-478a-90bf-898bfa490401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-104129.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-104129</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:GM lost \\$10.6B in 2005, continues losing market share.<br> Answer B:GM lost \\$10B in 2005, continues losing market share.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Strengths and weaknesses of arguments:<br>- expert_debater_sonnet_v3 argues that the \\$10B approximation in Answer B captures the essence of GM's massive losses and aligns with the key fact of losing market share. This is a reasonable argument, but it relies on an unverified quote for the \\$10B figure.<br>- expert_debater_mixtral_8_7B argues that Answer A with the precise \\$10.6B figure is more accurate and directly matches the verified quote from the story. This is a strong argument based on evidence from the verified quote.<br><br>2. Evaluating which answer is likely correct:<br>- Answer A states \"GM lost \\$10.6B in 2005, continues losing market share.\"<br>- The verified quote is &lt;v quote&gt;\\$10.6 billion loss in 2005&lt;/v quote&gt;, which directly supports the \\$10.6B figure in Answer A.<br>- Both answers correctly capture the detail of GM losing market share, supported by the verified quote &lt;v quote&gt;The company continues to lose market share.&lt;/v quote&gt;<br>- While the \\$10B approximation in Answer B is close, Answer A with the precise \\$10.6B figure is more factually consistent with the verified quote from the story.<br><br>3. Answer chosen as correct: Answer A<br><br>Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-138971.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-138971</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Diplomatic efforts to secure the release of American journalists Laura Ling and Euna Lee, who are accused of spying in North Korea and face trial, are ongoing.<br> Answer B:North Korea has announced plans to launch a satellite into orbit, which the US sees as a cover for testing ballistic missile technology.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 argues for Answer B, citing the quote \"&lt;v quote&gt;North Korea angered the world with recent nuclear test and missile launches.&lt;/v quote&gt;\" as evidence that the story covers North Korea's controversial missile and nuclear activities. This is a verified quote, lending credibility to their argument.<br>- expert_debater_mixtral_8_7B argues for Answer A, citing quotes like \"&lt;v quote&gt;Trapped in one of the most isolated countries in the world, North Korea, imprisoned, accused of spying and awaiting trial. This is what American journalists Laura Ling and Euna Lee are facing.&lt;/v quote&gt;\" and \"&lt;v quote&gt;With his wife's trial just one day away, Laura Ling's husband said the strain has been unbearable.&lt;/v quote&gt;\" as evidence that the story's main focus is on the journalists' situation. These are also verified quotes, supporting their argument.<br><br>2. Considering which answer is more likely to be correct:<br>- Both debaters present valid arguments supported by verified quotes from the story.<br>- However, expert_debater_mixtral_8_7B's argument that the story primarily focuses on the journalists' situation and their imminent trial seems more convincing based on the weight of evidence provided through multiple verified quotes emphasizing this aspect.<br>- While expert_debater_sonnet_v3 correctly points out that the story also covers North Korea's missile and nuclear activities, this appears to be secondary context rather than the main focus of the report.<br><br>3. The answer I choose as correct is Answer A: \"Diplomatic efforts to secure the release of American journalists Laura Ling and Euna Lee, who are accused of spying in North Korea and face trial, are ongoing.\"<br><br>Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-139946.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-139946</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Democrats gain 60 seats in Senate, giving them a filibuster-proof majority.<br> Answer B:This filibuster-proof majority means Democrats can pass legislation without Republican support.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 relies heavily on unverified quotes, which should not be trusted. The only verified quote is \"&lt;v quote&gt;filibuster-proof majority&lt;/v quote&gt;\", which does not directly support the claim that Democrats can pass legislation without Republican support.<br>- expert_debater_mixtral_8_7B's arguments are more consistent with the verified quote. They acknowledge that a filibuster-proof majority does not guarantee passing all legislation without Republican support, but it strengthens the Democrats' position to pass key legislation.<br><br>2. Analyzing the answers:<br>- Answer A: \"Democrats gain 60 seats in Senate, giving them a filibuster-proof majority.\" This is directly supported by the verified quote and is a factual statement from the story.<br>- Answer B: \"This filibuster-proof majority means Democrats can pass legislation without Republican support.\" This is an overstatement not fully supported by the verified quote. The filibuster-proof majority makes it easier for Democrats to pass legislation, but it does not necessarily mean they can pass all legislation without any Republican support.<br><br>3. Based on the verified quote and the debaters' arguments, Answer A is more likely to be correct as it is a factual statement from the story, while Answer B is an overstatement not fully supported by the evidence.<br><br>4. Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-145383.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-145383</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Over \\$260 million in private funding will support programs to engage students, improve teacher effectiveness, and promote hands-on learning.<br> Answer B:The private sector has committed over \\$260 million to the campaign, and the initial focus is on reaching young students with interactive science content.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 provides two verified quotes that support engaging students with interactive science content and hands-on learning, which aligns with Answer B. However, they do not provide verified quotes to refute the aspect of improving teacher effectiveness mentioned in Answer A.<br>- expert_debater_mixtral_8_7B provides a verified quote indicating that the initiatives aim to improve teacher effectiveness, supporting Answer A. They also provide verified quotes about engaging students and hands-on learning, which are consistent with both answers.<br><br>2. Analyzing the answers:<br>- Answer A: \"Over \\$260 million in private funding will support programs to engage students, improve teacher effectiveness, and promote hands-on learning.\"<br>This answer seems to accurately capture the multiple goals of the initiatives based on the verified quotes provided. The aspect of improving teacher effectiveness is supported by the quote from Obama, and the other components (engaging students and hands-on learning) are also supported by verified quotes.<br><br>- Answer B: \"The private sector has committed over \\$260 million to the campaign, and the initial focus is on reaching young students with interactive science content.\"<br>This answer is partially correct, as it accurately reflects the initial focus on engaging young students with interactive science content, as supported by verified quotes. However, it does not capture the broader goals of the initiatives, such as improving teacher effectiveness.<br><br>3. Based on the evidence presented, Answer A seems to be the most factually consistent summary, as it accurately captures the multiple goals of the initiatives, including engaging students, promoting hands-on learning, and improving teacher effectiveness.<br><br>4. Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-164885.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-164885</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Cuba celebrated the 50th anniversary of their victory over a US-backed invasion at the Bay of Pigs.<br> Answer B:170,000 Cubans have private businesses.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 provides a verified quote directly stating \"I read something like 170,000 Cubans have actually taken out licenses to open their own business.\" This strongly supports Answer B.<br>- expert_debater_mixtral_8_7B focuses on the Bay of Pigs celebration, but does not provide any verified quotes directly supporting Answer A.<br><br>2. Analyzing the answers:<br>- Answer A: \"Cuba celebrated the 50th anniversary of their victory over a US-backed invasion at the Bay of Pigs.\" While the debate mentions this celebration, there are no verified quotes directly confirming this as the most factually consistent summary.<br>- Answer B: \"170,000 Cubans have private businesses.\" The verified quote \"I read something like 170,000 Cubans have actually taken out licenses to open their own business\" directly supports this summary, making it more factually consistent with the evidence provided.<br><br>3. Based on the verified quote supporting Answer B and the lack of verified quotes directly supporting Answer A, Answer B is likely to be the most factually consistent summary.<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-173359.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-173359</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting.<br> Answer B:Though Jackson was in good health, these sedatives and anesthetics stopped his breathing and led to his death.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Strengths and weaknesses of arguments:<br>- expert_debater_sonnet_v3 provides verified quotes showing that Jackson was in good health, but the combination of drugs stopped his breathing and led to his death. This supports Answer B.<br>- expert_debater_mixtral_8_7B provides a verified quote emphasizing that Propofol should not be used outside a hospital setting, which is relevant but does not directly answer the question about which summary is more factually consistent.<br><br>2. Evaluating the answers:<br>- Answer A: \"The use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting.\" This is true based on the verified quote, but it does not fully summarize the factual circumstances of Jackson's death.<br>- Answer B: \"Though Jackson was in good health, these sedatives and anesthetics stopped his breathing and led to his death.\" This is directly supported by the verified quotes stating that Jackson was in good health, but the combination of drugs stopped his breathing and caused his death.<br><br>Therefore, Answer B is more factually consistent with the verified evidence presented.<br><br>3. Answer chosen as correct: Answer B<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-197627.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-197627</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The document mentions that President Obama will be attending an interfaith vigil in Newtown, Connecticut to provide comfort to the families and first responders affected by the shooting at Sandy Hook Elementary School.<br> Answer B:Connecticut police confirmed Adam Lanza fired hundreds of bullets, killing 26.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments and quotes:<br>- expert_debater_sonnet_v3 provides several verified quotes that directly support Answer B, such as:<br>&lt;v quote&gt;Adam Lanza shot and killed his mother, shooting her multiple times inside the house&lt;/v quote&gt;<br>&lt;v quote&gt;shot dozens of bullets and he had hundreds of unused ammunition as well, hundreds of bullets that were not even used&lt;/v quote&gt;<br>&lt;v quote&gt;Lanza mostly used a semiautomatic weapon called a Bushmaster inside the school to kill those 26 people inside the school.&lt;/v quote&gt;<br>These verified quotes align with the key details in Answer B about Lanza firing hundreds of bullets and killing 26 people.<br><br>- expert_debater_mixtral_8_7B does not provide any verified quotes to support Answer A about Obama attending an interfaith vigil. The quotes used are either unverified or not direct matches from the story text.<br><br>2. Evaluating which answer is more likely correct:<br>- Answer B has strong evidence from multiple verified quotes that directly support its key details about Lanza's actions and the number of victims.<br>- Answer A lacks verified quotes to support the details about Obama attending a vigil. The debater's arguments focus more on the significance of the vigil rather than factual evidence from the story.<br><br>3. Based on the verified quotes provided, Answer B (\"Connecticut police confirmed Adam Lanza fired hundreds of bullets, killing 26\") appears to be the more factually consistent summary of the story details.<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-201245.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-201245</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:New York City Mayor Michael Bloomberg is planning to ban the use of Styrofoam in stores and restaurants due to environmental concerns.<br> Answer B:What is the proposed ban on Styrofoam in stores?</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Strengths and weaknesses of arguments:<br>- expert_debater_sonnet_v3 provides a verified quote directly supporting the proposed ban on Styrofoam/plastic foam packaging in stores and restaurants. However, the quote does not mention environmental concerns as the motivation.<br>- expert_debater_mixtral_8_7B correctly points out that the story does not explicitly state the motivation for the ban. Their argument relies on inferring environmental concerns based on Bloomberg's history of environmental policies, but this is not directly supported by verified quotes from the story.<br><br>2. Evaluating which answer is likely correct:<br>- Answer A states that the ban is \"due to environmental concerns\", which is not directly supported by verified quotes from the story.<br>- Answer B simply asks \"What is the proposed ban on Styrofoam in stores?\", which is directly supported by the verified quote about banning Styrofoam/plastic foam packaging in stores and restaurants.<br>- While environmental concerns could be a reasonable inference based on Bloomberg's history, Answer B is more factually consistent with the verified information provided in the story.<br><br>3. The answer I choose as most likely correct is Answer B: \"What is the proposed ban on Styrofoam in stores?\"<br><br>Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-229050.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-229050</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The physician describes the phenomenon that saved the boy's life as a perfect storm of disaster that allowed him to survive by slowing down his body's need for oxygen and putting him in a frozen state until his oxygen was replenished.<br> Answer B:He is shocked that the recent 15-year-old stowaway survived with no lasting side effects.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 argues that Answer B (\"He is shocked that the recent 15-year-old stowaway survived with no lasting side effects\") is more factually consistent, citing quotes that show the doctor's shock and the lack of immediate side effects. However, the quotes used are not verified, so I cannot fully trust them.<br>- expert_debater_mixtral_8_7B argues for Answer A (\"The physician describes the phenomenon that saved the boy's life as a perfect storm of disaster that allowed him to survive by slowing down his body's need for oxygen and putting him in a frozen state until his oxygen was replenished\"), citing a verified quote that directly supports this summary: &lt;v quote&gt;\"This was a perfect storm of disaster that actually probably saved his life...&lt;/v quote&gt;<br><br>2. Thinking step-by-step:<br>- The verified quote provided by expert_debater_mixtral_8_7B directly aligns with Answer A, as it describes the phenomenon that allowed the boy to survive.<br>- While the doctor's shock is mentioned in the arguments, there are no verified quotes to support this aspect, so I cannot rely on it.<br>- Answer A seems to capture the core explanation given by the doctor, while Answer B is more of a general statement that lacks the specific details described in the verified quote.<br><br>3. Based on the verified quote supporting Answer A and the lack of verified evidence for Answer B, I believe Answer A is the most factually consistent summary.<br><br>4. Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-239067.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-239067</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Currency uncertainty would be a major issue in the event of a yes vote in the Scotland referendum on independence.<br> Answer B:Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the debaters' arguments:<br><br>expert_debater_sonnet_v3's arguments:<br>- Relies heavily on the verified quotes from Kenneth Rogoff to support their summary (Answer B)<br>- Accurately captures the key points raised by Rogoff about currency uncertainty, including the challenges of choosing a currency and establishing credibility<br>- Their summary incorporates the specific details mentioned by Rogoff<br><br>expert_debater_mixtral_8_7B's arguments:<br>- Also uses the verified quotes from Rogoff to support their summary (Answer A)<br>- Acknowledges the severity of currency uncertainty as a \"major issue\"<br>- However, their summary is more vague and does not include the specific details mentioned by Rogoff<br><br>2. Evaluating which answer is more likely to be correct:<br>- Answer B (\"Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility\") is more comprehensive and aligns better with the verified quotes from Rogoff<br>- Rogoff explicitly mentions the uncertainty around choosing a currency, the challenges with each option (pound, euro, new currency), and the time needed to establish credibility for a new currency<br>- Answer A (\"Currency uncertainty would be a major issue in the event of a yes vote in the Scotland referendum on independence\") is true but lacks the specific details provided by Rogoff<br><br>3. The answer I choose as correct is Answer B, as argued by expert_debater_sonnet_v3.<br><br>Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, False, False, False, False, True, False]\n",
      "CPU times: user 167 ms, sys: 7.67 ms, total: 175 ms\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "debate_judge_regular_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------DEBATE  JUDGE Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    judge_response = invoke_claude_judge_debate(debate_id = debate_id,\n",
    "                              question=question,\n",
    "                 answer_a = answer_a,\n",
    "                 answer_b = answer_b)\n",
    "    debate_judge_regular_answers.append(extract_final_answer(judge_response, flipped=False))\n",
    "    logger.info(f\" >>>>> invoke_mistral_judge_debate - judge_response  >>>>> {judge_response}\")\n",
    "    # Print the final response \n",
    "    format_final_response(debate_id, \n",
    "                          round_num=1, \n",
    "                          question=question, \n",
    "                          answer_a=answer_a, \n",
    "                          answer_b=answer_b, \n",
    "                          judge_response=judge_response)\n",
    "print(debate_judge_regular_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a46272-45c5-4e0f-be8e-0a0850e5df8b",
   "metadata": {},
   "source": [
    "### <a name=\"3\">LLM Debate: 2 expert LLMs, 1 naive judge - LLM-1 arguing for 2nd summary</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7f903-969c-427d-a1aa-732d0db39ba2",
   "metadata": {},
   "source": [
    "In this **flipped LLM Debate** - Claude(LLM-1) defends correct Summary and Mixtral(LLM-2) defends incorrect summary.\n",
    "\n",
    "\n",
    "Claude v3(Sonnet) argues for answer B(Ground Truth:True Answer) and generates rationale why that answer is correct. Mixtral 8X7B argues for answer A(Ground Truth:False Answer) and generates rationale why that answer is correct. This continues for N(=3 in this notebook) rounds. At the end of the debate, Claude as a judge adjudicates whether Claude's or Mixtral's rationale is correct and chooses a side to give the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0cd6c9f-880e-4a70-a625-8c6a964a4947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-104129 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-104129 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-138971 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-138971 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-139946 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-139946 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-145383 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-145383 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-164885 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-164885 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-173359 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-173359 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-197627 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-197627 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-201245 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-201245 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-229050 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-229050 Round #1..3 ======= \n",
      "\n",
      "=========== START OF 2 model FLIPPED DEBATE debate_id CNN-239067 Round #1..3 ======= \n",
      "\n",
      "=========== END OF 2 model FLIPPED DEBATE debate_id CNN-239067 Round #1..3 ======= \n",
      "\n",
      "CPU times: user 740 ms, sys: 55.7 ms, total: 795 ms\n",
      "Wall time: 18min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    \n",
    "    #### defending True - Claude\n",
    "    claude_defending_summary=answer_a\n",
    "    claude_opposing_summary=answer_b\n",
    "\n",
    "    #### defending False - Mixtral\n",
    "    mixtral_defending_summary=answer_b\n",
    "    mixtral_opposing_summary=answer_a\n",
    "    \n",
    "    delete_file(f\"./transcripts/full_transcript_debate_{debate_id}{FLIPPED_FILE_SUFFIX}.log\")\n",
    "\n",
    "    logger.info(f\"-------------2 model Debate -> Debate_id {debate_id}-------------------\")\n",
    "    print(f\"=========== START OF 2 model FLIPPED DEBATE debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "    for round_number in range(number_of_rounds):\n",
    "        time.sleep(10) # avoid throttling exceptions\n",
    "        logger.info(f\"START Debate with Claude Round #{round_number + 1} >>>>>> \\n\") \n",
    "        claude_debate_response = invoke_claude_v3(debate_id = debate_id + FLIPPED_FILE_SUFFIX,\n",
    "                         question=question,\n",
    "                         round_number = round_number + 1,\n",
    "                         summary_defending = claude_defending_summary, \n",
    "                         summary_opposing = claude_opposing_summary, \n",
    "                         complete_interview = complete_interview_transcript,\n",
    "                         debate=True\n",
    "                         )\n",
    "\n",
    "        logger.info(f\" >>>>> claude_debate_response Round #{round_number + 1} >>>>> {claude_debate_response}\")\n",
    "        logger.info(f\"END Debate with Claude Round #{round_number + 1} >>>>>> \\n\")\n",
    "\n",
    "        mixtral_debate_response = invoke_mistral(debate_id = debate_id + FLIPPED_FILE_SUFFIX,\n",
    "                     question=question,\n",
    "                     round_number = round_number + 1,\n",
    "                     summary_defending = mixtral_defending_summary, \n",
    "                     summary_opposing = mixtral_opposing_summary, \n",
    "                     complete_interview = complete_interview_transcript, \n",
    "                     )\n",
    "\n",
    "        logger.info(f\" >>>>> mixtral_debate_response Round #{round_number + 1} >>>>> {mixtral_debate_response}\")\n",
    "        logger.info(f\"END Debate with Mixtral Round #{round_number + 1} >>>>>> \\n\")\n",
    "    print(f\"=========== END OF 2 model FLIPPED DEBATE debate_id {debate_id} Round #1..{round_number + 1} ======= \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73c855-24d5-4893-8b10-afc91936bf04",
   "metadata": {},
   "source": [
    "## JUDGE for flipped LLM Debate:LLM-Claude arguing for 1st summary, LLM-Mixtral arguing for 2nd summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee103881-33b6-473f-8d9b-ad7a69aa20df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-104129_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-104129</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:GM lost \\$10.6B in 2005, continues losing market share.<br> Answer B:GM lost \\$10B in 2005, continues losing market share.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Strengths and weaknesses of arguments:<br>- expert_debater_sonnet_v3 relies heavily on the precise \\$10.6 billion loss figure stated in the verified quote from the story: &lt;v quote&gt;\"General Motors reporting a \\$10.6 billion loss in 2005\"&lt;/v quote&gt;. This is a strong argument since it comes directly from the primary source.<br>- expert_debater_mixtral_8_7B acknowledges the \\$10.6 billion figure from the story but argues that the \\$10 billion figure in Answer B could still be correct. However, they provide no verified evidence to support the \\$10 billion claim over the story's stated figure.<br>- Both debaters provide some additional context about GM's financial struggles, but this is less directly relevant to evaluating the accuracy of the loss figures.<br><br>2. Evaluating which answer is likely correct:<br>- The story is presented as the primary source, so the verified quote stating GM's loss as \\$10.6 billion should be considered the most reliable evidence.<br>- There is no verified evidence provided to support the \\$10 billion figure in Answer B over the story's stated \\$10.6 billion figure.<br>- Therefore, based on the verified quote from the primary source, Answer A (\"GM lost \\$10.6B in 2005, continues losing market share\") is more likely to be the factually consistent summary.<br><br>3. Answer chosen as correct: Answer A (\"GM lost \\$10.6B in 2005, continues losing market share\")<br><br>Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-138971_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-138971</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Diplomatic efforts to secure the release of American journalists Laura Ling and Euna Lee, who are accused of spying in North Korea and face trial, are ongoing.<br> Answer B:North Korea has announced plans to launch a satellite into orbit, which the US sees as a cover for testing ballistic missile technology.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 provides several verified quotes that directly support Answer A about diplomatic efforts to secure the release of the journalists, such as:<br>&lt;v quote&gt;\"Representatives from Sweden have been working behind the scenes to try to engage the North Koreans.\"&lt;/v quote&gt;<br>&lt;v quote&gt;\"Former Vice President Al Gore says he'd even consider going to North Korea to help.\"&lt;/v quote&gt;<br>These quotes provide strong evidence for Answer A.<br><br>- expert_debater_mixtral_8_7B cites some quotes to support Answer B about North Korea's satellite launch, but these quotes are unverified and should not be trusted:<br>&lt;u quote&gt;\"Complicating diplomacy, North Korea angered the world with recent nuclear test and missile launches.\" - Wendy Sherman&lt;/u quote&gt;<br>&lt;u quote&gt;\"I think what you would expect to see is something in between a prison sentence to, you know, perhaps even sentencing to some sort of labor camp.\" - Victor Cha&lt;/u quote&gt;<br>These quotes provide context but do not directly support Answer B over Answer A.<br><br>2. Considering which answer is more likely correct:<br>- The verified quotes provided by expert_debater_sonnet_v3 directly support the details in Answer A about diplomatic efforts to secure the journalists' release.<br>- While the story mentions North Korea's satellite launch plans as context, there are no verified quotes that directly support Answer B being a more factually consistent summary than Answer A.<br>- The central focus of the story seems to be on the journalists' trial and the efforts to secure their release, based on the verified quotes.<br><br>3. Therefore, based on the evidence presented, Answer A appears to be the more factually consistent summary of the story.<br><br>4. Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-139946_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-139946</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Democrats gain 60 seats in Senate, giving them a filibuster-proof majority.<br> Answer B:This filibuster-proof majority means Democrats can pass legislation without Republican support.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br><br>expert_debater_sonnet_v3's arguments:<br>- Relies on verified quotes from the story that cast doubt on the claim that a filibuster-proof majority means Democrats can pass legislation without Republican support. The quotes suggest this is an assumption rather than a factual statement.<br>- Points out that Al Franken himself suggests he will represent all Minnesotans, not just be the 60th Democratic vote.<br>- Seems to have a stronger argument based on the verified quotes provided.<br><br>expert_debater_mixtral_8_7B's arguments:<br>- Claims that the story provides implicit support for the idea that a filibuster-proof majority allows Democrats to pass legislation without Republican support in terms of ending a filibuster.<br>- However, does not provide any verified quotes to support this claim.<br>- Relies more on assumptions and interpretations rather than direct evidence from the story.<br><br>2. Evaluating the answers:<br>Answer A: \"Democrats gain 60 seats in Senate, giving them a filibuster-proof majority.\"<br>- This statement is directly supported by a verified quote from the story, so it seems to be factually consistent.<br><br>Answer B: \"This filibuster-proof majority means Democrats can pass legislation without Republican support.\"<br>- The story casts doubt on this claim, suggesting it is an assumption rather than a factual statement, based on the verified quotes provided.<br>- Therefore, Answer A appears to be the more factually consistent summary based on the evidence presented.<br><br>3. The answer I choose as correct is Answer A: \"Democrats gain 60 seats in Senate, giving them a filibuster-proof majority.\"<br><br>4. Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-145383_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-145383</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Over \\$260 million in private funding will support programs to engage students, improve teacher effectiveness, and promote hands-on learning.<br> Answer B:The private sector has committed over \\$260 million to the campaign, and the initial focus is on reaching young students with interactive science content.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the debaters' arguments:<br><br>expert_debater_sonnet_v3's arguments:<br>- Relies on verified quotes to support the claim that the campaign aims to improve overall science and math education, engage students, improve teacher effectiveness, and promote hands-on learning.<br>- The quotes seem to align with Answer A, which covers these broader goals.<br><br>expert_debater_mixtral_8_7B's arguments:<br>- Focuses on the \"initial focus\" being on reaching young students with interactive science content, citing verified quotes about Sesame Street and Discovery Communications' initiatives.<br>- However, the quotes do not necessarily contradict the broader goals mentioned in Answer A.<br><br>2. Evaluating which answer is more likely correct:<br>- Answer A covers the broader goals of the campaign as stated in the verified quotes, including improving overall science and math education, engaging students, improving teacher effectiveness, and promoting hands-on learning.<br>- While Answer B is not necessarily incorrect, as the \"initial focus\" on young students and interactive content is supported by verified quotes, it is a narrower summary that does not capture the full scope of the campaign's goals.<br>- Based on the verified quotes provided, Answer A seems to be a more comprehensive and accurate summary of the campaign's goals.<br><br>3. The answer I choose as correct is Answer A: \"Over \\$260 million in private funding will support programs to engage students, improve teacher effectiveness, and promote hands-on learning.\"<br><br>4. Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-164885_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-164885</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Cuba celebrated the 50th anniversary of their victory over a US-backed invasion at the Bay of Pigs.<br> Answer B:170,000 Cubans have private businesses.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the debaters' arguments:<br><br>expert_debater_sonnet_v3's arguments:<br>- Relies heavily on the verified quote about the \"massive celebration of Cuba's past victories\" and the Bay of Pigs anniversary, which supports Answer A.<br>- Dismisses the private business detail as minor, but the transcript does provide some specific information about it.<br><br>expert_debater_mixtral_8_7B's arguments:<br>- Acknowledges the Bay of Pigs celebration but argues that the economic reforms are a major focus of the transcript.<br>- Provides a verified quote highlighting the extent of the planned economic changes, including layoffs and allowing more private businesses.<br>- This evidence supports Answer B being more factually consistent with the transcript's overall content.<br><br>2. Evaluating which answer is more likely correct:<br>- While the Bay of Pigs anniversary celebration is a significant event covered in the transcript, the economic reforms and plans for more private enterprise also receive substantial coverage.<br>- The verified quote provided by expert_debater_mixtral_8_7B directly supports the summary in Answer B about the number of private businesses.<br>- No verified quote is provided that directly supports the summary in Answer A about the Bay of Pigs anniversary celebration.<br>- Based on the evidence presented, Answer B appears to be more factually consistent with the overall content of the transcript.<br><br>3. The answer I choose as most likely correct is Answer B: \"170,000 Cubans have private businesses.\"<br><br>4. Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-173359_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-173359</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting.<br> Answer B:Though Jackson was in good health, these sedatives and anesthetics stopped his breathing and led to his death.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Strengths and weaknesses of arguments:<br>- expert_debater_sonnet_v3 provides several verified quotes from Dr. Cain that directly support Answer A, emphasizing that Propofol and the combination of drugs found in Jackson's system should not be used outside a hospital setting due to the need for monitoring vital signs. This argument is well-supported by evidence from the verified quotes.<br>- expert_debater_mixtral_8_7B acknowledges the dangers of using Propofol outside a hospital setting, but argues that Answer B is more factually consistent because it highlights Jackson's good health in addition to the role of the sedatives and anesthetics in his death. However, this argument relies heavily on an unverified quote about Jackson's good health, which should not be fully trusted.<br><br>2. Evaluating which answer is likely correct:<br>- Answer A focuses specifically on the inappropriate use of Propofol and the combination of drugs outside a hospital setting, which is directly supported by multiple verified quotes from Dr. Cain.<br>- Answer B includes the context of Jackson's good health, but this detail is not verified and is less central to the issue raised by Dr. Cain regarding the improper use of the drugs.<br>- Based on the verified evidence provided, Answer A appears to be the more factually consistent summary, as it accurately captures the main concern expressed by Dr. Cain about the use of these drugs outside a hospital setting.<br><br>3. The answer I choose as correct is Answer A: \"The use of multiple drugs together, including Propofol, is not recommended outside of a hospital setting.\"<br><br>Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-197627_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-197627</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The document mentions that President Obama will be attending an interfaith vigil in Newtown, Connecticut to provide comfort to the families and first responders affected by the shooting at Sandy Hook Elementary School.<br> Answer B:Connecticut police confirmed Adam Lanza fired hundreds of bullets, killing 26.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 provides two verified quotes that directly support Answer A about President Obama attending an interfaith vigil in Newtown. These quotes seem trustworthy and factually consistent with the story.<br>- expert_debater_mixtral_8_7B provides one verified quote that supports details about the shooting, weapons used, and number of victims. However, this quote does not directly confirm the summary in Answer B about the number of bullets fired.<br><br>2. Analyzing the answers:<br>- Answer A is supported by verified quotes that align with the story's context of President Obama visiting Newtown after the shooting.<br>- Answer B, while providing details about the shooting, lacks a verified quote confirming the specific number of bullets fired by Adam Lanza.<br><br>3. Based on the verified evidence presented, Answer A seems more factually consistent with the story. The verified quotes directly support the summary about President Obama attending an interfaith vigil in Newtown.<br><br>4. Final Answer: A<br>&lt;/thinking&gt;<br><br>Final Answer: A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-201245_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-201245</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:New York City Mayor Michael Bloomberg is planning to ban the use of Styrofoam in stores and restaurants due to environmental concerns.<br> Answer B:What is the proposed ban on Styrofoam in stores?</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments:<br>- expert_debater_sonnet_v3 provides verified quotes that support Answer A, showing Bloomberg's plan to ban Styrofoam/plastic foam packaging in NYC stores and restaurants due to environmental concerns. The quotes directly confirm the details in Answer A.<br>- expert_debater_mixtral_8_7B's argument for Answer B relies on an unverified quote, which should not be trusted. The verified quote they provide only confirms there is a proposed ban on Styrofoam in stores, but does not support Answer B being the most factually consistent summary.<br><br>2. Thinking step-by-step:<br>- The question asks for the most factually consistent summary of the story details.<br>- Answer A provides specific details about the proposed ban (Styrofoam/plastic foam packaging), the locations (stores and restaurants), the person behind it (Mayor Bloomberg), and the motivation (environmental concerns).<br>- These details are directly supported by verified quotes from the story.<br>- Answer B only mentions the proposed ban on Styrofoam in stores, without any of the additional factual details present in Answer A.<br>- Therefore, Answer A is more factually consistent with the information provided in the verified quotes from the story.<br><br>3. The answer I choose as correct is Answer A: \"New York City Mayor Michael Bloomberg is planning to ban the use of Styrofoam in stores and restaurants due to environmental concerns.\"<br><br>4. Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-229050_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-229050</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:The physician describes the phenomenon that saved the boy's life as a perfect storm of disaster that allowed him to survive by slowing down his body's need for oxygen and putting him in a frozen state until his oxygen was replenished.<br> Answer B:He is shocked that the recent 15-year-old stowaway survived with no lasting side effects.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the arguments and verified quotes:<br><br>expert_debater_sonnet_v3's arguments:<br>- Relies heavily on verified quotes from the doctor describing the \"perfect storm of disaster\" that allowed the boy to survive by slowing his body's need for oxygen and putting him in a \"frozen or suspended state\".<br>- The quotes support the claim that the doctor understands the physiological reasons behind the boy's survival, beyond just being shocked.<br>- The quotes also show the doctor's amazement stems from the rarity of these conditions aligning perfectly.<br><br>expert_debater_mixtral_8_7B's arguments:<br>- Relies on a verified quote showing the doctor's surprise at the boy walking away without lasting effects, which supports the claim that the doctor was shocked.<br>- However, the arguments do not fully address the doctor's detailed explanation of the physiological phenomenon that allowed survival.<br><br>2. Evaluating which answer is more likely correct:<br>- Answer A focuses on the doctor's explanation of the \"perfect storm of disaster\" that allowed the boy's survival by slowing his body's need for oxygen and putting him in a frozen state. This is well-supported by the verified quotes.<br>- Answer B focuses solely on the doctor being shocked that the boy survived with no lasting side effects. While this is supported by a verified quote, it does not capture the doctor's detailed explanation of the physiological phenomenon.<br>- Therefore, Answer A seems to be a more comprehensive and accurate summary of the doctor's statements, as it incorporates both his explanation of the phenomenon and his amazement at the rarity of the circumstances.<br><br>3. The answer I choose as correct is Answer A: \"The physician describes the phenomenon that saved the boy's life as a perfect storm of disaster that allowed him to survive by slowing down his body's need for oxygen and putting him in a frozen state until his oxygen was replenished.\"<br><br>4. Final Answer: A<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke_claude_judge_debate :: transcript_filename >>>> /home/ec2-user/SageMaker/improve-factual-consistency-with-llm-debate-technique/transcripts/full_transcript_debate_CNN-239067_flipped_.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debate ID</th>\n",
       "      <th>Round #</th>\n",
       "      <th>Task Question</th>\n",
       "      <th>Judge Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-239067</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of these summaries is the most factually consistent one?<br> Answer A:Currency uncertainty would be a major issue in the event of a yes vote in the Scotland referendum on independence.<br> Answer B:Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility.</td>\n",
       "      <td>&lt;thinking&gt;<br>1. Evaluating the debaters' arguments:<br><br>expert_debater_sonnet_v3's arguments:<br>- Relies on the verified quote from Kenneth Rogoff: &lt;v quote&gt;When you got through with this divorce, suddenly you don\\'t have the currency. What currency do you have? That\\'s one of the first questions people will have, it\\'s one of the first questions investors have.&lt;/v quote&gt;<br>- Also cites the verified quote: &lt;v quote&gt;They say it\\'s the pound. The British say it\\'s not the pound. People say, well, maybe Scotland will join the euro. That creates a lot of problems for the rest of the eurozone. And they could have their own currency, but that\\'s not a magic elixir when you don\\'t have long credibility.&lt;/v quote&gt;<br>- These quotes support the claim that currency uncertainty would be a \"major issue\" in the event of Scottish independence.<br><br>expert_debater_mixtral_8_7B's arguments:<br>- Also relies on the same verified quotes from Kenneth Rogoff.<br>- The summary accurately captures the currency uncertainty as a significant issue arising from a yes vote, with questions about the currency and establishing credibility.<br>- Does not overstate or understate the magnitude of the issue based on the verified quotes.<br><br>2. Evaluating which answer is more likely correct:<br>- Both summaries acknowledge that currency uncertainty would be a major issue/significant problem in the event of Scottish independence.<br>- Answer A states it would be a \"major issue\", while Answer B states it \"would arise\" with questions about the currency and establishing credibility.<br>- The verified quotes support both summaries, but Answer B provides more specific details about the nature of the uncertainty (questions about currency and credibility).<br>- Therefore, Answer B is more comprehensive and aligned with the verified quotes.<br><br>3. The answer I choose as most likely correct is Answer B: \"Currency uncertainty would arise in the event of a yes vote in the Scottish referendum, with questions about what currency would be used and how long it would take to establish credibility.\"<br><br>Final Answer: B<br>&lt;/thinking&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, False, True, True, True, True, False]\n",
      "CPU times: user 110 ms, sys: 8.3 ms, total: 118 ms\n",
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "debate_judge_flipped_answers = list()\n",
    "for index, row in final_dataset.iterrows():\n",
    "    time.sleep(10) # avoid throttling exceptions\n",
    "    debate_id = row['doc_id']\n",
    "    answer_a = row['summ_sent_correct_manual']\n",
    "    answer_b = row['summ_sent_incorrect_original']\n",
    "    complete_interview_transcript = row['source']\n",
    "    logger.info(f\"-------------DEBATE FLIPPED JUDGE Debate_id {debate_id}-------------------\")\n",
    "\n",
    "    judge_response = invoke_claude_judge_debate(debate_id = debate_id + FLIPPED_FILE_SUFFIX,\n",
    "                              question=question,\n",
    "                              answer_a = answer_a,\n",
    "                              answer_b = answer_b)\n",
    "    debate_judge_flipped_answers.append(extract_final_answer(judge_response, flipped=False))\n",
    "    logger.info(f\" >>>>> Flipped invoke_mistral_judge_debate - judge_response  >>>>> {judge_response}\")\n",
    "    \n",
    "    # Print the final response \n",
    "    format_final_response(debate_id, \n",
    "                          round_num=1, \n",
    "                          question=question, \n",
    "                          answer_a=answer_a, \n",
    "                          answer_b=answer_b, \n",
    "                          judge_response=judge_response)\n",
    "print(debate_judge_flipped_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd391f-2c5b-4385-84d9-38da4515699f",
   "metadata": {},
   "source": [
    "## <a name=\"4\">Accuracy of LLM Debate</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dee9e72-42c2-4963-a68c-60941c472e29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, False, False, False, False, True, False]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debate_judge_regular_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26b689ae-7377-41db-afce-c80bf65fc1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, False, True, True, True, True, False]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debate_judge_flipped_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c47c3faa-1ddd-4f7b-864f-efac1c23a155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_debate_judge = find_num_matching_elements(debate_judge_regular_answers, debate_judge_flipped_answers)/total_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f13e301-ce93-49d5-ab48-430245eff5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_debate_judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab6f33fc-52aa-442a-bc76-ed55f15d560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_naive_judge': 0.2, 'accuracy_expert_judge': 0.3, 'accuracy_consultant_judge': 0.1, 'accuracy_debate_judge': 0.7}\n",
      "notebook results saved in results folder\n"
     ]
    }
   ],
   "source": [
    "# save the results\n",
    "results_dict = {\"accuracy_debate_judge\" : accuracy_debate_judge}\n",
    "save_each_experiment_result(results_dict)\n",
    "print(\"notebook results saved in results folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fbd255-a593-438f-a44f-fce489056f26",
   "metadata": {},
   "source": [
    "## <a name=\"5\">Compare Accuracies across experiments/methods.</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97c205-fa79-4c09-b115-0cd3e9cae864",
   "metadata": {},
   "source": [
    "Here we compare the accuracies of each method/experiment to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39472b99-25dc-4d47-8242-450df4c13559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_naive_judge': 0.2, 'accuracy_expert_judge': 0.3, 'accuracy_consultant_judge': 0.1, 'accuracy_debate_judge': 0.7}\n",
      "{'accuracy_naive_judge': 0.2, 'accuracy_expert_judge': 0.3, 'accuracy_consultant_judge': 0.1, 'accuracy_debate_judge': 0.7}\n",
      "{'accuracy_naive_judge': 0.2, 'accuracy_expert_judge': 0.3, 'accuracy_consultant_judge': 0.1, 'accuracy_debate_judge': 0.7}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Naive Judge</th>\n",
       "      <th>Expert Judge</th>\n",
       "      <th>LLM Consultancy</th>\n",
       "      <th>LLM Debate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_naive_judge = get_each_experiment_result(\"accuracy_naive_judge\")\n",
    "accuracy_expert_judge = get_each_experiment_result(\"accuracy_expert_judge\")\n",
    "accuracy_consultant_judge = get_each_experiment_result(\"accuracy_consultant_judge\")\n",
    "\n",
    "final_accuracy_comparison(\n",
    "    accuracy_naive_judge = accuracy_naive_judge,\n",
    "    accuracy_expert_judge = accuracy_expert_judge,\n",
    "    accuracy_consultant_judge = accuracy_consultant_judge,\n",
    "    accuracy_debate_judge = accuracy_debate_judge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e751a8d-fd46-4192-b100-655e8342ffe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the plot\n",
    "x_values = [ \"Naive Judge\", \"Expert Judge\", \"LLM Consultant\", \"LLM Debate\"]\n",
    "y_values = [ accuracy_naive_judge, accuracy_expert_judge, accuracy_consultant_judge, accuracy_debate_judge]\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Compare Accuracies across experiments')\n",
    "plt.xlabel('Experiment Type')\n",
    "plt.ylabel('Accuracy')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b11eae-cb6b-4196-a91e-56b2f3c4b076",
   "metadata": {},
   "source": [
    "### <a name=\"6\">Choose expert LLM using Win Rate measured during LLM Debate (Experiment 4) </a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0682b92-dcda-4949-b9ac-49c027e18226",
   "metadata": {},
   "source": [
    "With this win rate of expert models, we emprically understand which LLM as a debater is more successful than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cc2b8aa-0335-4d8a-811d-9da60dd69826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_common_regular_value =True , regular_count = 5\n",
      "most_common_flipped_value =True , flipped_count = 8\n",
      "\n",
      " claude_regular_win_rate :: 0.5 \n",
      "                \n",
      " mistral_regular_win_rate :: 0.5 \n",
      "                \n",
      " claude_flipped_win_rate :: 0.8\n",
      "                \n",
      " mistral_flipped_win_rate :: 0.19999999999999996 \n"
     ]
    }
   ],
   "source": [
    "claude_avg_win_rate, mixtral_avg_win_rate = get_win_rate_per_model(\n",
    "    debate_judge_regular_answers, \n",
    "    debate_judge_flipped_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d719aa86-ca8b-436a-8b6d-9ca1a3ba08db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claude Win Rate</th>\n",
       "      <th>Mixtral Win Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "win_rate_comparison(claude_avg_win_rate, mixtral_avg_win_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc1319a9-e85c-4d29-92d7-60acd1a21e51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the plot\n",
    "%matplotlib inline\n",
    "x_values = [ \"Claude v3 Sonnet\", \"Mixtral\"]\n",
    "y_values = [claude_avg_win_rate, mixtral_avg_win_rate]\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Compare average win-rate across expert LLMs')\n",
    "plt.xlabel('average win-rate')\n",
    "plt.ylabel('win-rate')\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
